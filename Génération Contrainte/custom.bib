@techreport{He2023,
  author     = {He, Mutian and Garner, Philip N.},
  title      = {Can {ChatGPT} {Detect} {Intent}? {Evaluating} {Large} {Language} {Models} for {Spoken} {Language} {Understanding}},
  year       = {2023},
  month      = aug,
  note       = {arXiv:2305.13512 [cs, eess] type: article},
  abstract   = {Recently, large pretrained language models have demonstrated strong language understanding capabilities. This is particularly reflected in their zero-shot and in-context learning abilities on downstream tasks through prompting. To assess their impact on spoken language understanding (SLU), we evaluate several such models like ChatGPT and OPT of different sizes on multiple benchmarks. We verify the emergent ability unique to the largest models as they can reach intent classification accuracy close to that of supervised models with zero or few shots on various languages given oracle transcripts. By contrast, the results for smaller models fitting a single GPU fall far behind. We note that the error cases often arise from the annotation scheme of the dataset; responses from ChatGPT are still reasonable. We show, however, that the model is worse at slot filling, and its performance is sensitive to ASR errors, suggesting serious challenges for the application of those textual models on SLU.},
  annote     = {Comment: 6 pages, 2 figures; Accepted by Interspeech 2023},
  file       = {:he_can_2023 - Can ChatGPT Detect Intent_ Evaluating Large Language Models for Spoken Language Understanding.pdf:PDF},
  groups     = {AttempsSLU},
  keywords   = {Computer Science - Computation and Language, Computer Science - Artificial Intelligence, Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing},
  school     = {arXiv},
  shorttitle = {Can {ChatGPT} {Detect} {Intent}?},
  url        = {http://arxiv.org/abs/2305.13512},
  urldate    = {2024-04-10}
}

@inproceedings{Cui2021,
  author    = {Cui, Leyang and Wu, Yu and Liu, Jian and Yang, Sen and Zhang, Yue},
  booktitle = {Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021},
  title     = {Template-Based Named Entity Recognition Using {BART}},
  year      = {2021},
  address   = {Online},
  editor    = {Zong, Chengqing and Xia, Fei and Li, Wenjie and Navigli, Roberto},
  month     = aug,
  pages     = {1835--1845},
  publisher = {Association for Computational Linguistics},
  doi       = {10.18653/v1/2021.findings-acl.161},
  groups    = {Constrained Prompting},
  url       = {https://aclanthology.org/2021.findings-acl.161}
}

@inproceedings{Shen2023_promptner,
  author    = {Shen, Yongliang and Tan, Zeqi and Wu, Shuhui and Zhang, Wenqi and Zhang, Rongsheng and Xi, Yadong and Lu, Weiming and Zhuang, Yueting},
  booktitle = {Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
  title     = {{P}rompt{NER}: Prompt Locating and Typing for Named Entity Recognition},
  year      = {2023},
  address   = {Toronto, Canada},
  editor    = {Rogers, Anna and Boyd-Graber, Jordan and Okazaki, Naoaki},
  month     = jul,
  pages     = {12492--12507},
  publisher = {Association for Computational Linguistics},
  abstract  = {Prompt learning is a new paradigm for utilizing pre-trained language models and has achieved great success in many tasks. To adopt prompt learning in the NER task, two kinds of methods have been explored from a pair of symmetric perspectives, populating the template by enumerating spans to predict their entity types or constructing type-specific prompts to locate entities. However, these methods not only require a multi-round prompting manner with a high time overhead and computational cost, but also require elaborate prompt templates, that are difficult to apply in practical scenarios. In this paper, we unify entity locating and entity typing into prompt learning, and design a dual-slot multi-prompt template with the position slot and type slot to prompt locating and typing respectively. Multiple prompts can be input to the model simultaneously, and then the model extracts all entities by parallel predictions on the slots. To assign labels for the slots during training, we design a dynamic template filling mechanism that uses the extended bipartite graph matching between prompts and the ground-truth entities. We conduct experiments in various settings, including resource-rich flat and nested NER datasets and low-resource in-domain and cross-domain datasets. Experimental results show that the proposed model achieves a significant performance improvement, especially in the cross-domain few-shot setting, which outperforms the state-of-the-art model by +7.7{\%} on average.},
  doi       = {10.18653/v1/2023.acl-long.698},
  groups    = {Constrained Prompting},
  url       = {https://aclanthology.org/2023.acl-long.698}
}

@inproceedings{Mirza2024,
  author    = {Mirza, Paramita and Sudhi, Viju and Sahoo, Soumya and Bhat, Sinchana Ramakanth},
  booktitle = {Proceedings of the 2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation (LREC-COLING 2024)},
  title     = {ILLUMINER: Instruction-tuned Large Language Models as Few-shot Intent Classifier and Slot Filler},
  year      = {2024},
  address   = {Torino, Italy},
  month     = may
}


@article{Ye2023,
  author         = {Ye, Feiyang and Huang, Liang and Liang, Senjie and Chi, KaiKai},
  journal        = {Information},
  title          = {Decomposed Two-Stage Prompt Learning for Few-Shot Named Entity Recognition},
  year           = {2023},
  issn           = {2078-2489},
  number         = {5},
  volume         = {14},
  abstract       = {Named entity recognition (NER) in a few-shot setting is an extremely challenging task, and most existing methods fail to account for the gap between NER tasks and pre-trained language models. Although prompt learning has been successfully applied in few-shot classification tasks, adapting to token-level classification similar to the NER task presents challenges in terms of time consumption and efficiency. In this work, we propose a decomposed prompt learning NER framework for few-shot settings, decomposing the NER task into two stages: entity locating and entity typing. In training, the location information of distant labels is used to train the entity locating model. A concise but effective prompt template is built to train the entity typing model. In inference, a pipeline approach is used to handle the entire NER task, which elegantly resolves time-consuming and inefficient problems. Specifically, a well-trained entity locating model is used to predict entity spans for each input. The input is then transformed using prompt templates, and the well-trained entity typing model is used to predict their types in a single step. Experimental results demonstrate that our framework outperforms previous prompt-based methods by an average of 2.3–12.9% in F1 score while achieving the best trade-off between accuracy and inference speed.},
  article-number = {262},
  doi            = {10.3390/info14050262},
  groups         = {Constrained Prompting},
  url            = {https://www.mdpi.com/2078-2489/14/5/262}
}

@inproceedings{Schick2021_smallLM,
  author    = {Schick, Timo and Sch{\"u}tze, Hinrich},
  booktitle = {Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies},
  title     = {It{'}s Not Just Size That Matters: Small Language Models Are Also Few-Shot Learners},
  year      = {2021},
  address   = {Online},
  editor    = {Toutanova, Kristina and Rumshisky, Anna and Zettlemoyer, Luke and Hakkani-Tur, Dilek and Beltagy, Iz and Bethard, Steven and Cotterell, Ryan and Chakraborty, Tanmoy and Zhou, Yichao},
  month     = jun,
  pages     = {2339--2352},
  publisher = {Association for Computational Linguistics},
  abstract  = {When scaled to hundreds of billions of parameters, pretrained language models such as GPT-3 (Brown et al., 2020) achieve remarkable few-shot performance. However, enormous amounts of compute are required for training and applying such big models, resulting in a large carbon footprint and making it difficult for researchers and practitioners to use them. We show that performance similar to GPT-3 can be obtained with language models that are much {``}greener{''} in that their parameter count is several orders of magnitude smaller. This is achieved by converting textual inputs into cloze questions that contain a task description, combined with gradient-based optimization; exploiting unlabeled data gives further improvements. We identify key factors required for successful natural language understanding with small language models.},
  doi       = {10.18653/v1/2021.naacl-main.185},
  groups    = {Constrained Prompting},
  url       = {https://aclanthology.org/2021.naacl-main.185}
}

@article{Liu2023_suervey_isk,
  author     = {Liu, Pengfei and Yuan, Weizhe and Fu, Jinlan and Jiang, Zhengbao and Hayashi, Hiroaki and Neubig, Graham},
  journal    = {ACM Comput. Surv.},
  title      = {Pre-train, Prompt, and Predict: A Systematic Survey of Prompting Methods in Natural Language Processing},
  year       = {2023},
  issn       = {0360-0300},
  month      = {jan},
  number     = {9},
  volume     = {55},
  abstract   = {This article surveys and organizes research works in a new paradigm in natural language processing, which we dub “prompt-based learning.” Unlike traditional supervised learning, which trains a model to take in an input x and predict an output y as P(y|x), prompt-based learning is based on language models that model the probability of text directly. To use these models to perform prediction tasks, the original input x is modified using a template into a textual string prompt x′ that has some unfilled slots, and then the language model is used to probabilistically fill the unfilled information to obtain a final string x̂, from which the final output y can be derived. This framework is powerful and attractive for a number of reasons: It allows the language model to be pre-trained on massive amounts of raw text, and by defining a new prompting function the model is able to perform few-shot or even zero-shot learning, adapting to new scenarios with few or no labeled data. In this article, we introduce the basics of this promising paradigm, describe a unified set of mathematical notations that can cover a wide variety of existing work, and organize existing work along several dimensions, e.g.,&nbsp;the choice of pre-trained language models, prompts, and tuning strategies. To make the field more accessible to interested beginners, we not only make a systematic review of existing works and a highly structured typology of prompt-based concepts but also release other resources, e.g., a website  including constantly updated survey and paperlist.},
  address    = {New York, NY, USA},
  articleno  = {195},
  doi        = {10.1145/3560815},
  groups     = {General LLM-Prompting},
  issue_date = {September 2023},
  keywords   = {Pre-trained language models, prompting},
  numpages   = {35},
  publisher  = {Association for Computing Machinery},
  url        = {https://doi.org/10.1145/3560815}
}

@misc{Qin2021,
  author        = {Libo Qin and Tianbao Xie and Wanxiang Che and Ting Liu},
  title         = {A Survey on Spoken Language Understanding: Recent Advances and New Frontiers},
  year          = {2021},
  archiveprefix = {arXiv},
  eprint        = {2103.03095},
  primaryclass  = {cs.CL}
}

@misc{Yin2024,
  author        = {Shangjian Yin and Peijie Huang and Yuhong Xu and Haojing Huang and Jiatian Chen},
  title         = {Do Large Language Model Understand Multi-Intent Spoken Language ?},
  year          = {2024},
  archiveprefix = {arXiv},
  eprint        = {2403.04481},
  groups        = {AttempsSLU},
  primaryclass  = {cs.CL}
}

@article{liu_few-shot_2022,
  title    = {Few-{Shot} {Parameter}-{Efficient} {Fine}-{Tuning} is {Better} and {Cheaper} than {In}-{Context} {Learning}},
  volume   = {35},
  url      = {https://proceedings.neurips.cc/paper_files/paper/2022/hash/0cde695b83bd186c1fd456302888454c-Abstract-Conference.html},
  language = {en},
  urldate  = {2023-09-29},
  journal  = {Advances in Neural Information Processing Systems},
  author   = {Liu, Haokun and Tam, Derek and Muqeeth, Mohammed and Mohta, Jay and Huang, Tenghao and Bansal, Mohit and Raffel, Colin A.},
  month    = dec,
  year     = {2022},
  pages    = {1950--1965}
}


@misc{flant5,
  title     = {Finetuned {Language} {Models} {Are} {Zero}-{Shot} {Learners}},
  url       = {http://arxiv.org/abs/2109.01652},
  doi       = {10.48550/arXiv.2109.01652},
  abstract  = {This paper explores a simple method for improving the zero-shot learning abilities of language models. We show that instruction tuning -- finetuning language models on a collection of tasks described via instructions -- substantially improves zero-shot performance on unseen tasks. We take a 137B parameter pretrained language model and instruction-tune it on over 60 NLP tasks verbalized via natural language instruction templates. We evaluate this instruction-tuned model, which we call FLAN, on unseen task types. FLAN substantially improves the performance of its unmodified counterpart and surpasses zero-shot 175B GPT-3 on 20 of 25 tasks that we evaluate. FLAN even outperforms few-shot GPT-3 by a large margin on ANLI, RTE, BoolQ, AI2-ARC, OpenbookQA, and StoryCloze. Ablation studies reveal that number of finetuning datasets, model scale, and natural language instructions are key to the success of instruction tuning.},
  urldate   = {2023-09-18},
  publisher = {arXiv},
  author    = {Wei, Jason and Bosma, Maarten and Zhao, Vincent Y. and Guu, Kelvin and Yu, Adams Wei and Lester, Brian and Du, Nan and Dai, Andrew M. and Le, Quoc V.},
  month     = feb,
  year      = {2022},
  note      = {arXiv:2109.01652 [cs]},
  keywords  = {Computer Science - Computation and Language}
}

@misc{zheng_judging_2023,
  title    = {Judging {LLM}-as-a-judge with {MT}-{Bench} and {Chatbot} {Arena}},
  url      = {https://arxiv.org/abs/2306.05685v2},
  abstract = {Evaluating large language model (LLM) based chat assistants is challenging due to their broad capabilities and the inadequacy of existing benchmarks in measuring human preferences. To address this, we explore using strong LLMs as judges to evaluate these models on more open-ended questions. We examine the usage and limitations of LLM-as-a-judge, including position, verbosity, and self-enhancement biases, as well as limited reasoning ability, and propose solutions to mitigate some of them. We then verify the agreement between LLM judges and human preferences by introducing two benchmarks: MT-bench, a multi-turn question set; and Chatbot Arena, a crowdsourced battle platform. Our results reveal that strong LLM judges like GPT-4 can match both controlled and crowdsourced human preferences well, achieving over 80{\textbackslash}\% agreement, the same level of agreement between humans. Hence, LLM-as-a-judge is a scalable and explainable way to approximate human preferences, which are otherwise very expensive to obtain. Additionally, we show our benchmark and traditional benchmarks complement each other by evaluating several variants of LLaMA and Vicuna. We will publicly release MT-bench questions, 3K expert votes, and 30K conversations with human preferences from Chatbot Arena.},
  language = {en},
  urldate  = {2023-10-09},
  journal  = {arXiv.org},
  author   = {Zheng, Lianmin and Chiang, Wei-Lin and Sheng, Ying and Zhuang, Siyuan and Wu, Zhanghao and Zhuang, Yonghao and Lin, Zi and Li, Zhuohan and Li, Dacheng and Xing, Eric P. and Zhang, Hao and Gonzalez, Joseph E. and Stoica, Ion},
  month    = jun,
  year     = {2023}
}

@inproceedings{muennighoff_crosslingual_2023,
  address   = {Toronto, Canada},
  title     = {Crosslingual {Generalization} through {Multitask} {Finetuning}},
  url       = {https://aclanthology.org/2023.acl-long.891},
  doi       = {10.18653/v1/2023.acl-long.891},
  abstract  = {Multitask prompted finetuning (MTF) has been shown to help large language models generalize to new tasks in a zero-shot setting, but so far explorations of MTF have focused on English data and models. We apply MTF to the pretrained multilingual BLOOM and mT5 model families to produce finetuned variants called BLOOMZ and mT0. We find finetuning large multilingual language models on English tasks with English prompts allows for task genrealization to non-English languages that appear only in the pretraining corpus. Finetuning on multilingual tasks with English prompts further improves performance on English and non-English tasks leading to various state-of-the-art zero-shot results. We also investigate finetuning on multilingual tasks with prompts that have been machine-translated from English to match the language of each dataset. We find training on these machine-translated prompts leads to better performance on human-written prompts in the respective languages. Surprisingly, we find models are capable of zero-shot generalization to tasks in languages they have never intentionally seen. We conjecture that the models are learning higher-level capabilities that are both task- and language-agnostic. In addition, we introduce xP3, a composite of supervised datasets in 46 languages with English and machine-translated prompts. Our code, datasets and models are freely available at https://github.com/bigscience-workshop/xmtf.},
  urldate   = {2023-10-09},
  booktitle = {Proceedings of the 61st {Annual} {Meeting} of the {Association} for {Computational} {Linguistics} ({Volume} 1: {Long} {Papers})},
  publisher = {Association for Computational Linguistics},
  author    = {Muennighoff, Niklas and Wang, Thomas and Sutawika, Lintang and Roberts, Adam and Biderman, Stella and Le Scao, Teven and Bari, M Saiful and Shen, Sheng and Yong, Zheng Xin and Schoelkopf, Hailey and Tang, Xiangru and Radev, Dragomir and Aji, Alham Fikri and Almubarak, Khalid and Albanie, Samuel and Alyafeai, Zaid and Webson, Albert and Raff, Edward and Raffel, Colin},
  month     = jul,
  year      = {2023},
  pages     = {15991--16111}
}


@misc{instruct-llm-survey,
  title      = {Instruction {Tuning} for {Large} {Language} {Models}: {A} {Survey}},
  shorttitle = {Instruction {Tuning} for {Large} {Language} {Models}},
  url        = {http://arxiv.org/abs/2308.10792},
  abstract   = {This paper surveys research works in the quickly advancing field of instruction tuning (IT), a crucial technique to enhance the capabilities and controllability of large language models (LLMs). Instruction tuning refers to the process of further training LLMs on a dataset consisting of {\textbackslash}textsc\{(instruction, output)\} pairs in a supervised fashion, which bridges the gap between the next-word prediction objective of LLMs and the users' objective of having LLMs adhere to human instructions. In this work, we make a systematic review of the literature, including the general methodology of IT, the construction of IT datasets, the training of IT models, and applications to different modalities, domains and applications, along with an analysis on aspects that influence the outcome of IT (e.g., generation of instruction outputs, size of the instruction dataset, etc). We also review the potential pitfalls of IT along with criticism against it, along with efforts pointing out current deficiencies of existing strategies and suggest some avenues for fruitful research.},
  urldate    = {2023-09-18},
  publisher  = {arXiv},
  author     = {Zhang, Shengyu and Dong, Linfeng and Li, Xiaoya and Zhang, Sen and Sun, Xiaofei and Wang, Shuhe and Li, Jiwei and Hu, Runyi and Zhang, Tianwei and Wu, Fei and Wang, Guoyin},
  month      = aug,
  year       = {2023},
  note       = {arXiv:2308.10792 [cs]},
  keywords   = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning}
}

@inproceedings{brown_language_2020,
  title     = {Language {Models} are {Few}-{Shot} {Learners}},
  volume    = {33},
  url       = {https://papers.nips.cc/paper/2020/hash/1457c0d6bfcb4967418bfb8ac142f64a-Abstract.html},
  abstract  = {We demonstrate that scaling up language models greatly improves task-agnostic, few-shot performance, sometimes even becoming competitive with prior state-of-the-art fine-tuning approaches. Specifically, we train GPT-3, an autoregressive language model with 175 billion parameters, 10x more than any previous non-sparse language model, and test its performance in the few-shot setting.  For all tasks, GPT-3 is applied without any gradient updates or fine-tuning, with tasks and few-shot demonstrations specified purely via text interaction with the model.  GPT-3 achieves strong performance on many NLP datasets, including translation, question-answering, and cloze tasks. We also identify some datasets where GPT-3's few-shot learning still struggles, as well as some datasets where GPT-3 faces methodological issues related to training on large web corpora.},
  urldate   = {2023-09-29},
  booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
  publisher = {Curran Associates, Inc.},
  author    = {Brown, Tom and others},
  year      = {2020},
  pages     = {1877--1901}
}



@inproceedings{peft-lora,
  title      = {{LoRA}: {Low}-{Rank} {Adaptation} of {Large} {Language} {Models}},
  shorttitle = {{LoRA}},
  url        = {https://openreview.net/forum?id=nZeVKeeFYf9},
  abstract   = {An important paradigm of natural language processing consists of large-scale pre-training on general domain data and adaptation to particular tasks or domains. As we pre-train larger models, full fine-tuning, which retrains all model parameters, becomes less feasible. Using GPT-3 175B as an example -- deploying independent instances of fine-tuned models, each with 175B parameters, is prohibitively expensive. We propose Low-Rank Adaptation, or LoRA, which freezes the pre-trained model weights and injects trainable rank decomposition matrices into each layer of the Transformer architecture, greatly reducing the number of trainable parameters for downstream tasks. Compared to GPT-3 175B fine-tuned with Adam, LoRA can reduce the number of trainable parameters by a factor of 10,000 and the GPU memory requirement by a factor of 3. LoRA performs on-par or better than fine-tuning in model quality on RoBERTa, DeBERTa, GPT-2, and GPT-3, despite having fewer trainable parameters, a higher training throughput, and, unlike adapters, no additional inference latency. We also provide an empirical investigation into rank-deficiency in language model adaptation, which sheds light on the efficacy of LoRA. We release a package that facilitates the integration of LoRA with PyTorch models and provide our implementations and model checkpoints for RoBERTa, DeBERTa, and GPT-2 at https://github.com/microsoft/LoRA.},
  language   = {en},
  urldate    = {2023-10-09},
  author     = {Hu, Edward J. and Shen, Yelong and Wallis, Phillip and Allen-Zhu, Zeyuan and Li, Yuanzhi and Wang, Shean and Wang, Lu and Chen, Weizhu},
  booktitle  = {International Conference on Learning Representations},
  year       = {2022}
}

@inproceedings{hou_inverse_2022,
  address   = {Dublin, Ireland},
  title     = {Inverse is {Better}! {Fast} and {Accurate} {Prompt} for {Few}-shot {Slot} {Tagging}},
  url       = {https://aclanthology.org/2022.findings-acl.53},
  doi       = {10.18653/v1/2022.findings-acl.53},
  abstract  = {Prompting methods recently achieve impressive success in few-shot learning. These methods modify input samples with prompt sentence pieces, and decode label tokens to map samples to corresponding labels. However, such a paradigm is very inefficient for the task of slot tagging. Since slot tagging samples are multiple consecutive words in a sentence, the prompting methods have to enumerate all n-grams token spans to find all the possible slots, which greatly slows down the prediction. To tackle this, we introduce an inverse paradigm for prompting. Different from the classic prompts mapping tokens to labels, we reversely predict slot values given slot types. Such inverse prompting only requires a one-turn prediction for each slot type and greatly speeds up the prediction. Besides, we propose a novel Iterative Prediction Strategy, from which the model learns to refine predictions by considering the relations between different slot types. We find, somewhat surprisingly, the proposed method not only predicts faster but also significantly improves the effect (improve over 6.1 F1-scores on 10-shot setting) and achieves new state-of-the-art performance.},
  urldate   = {2023-09-18},
  booktitle = {Findings of the {Association} for {Computational} {Linguistics}: {ACL} 2022},
  publisher = {Association for Computational Linguistics},
  author    = {Hou, Yutai and Chen, Cheng and Luo, Xianzhen and Li, Bohan and Che, Wanxiang},
  month     = may,
  year      = {2022},
  pages     = {637--647}
}

@inproceedings{li_generative_2023,
  address   = {Toronto, Canada},
  title     = {Generative {Zero}-{Shot} {Prompt} {Learning} for {Cross}-{Domain} {Slot} {Filling} with {Inverse} {Prompting}},
  url       = {https://aclanthology.org/2023.findings-acl.52},
  abstract  = {Zero-shot cross-domain slot filling aims to transfer knowledge from the labeled source domain to the unlabeled target domain. Existing models either encode slot descriptions and examples or design handcrafted question templates using heuristic rules, suffering from poor generalization capability or robustness. In this paper, we propose a generative zero-shot prompt learning framework for cross-domain slot filling, both improving generalization and robustness than previous work. Besides, we introduce a novel inverse prompting strategy to distinguish different slot types to avoid the multiple prediction problem, and an efficient prompt tuning strategy to boost higher performance only training fewer prompt parameters. Experiments and analysis demonstrate the effectiveness of our proposed framework, especially huge improvements (+13.44\% F1) on the unseen slots.},
  urldate   = {2023-08-04},
  booktitle = {Findings of the {Association} for {Computational} {Linguistics}: {ACL} 2023},
  publisher = {Association for Computational Linguistics},
  author    = {Li, Xuefeng and Wang, Liwen and Dong, Guanting and He, Keqing and Zhao, Jinzheng and Lei, Hao and Liu, Jiachi and Xu, Weiran},
  month     = jul,
  year      = {2023},
  pages     = {825--834}
}

@inproceedings{rosenbaum_linguist_2022,
  address    = {Gyeongju, Republic of Korea},
  title      = {{LINGUIST}: {Language} {Model} {Instruction} {Tuning} to {Generate} {Annotated} {Utterances} for {Intent} {Classification} and {Slot} {Tagging}},
  shorttitle = {{LINGUIST}},
  url        = {https://aclanthology.org/2022.coling-1.18},
  abstract   = {We present LINGUIST, a method for generating annotated data for Intent Classification and Slot Tagging (IC+ST), via fine-tuning AlexaTM 5B, a 5-billion-parameter multilingual sequence-to-sequence (seq2seq) model, on a flexible instruction prompt. In a 10-shot novel intent setting for the SNIPS dataset, LINGUIST surpasses state-of-the-art approaches (Back-Translation and Example Extrapolation) by a wide margin, showing absolute improvement for the target intents of +1.9 points on IC Recall and +2.5 points on ST F1 Score. In the zero-shot cross-lingual setting of the mATIS++ dataset, LINGUIST out-performs a strong baseline of Machine Translation with Slot Alignment by +4.14 points absolute on ST F1 Score across 6 languages, while matching performance on IC. Finally, we verify our results on an internal large-scale multilingual dataset for conversational agent IC+ST and show significant improvements over a baseline which uses Back-Translation, Paraphrasing and Slot Catalog Resampling. To our knowledge, we are the first to demonstrate instruction fine-tuning of a large-scale seq2seq model to control the outputs of multilingual intent- and slot-labeled data generation.},
  urldate    = {2023-09-19},
  booktitle  = {Proceedings of the 29th {International} {Conference} on {Computational} {Linguistics}},
  publisher  = {International Committee on Computational Linguistics},
  author     = {Rosenbaum, Andy and Soltan, Saleh and Hamza, Wael and Versley, Yannick and Boese, Markus},
  month      = oct,
  year       = {2022},
  pages      = {218--241}
}

@misc{chen_bert_2019,
  title     = {{BERT} for {Joint} {Intent} {Classification} and {Slot} {Filling}},
  url       = {http://arxiv.org/abs/1902.10909},
  abstract  = {Intent classification and slot filling are two essential tasks for natural language understanding. They often suffer from small-scale human-labeled training data, resulting in poor generalization capability, especially for rare words. Recently a new language representation model, BERT (Bidirectional Encoder Representations from Transformers), facilitates pre-training deep bidirectional representations on large-scale unlabeled corpora, and has created state-of-the-art models for a wide variety of natural language processing tasks after simple fine-tuning. However, there has not been much effort on exploring BERT for natural language understanding. In this work, we propose a joint intent classification and slot filling model based on BERT. Experimental results demonstrate that our proposed model achieves significant improvement on intent classification accuracy, slot filling F1, and sentence-level semantic frame accuracy on several public benchmark datasets, compared to the attention-based recurrent neural network models and slot-gated models.},
  urldate   = {2023-09-28},
  publisher = {arXiv},
  author    = {Chen, Qian and Zhuo, Zhu and Wang, Wen},
  month     = feb,
  year      = {2019},
  note      = {arXiv:1902.10909 [cs]},
  keywords  = {Computer Science - Computation and Language}
}

@misc{tunstall_efficient_2022,
  title    = {Efficient {Few}-{Shot} {Learning} {Without} {Prompts}},
  url      = {https://arxiv.org/abs/2209.11055v1},
  abstract = {Recent few-shot methods, such as parameter-efficient fine-tuning (PEFT) and pattern exploiting training (PET), have achieved impressive results in label-scarce settings. However, they are difficult to employ since they are subject to high variability from manually crafted prompts, and typically require billion-parameter language models to achieve high accuracy. To address these shortcomings, we propose SetFit (Sentence Transformer Fine-tuning), an efficient and prompt-free framework for few-shot fine-tuning of Sentence Transformers (ST). SetFit works by first fine-tuning a pretrained ST on a small number of text pairs, in a contrastive Siamese manner. The resulting model is then used to generate rich text embeddings, which are used to train a classification head. This simple framework requires no prompts or verbalizers, and achieves high accuracy with orders of magnitude less parameters than existing techniques. Our experiments show that SetFit obtains comparable results with PEFT and PET techniques, while being an order of magnitude faster to train. We also show that SetFit can be applied in multilingual settings by simply switching the ST body. Our code is available at https://github.com/huggingface/setfit and our datasets at https://huggingface.co/setfit .},
  language = {en},
  urldate  = {2023-09-29},
  journal  = {arXiv.org},
  author   = {Tunstall, Lewis and Reimers, Nils and Jo, Unso Eun Seo and Bates, Luke and Korat, Daniel and Wasserblat, Moshe and Pereg, Oren},
  month    = sep,
  year     = {2022}
}

@inproceedings{lin_selective_2023,
  address   = {Dubrovnik, Croatia},
  title     = {Selective {In}-{Context} {Data} {Augmentation} for {Intent} {Detection} using {Pointwise} {V}-{Information}},
  url       = {https://aclanthology.org/2023.eacl-main.107},
  doi       = {10.18653/v1/2023.eacl-main.107},
  abstract  = {This work focuses on in-context data augmentation for intent detection. Having found that augmentation via in-context prompting of large pre-trained language models (PLMs) alone does not improve performance, we introduce a novel approach based on PLMs and pointwise V-information (PVI), a metric that can measure the usefulness of a datapoint for training a model. Our method first fine-tunes a PLM on a small seed of training data and then synthesizes new datapoints - utterances that correspond to given intents. It then employs intent-aware filtering, based on PVI, to remove datapoints that are not helpful to the downstream intent classifier. Our method is thus able to leverage the expressive power of large language models to produce diverse training data. Empirical results demonstrate that our method can produce synthetic training data that achieve state-of-the-art performance on three challenging intent detection datasets under few-shot settings (1.28\% absolute improvement in 5-shot and 1.18\% absolute in 10-shot, on average) and perform on par with the state-of-the-art in full-shot settings (within 0.01\% absolute, on average).},
  urldate   = {2023-09-29},
  booktitle = {Proceedings of the 17th {Conference} of the {European} {Chapter} of the {Association} for {Computational} {Linguistics}},
  publisher = {Association for Computational Linguistics},
  author    = {Lin, Yen-Ting and Papangelis, Alexandros and Kim, Seokhwan and Lee, Sungjin and Hazarika, Devamanyu and Namazifar, Mahdi and Jin, Di and Liu, Yang and Hakkani-Tur, Dilek},
  month     = may,
  year      = {2023},
  pages     = {1463--1476}
}

@misc{touvron_llama_2023,
  title      = {{LLaMA}: {Open} and {Efficient} {Foundation} {Language} {Models}},
  shorttitle = {{LLaMA}},
  url        = {http://arxiv.org/abs/2302.13971},
  doi        = {10.48550/arXiv.2302.13971},
  abstract   = {We introduce LLaMA, a collection of foundation language models ranging from 7B to 65B parameters. We train our models on trillions of tokens, and show that it is possible to train state-of-the-art models using publicly available datasets exclusively, without resorting to proprietary and inaccessible datasets. In particular, LLaMA-13B outperforms GPT-3 (175B) on most benchmarks, and LLaMA-65B is competitive with the best models, Chinchilla-70B and PaLM-540B. We release all our models to the research community.},
  urldate    = {2023-09-29},
  publisher  = {arXiv},
  author     = {Touvron, Hugo and others},
  month      = feb,
  year       = {2023},
  note       = {arXiv:2302.13971 [cs]},
  keywords   = {Computer Science - Computation and Language}
}

@misc{touvron_llama2_2023,
  title      = {Llama 2: {Open} {Foundation} and {Fine}-{Tuned} {Chat} {Models}},
  shorttitle = {Llama 2},
  url        = {http://arxiv.org/abs/2307.09288},
  doi        = {10.48550/arXiv.2307.09288},
  abstract   = {In this work, we develop and release Llama 2, a collection of pretrained and fine-tuned large language models (LLMs) ranging in scale from 7 billion to 70 billion parameters. Our fine-tuned LLMs, called Llama 2-Chat, are optimized for dialogue use cases. Our models outperform open-source chat models on most benchmarks we tested, and based on our human evaluations for helpfulness and safety, may be a suitable substitute for closed-source models. We provide a detailed description of our approach to fine-tuning and safety improvements of Llama 2-Chat in order to enable the community to build on our work and contribute to the responsible development of LLMs.},
  urldate    = {2023-09-29},
  publisher  = {arXiv},
  author     = {Touvron, Hugo and others},
  month      = jul,
  year       = {2023},
  note       = {arXiv:2307.09288 [cs]},
  keywords   = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language}
}

@article{falcon40b,
  title  = {{Falcon-40B}: an open large language model with state-of-the-art performance},
  author = {Almazrouei, Ebtesam and others},
  year   = {2023}
}

@misc{goodfellow_empirical_2015,
  title     = {An {Empirical} {Investigation} of {Catastrophic} {Forgetting} in {Gradient}-{Based} {Neural} {Networks}},
  url       = {http://arxiv.org/abs/1312.6211},
  doi       = {10.48550/arXiv.1312.6211},
  abstract  = {Catastrophic forgetting is a problem faced by many machine learning models and algorithms. When trained on one task, then trained on a second task, many machine learning models "forget" how to perform the first task. This is widely believed to be a serious problem for neural networks. Here, we investigate the extent to which the catastrophic forgetting problem occurs for modern neural networks, comparing both established and recent gradient-based training algorithms and activation functions. We also examine the effect of the relationship between the first task and the second task on catastrophic forgetting. We find that it is always best to train using the dropout algorithm--the dropout algorithm is consistently best at adapting to the new task, remembering the old task, and has the best tradeoff curve between these two extremes. We find that different tasks and relationships between tasks result in very different rankings of activation function performance. This suggests the choice of activation function should always be cross-validated.},
  urldate   = {2023-09-29},
  publisher = {arXiv},
  author    = {Goodfellow, Ian J. and Mirza, Mehdi and Xiao, Da and Courville, Aaron and Bengio, Yoshua},
  month     = mar,
  year      = {2015},
  note      = {arXiv:1312.6211 [cs, stat]},
  keywords  = {Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, Statistics - Machine Learning}
}

@misc{qin2021survey,
  title         = {A Survey on Spoken Language Understanding: Recent Advances and New Frontiers},
  author        = {Libo Qin and Tianbao Xie and Wanxiang Che and Ting Liu},
  year          = {2021},
  eprint        = {2103.03095},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL}
}
@article{castellucci2019multilingual,
  title   = {Multi-lingual Intent Detection and Slot Filling in a Joint BERT-based Model},
  author  = {Giuseppe Castellucci and Valentina Bellomaria and Andrea Favalli and Raniero Romagnoli},
  year    = {2019},
  journal = {arXiv preprint arXiv:1907.02884}
}

@inproceedings{qin-etal-2019-stack,
  title     = {A Stack-Propagation Framework with Token-Level Intent Detection for Spoken Language Understanding},
  author    = {Qin, Libo  and
               Che, Wanxiang  and
               Li, Yangming  and
               Wen, Haoyang  and
               Liu, Ting},
  booktitle = {Proc. of EMNLP-IJCNLP},
  year      = {2019},
  url       = {https://www.aclweb.org/anthology/D19-1214},
  doi       = {10.18653/v1/D19-1214},
  abstract  = {Intent detection and slot filling are two main tasks for building a spoken language understanding (SLU) system. The two tasks are closely tied and the slots often highly depend on the intent. In this paper, we propose a novel framework for SLU to better incorporate the intent information, which further guiding the slot filling. In our framework, we adopt a joint model with Stack-Propagation which can directly use the intent information as input for slot filling, thus to capture the intent semantic knowledge. In addition, to further alleviate the error propagation, we perform the token-level intent detection for the Stack-Propagation framework. Experiments on two publicly datasets show that our model achieves the state-of-the-art performance and outperforms other previous methods by a large margin. Finally, we use the Bidirectional Encoder Representation from Transformer (BERT) model in our framework, which further boost our performance in SLU task.}
}
@inproceedings{servan-etal-2024-malbert-compact,
  title     = {m{ALBERT}: Is a Compact Multilingual {BERT} Model Still Worth It?},
  author    = {Servan, Christophe  and
               Ghannay, Sahar  and
               Rosset, Sophie},
  editor    = {Calzolari, Nicoletta  and
               Kan, Min-Yen  and
               Hoste, Veronique  and
               Lenci, Alessandro  and
               Sakti, Sakriani  and
               Xue, Nianwen},
  booktitle = {Proceedings of the 2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation (LREC-COLING 2024)},
  month     = may,
  year      = {2024},
  address   = {Torino, Italia},
  publisher = {ELRA and ICCL},
  url       = {https://aclanthology.org/2024.lrec-main.960},
  pages     = {11023--11029},
  abstract  = {Within the current trend of Pretained Language Models (PLM), emerge more and more criticisms about the ethical and ecological impact of such models. In this article, considering these critical remarks, we propose to focus on smaller models, such as compact models like ALBERT, which are more ecologically virtuous than these PLM. However, PLMs enable huge breakthroughs in Natural Language Processing tasks, such as Spoken and Natural Language Understanding, classification, Question{--}Answering tasks. PLMs also have the advantage of being multilingual, and, as far as we know, a multilingual version of compact ALBERT models does not exist. Considering these facts, we propose the free release of the first version of a multilingual compact ALBERT model, pre-trained using Wikipedia data, which complies with the ethical aspect of such a language model. We also evaluate the model against classical multilingual PLMs in classical NLP tasks. Finally, this paper proposes a rare study on the subword tokenization impact on language performances.}
}
@inproceedings{hakkani2016multi,
  title     = {Multi-domain joint semantic frame parsing using bi-directional rnn-lstm.},
  author    = {Hakkani-T{\"u}r, Dilek and T{\"u}r, G{\"o}khan and Celikyilmaz, Asli and Chen, Yun-Nung and Gao, Jianfeng and Deng, Li and Wang, Ye-Yi},
  booktitle = {Interspeech},
  year      = {2016}
}
@inproceedings{qin2020cointeractive,
  title     = {A Co-Interactive Transformer for Joint Slot Filling and Intent Detection},
  author    = {Libo Qin and Tailu Liu and Wanxiang Che and Bingbing Kang and Sendong Zhao and Ting Liu},
  booktitle = {ICASSP},
  year      = {2021}
}

@misc{zhang2024instruction,
  title         = {Instruction Tuning for Large Language Models: A Survey},
  author        = {Shengyu Zhang and Linfeng Dong and Xiaoya Li and Sen Zhang and Xiaofei Sun and Shuhe Wang and Jiwei Li and Runyi Hu and Tianwei Zhang and Fei Wu and Guoyin Wang},
  year          = {2024},
  eprint        = {2308.10792},
  archiveprefix = {arXiv},
  primaryclass  = {id='cs.CL' full_name='Computation and Language' is_active=True alt_name='cmp-lg' in_archive='cs' is_general=False description='Covers natural language processing. Roughly includes material in ACM Subject Class I.2.7. Note that work on artificial languages (programming languages, logics, formal systems) that does not explicitly address natural-language issues broadly construed (natural-language processing, computational linguistics, speech, text retrieval, etc.) is not appropriate for this area.'}
}

@inproceedings{wang-etal-2023-self-instruct,
  title     = {Self-Instruct: Aligning Language Models with Self-Generated Instructions},
  author    = {Wang, Yizhong  and
               Kordi, Yeganeh  and
               Mishra, Swaroop  and
               Liu, Alisa  and
               Smith, Noah A.  and
               Khashabi, Daniel  and
               Hajishirzi, Hannaneh},
  editor    = {Rogers, Anna  and
               Boyd-Graber, Jordan  and
               Okazaki, Naoaki},
  booktitle = {Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
  month     = jul,
  year      = {2023},
  address   = {Toronto, Canada},
  publisher = {Association for Computational Linguistics},
  url       = {https://aclanthology.org/2023.acl-long.754},
  doi       = {10.18653/v1/2023.acl-long.754},
  pages     = {13484--13508},
  abstract  = {Large {``}instruction-tuned{''} language models (i.e., finetuned to respond to instructions) have demonstrated a remarkable ability to generalize zero-shot to new tasks. Nevertheless, they depend heavily on human-written instruction data that is often limited in quantity, diversity, and creativity, therefore hindering the generality of the tuned model. We introduce Self-Instruct, a framework for improving the instruction-following capabilities of pretrained language models by bootstrapping off their own generations. Our pipeline generates instructions, input, and output samples from a language model, then filters invalid or similar ones before using them to finetune the original model. Applying our method to the vanilla GPT3, we demonstrate a 33{\%} absolute improvement over the original model on Super-NaturalInstructions, on par with the performance of InstructGPT-001, which was trained with private user data and human annotations. For further evaluation, we curate a set of expert-written instructions for novel tasks, and show through human evaluation that tuning GPT3 with Self-Instruct outperforms using existing public instruction datasets by a large margin, leaving only a 5{\%} absolute gap behind InstructGPT-001. Self-Instruct provides an almost annotation-free method for aligning pre-trained language models with instructions, and we release our large synthetic dataset to facilitate future studies on instruction tuning.}
}
@misc{workshop_bloom_2022,
  title      = {{BLOOM}: {A} {176B}-{Parameter} {Open}-{Access} {Multilingual} {Language} {Model}},
  shorttitle = {{BLOOM}},
  url        = {https://arxiv.org/abs/2211.05100v4},
  abstract   = {Large language models (LLMs) have been shown to be able to perform new tasks based on a few demonstrations or natural language instructions. While these capabilities have led to widespread adoption, most LLMs are developed by resource-rich organizations and are frequently kept from the public. As a step towards democratizing this powerful technology, we present BLOOM, a 176B-parameter open-access language model designed and built thanks to a collaboration of hundreds of researchers. BLOOM is a decoder-only Transformer language model that was trained on the ROOTS corpus, a dataset comprising hundreds of sources in 46 natural and 13 programming languages (59 in total). We find that BLOOM achieves competitive performance on a wide variety of benchmarks, with stronger results after undergoing multitask prompted finetuning. To facilitate future research and applications using LLMs, we publicly release our models and code under the Responsible AI License.},
  language   = {en},
  urldate    = {2023-09-30},
  journal    = {arXiv.org},
  author     = {Workshop, BigScience and others},
  month      = nov,
  year       = {2022}
}

@article{weld_survey_sf_2023,
  title    = {A {Survey} of {Joint} {Intent} {Detection} and {Slot} {Filling} {Models} in {Natural} {Language} {Understanding}},
  volume   = {55},
  issn     = {0360-0300, 1557-7341},
  url      = {https://dl.acm.org/doi/10.1145/3547138},
  doi      = {10.1145/3547138},
  abstract = {Intent classification, to identify the speaker’s intention, and slot filling, to label each token with a semantic type, are critical tasks in natural language understanding. Traditionally the two tasks have been addressed independently. More recently joint models that address the two tasks together have achieved state-of-the-art performance for each task and have shown there exists a strong relationship between the two. In this survey, we bring the coverage of methods up to 2021 including the many applications of deep learning in the field. As well as a technological survey, we look at issues addressed in the joint task and the approaches designed to address these issues. We cover datasets, evaluation metrics, and experiment design and supply a summary of reported performance on the standard datasets.},
  language = {en},
  number   = {8},
  urldate  = {2023-09-28},
  journal  = {ACM Computing Surveys},
  author   = {Weld, Henry and Huang, Xiaoqi and Long, Siqu and Poon, Josiah and Han, Soyeon Caren},
  month    = aug,
  year     = {2023},
  pages    = {1--38}
}

@inproceedings{houlsby_parameter-efficient_2019,
  title     = {Parameter-{Efficient} {Transfer} {Learning} for {NLP}},
  url       = {https://proceedings.mlr.press/v97/houlsby19a.html},
  abstract  = {Fine-tuning large pretrained models is an effective transfer mechanism in NLP. However, in the presence of many downstream tasks, fine-tuning is parameter inefficient: an entire new model is required for every task. As an alternative, we propose transfer with adapter modules. Adapter modules yield a compact and extensible model; they add only a few trainable parameters per task, and new tasks can be added without revisiting previous ones. The parameters of the original network remain fixed, yielding a high degree of parameter sharing. To demonstrate adapter’s effectiveness, we transfer the recently proposed BERT Transformer model to 262626 diverse text classification tasks, including the GLUE benchmark. Adapters attain near state-of-the-art performance, whilst adding only a few parameters per task. On GLUE, we attain within 0.80.80.8\% of the performance of full fine-tuning, adding only 3.63.63.6\% parameters per task. By contrast, fine-tuning trains 100100100\% of the parameters per task.},
  language  = {en},
  urldate   = {2023-10-09},
  booktitle = {Proceedings of the 36th {International} {Conference} on {Machine} {Learning}},
  publisher = {PMLR},
  author    = {Houlsby, Neil and Giurgiu, Andrei and Jastrzebski, Stanislaw and Morrone, Bruna and Laroussilhe, Quentin De and Gesmundo, Andrea and Attariyan, Mona and Gelly, Sylvain},
  month     = may,
  year      = {2019},
  note      = {ISSN: 2640-3498},
  pages     = {2790--2799}
}

@inproceedings{lester_power_2021,
  address   = {Online and Punta Cana, Dominican Republic},
  title     = {The {Power} of {Scale} for {Parameter}-{Efficient} {Prompt} {Tuning}},
  url       = {https://aclanthology.org/2021.emnlp-main.243},
  doi       = {10.18653/v1/2021.emnlp-main.243},
  abstract  = {In this work, we explore “prompt tuning,” a simple yet effective mechanism for learning “soft prompts” to condition frozen language models to perform specific downstream tasks. Unlike the discrete text prompts used by GPT-3, soft prompts are learned through backpropagation and can be tuned to incorporate signals from any number of labeled examples. Our end-to-end learned approach outperforms GPT-3's few-shot learning by a large margin. More remarkably, through ablations on model size using T5, we show that prompt tuning becomes more competitive with scale: as models exceed billions of parameters, our method “closes the gap” and matches the strong performance of model tuning (where all model weights are tuned). This finding is especially relevant because large models are costly to share and serve and the ability to reuse one frozen model for multiple downstream tasks can ease this burden. Our method can be seen as a simplification of the recently proposed “prefix tuning” of Li and Liang (2021) and we provide a comparison to this and other similar approaches. Finally, we show that conditioning a frozen model with soft prompts confers benefits in robustness to domain transfer and enables efficient “prompt ensembling.” We release code and model checkpoints to reproduce our experiments.},
  urldate   = {2023-10-09},
  booktitle = {Proceedings of the 2021 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing}},
  publisher = {Association for Computational Linguistics},
  author    = {Lester, Brian and Al-Rfou, Rami and Constant, Noah},
  month     = nov,
  year      = {2021},
  pages     = {3045--3059}
}

@inproceedings{li_prefix-tuning_2021,
  address    = {Online},
  title      = {Prefix-{Tuning}: {Optimizing} {Continuous} {Prompts} for {Generation}},
  shorttitle = {Prefix-{Tuning}},
  url        = {https://aclanthology.org/2021.acl-long.353},
  doi        = {10.18653/v1/2021.acl-long.353},
  abstract   = {Fine-tuning is the de facto way of leveraging large pretrained language models for downstream tasks. However, fine-tuning modifies all the language model parameters and therefore necessitates storing a full copy for each task. In this paper, we propose prefix-tuning, a lightweight alternative to fine-tuning for natural language generation tasks, which keeps language model parameters frozen and instead optimizes a sequence of continuous task-specific vectors, which we call the prefix. Prefix-tuning draws inspiration from prompting for language models, allowing subsequent tokens to attend to this prefix as if it were “virtual tokens”. We apply prefix-tuning to GPT-2 for table-to-text generation and to BART for summarization. We show that by learning only 0.1\% of the parameters, prefix-tuning obtains comparable performance in the full data setting, outperforms fine-tuning in low-data settings, and extrapolates better to examples with topics that are unseen during training.},
  urldate    = {2023-10-09},
  booktitle  = {Proceedings of the 59th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics} and the 11th {International} {Joint} {Conference} on {Natural} {Language} {Processing} ({Volume} 1: {Long} {Papers})},
  publisher  = {Association for Computational Linguistics},
  author     = {Li, Xiang Lisa and Liang, Percy},
  month      = aug,
  year       = {2021},
  pages      = {4582--4597}
}


@inproceedings{devlin_bert_2019,
  address    = {Minneapolis, Minnesota},
  title      = {{BERT}: {Pre}-training of {Deep} {Bidirectional} {Transformers} for {Language} {Understanding}},
  shorttitle = {{BERT}},
  url        = {https://aclanthology.org/N19-1423},
  doi        = {10.18653/v1/N19-1423},
  abstract   = {We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5 (7.7 point absolute improvement), MultiNLI accuracy to 86.7\% (4.6\% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).},
  urldate    = {2023-10-08},
  booktitle  = {Proceedings of the 2019 {Conference} of the {North} {American} {Chapter} of the {Association} for {Computational} {Linguistics}: {Human} {Language} {Technologies}, {Volume} 1 ({Long} and {Short} {Papers})},
  publisher  = {Association for Computational Linguistics},
  author     = {Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  month      = jun,
  year       = {2019},
  pages      = {4171--4186}
}

@misc{liu_roberta_2019,
  title      = {{RoBERTa}: {A} {Robustly} {Optimized} {BERT} {Pretraining} {Approach}},
  shorttitle = {{RoBERTa}},
  url        = {http://arxiv.org/abs/1907.11692},
  doi        = {10.48550/arXiv.1907.11692},
  abstract   = {Language model pretraining has led to significant performance gains but careful comparison between different approaches is challenging. Training is computationally expensive, often done on private datasets of different sizes, and, as we will show, hyperparameter choices have significant impact on the final results. We present a replication study of BERT pretraining (Devlin et al., 2019) that carefully measures the impact of many key hyperparameters and training data size. We find that BERT was significantly undertrained, and can match or exceed the performance of every model published after it. Our best model achieves state-of-the-art results on GLUE, RACE and SQuAD. These results highlight the importance of previously overlooked design choices, and raise questions about the source of recently reported improvements. We release our models and code.},
  urldate    = {2023-10-08},
  publisher  = {arXiv},
  author     = {Liu, Yinhan and Ott, Myle and Goyal, Naman and Du, Jingfei and Joshi, Mandar and Chen, Danqi and Levy, Omer and Lewis, Mike and Zettlemoyer, Luke and Stoyanov, Veselin},
  month      = jul,
  year       = {2019},
  note       = {arXiv:1907.11692 [cs]},
  keywords   = {Computer Science - Computation and Language}
}

@inproceedings{parikh_exploring_2023,
  address   = {Toronto, Canada},
  title     = {Exploring {Zero} and {Few}-shot {Techniques} for {Intent} {Classification}},
  url       = {https://aclanthology.org/2023.acl-industry.71},
  abstract  = {Conversational NLU providers often need to scale to thousands of intent-classification models where new customers often face the cold-start problem. Scaling to so many customers puts a constraint on storage space as well. In this paper, we explore four different zero and few-shot intent classification approaches with this low-resource constraint: 1) domain adaptation, 2) data augmentation, 3) zero-shot intent classification using descriptions large language models (LLMs), and 4) parameter-efficient fine-tuning of instruction-finetuned language models. Our results show that all these approaches are effective to different degrees in low-resource settings. Parameter-efficient fine-tuning using T-few recipe on Flan-T5 yields the best performance even with just one sample per intent. We also show that the zero-shot method of prompting LLMs using intent descriptions is also very competitive.},
  urldate   = {2023-08-04},
  booktitle = {Proceedings of the 61st {Annual} {Meeting} of the {Association} for {Computational} {Linguistics} ({Volume} 5: {Industry} {Track})},
  publisher = {Association for Computational Linguistics},
  author    = {Parikh, Soham and Tiwari, Mitul and Tumbade, Prashil and Vohra, Quaizar},
  month     = jul,
  year      = {2023},
  pages     = {744--751}
}

@inproceedings{zhang_few-shot_2021,
  address   = {Online and Punta Cana, Dominican Republic},
  title     = {Few-{Shot} {Intent} {Detection} via {Contrastive} {Pre}-{Training} and {Fine}-{Tuning}},
  url       = {https://aclanthology.org/2021.emnlp-main.144},
  doi       = {10.18653/v1/2021.emnlp-main.144},
  abstract  = {In this work, we focus on a more challenging few-shot intent detection scenario where many intents are fine-grained and semantically similar. We present a simple yet effective few-shot intent detection schema via contrastive pre-training and fine-tuning. Specifically, we first conduct self-supervised contrastive pre-training on collected intent datasets, which implicitly learns to discriminate semantically similar utterances without using any labels. We then perform few-shot intent detection together with supervised contrastive learning, which explicitly pulls utterances from the same intent closer and pushes utterances across different intents farther. Experimental results show that our proposed method achieves state-of-the-art performance on three challenging intent detection datasets under 5-shot and 10-shot settings.},
  urldate   = {2023-09-29},
  booktitle = {Proceedings of the 2021 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing}},
  publisher = {Association for Computational Linguistics},
  author    = {Zhang, Jianguo and Bui, Trung and Yoon, Seunghyun and Chen, Xiang and Liu, Zhiwei and Xia, Congying and Tran, Quan Hung and Chang, Walter and Yu, Philip},
  month     = nov,
  year      = {2021},
  pages     = {1906--1912}
}

@inproceedings{yu_few-shot_2021,
  address   = {Online},
  title     = {Few-shot {Intent} {Classification} and {Slot} {Filling} with {Retrieved} {Examples}},
  url       = {https://aclanthology.org/2021.naacl-main.59},
  doi       = {10.18653/v1/2021.naacl-main.59},
  abstract  = {Few-shot learning arises in important practical scenarios, such as when a natural language understanding system needs to learn new semantic labels for an emerging, resource-scarce domain. In this paper, we explore retrieval-based methods for intent classification and slot filling tasks in few-shot settings. Retrieval-based methods make predictions based on labeled examples in the retrieval index that are similar to the input, and thus can adapt to new domains simply by changing the index without having to retrain the model. However, it is non-trivial to apply such methods on tasks with a complex label space like slot filling. To this end, we propose a span-level retrieval method that learns similar contextualized representations for spans with the same label via a novel batch-softmax objective. At inference time, we use the labels of the retrieved spans to construct the final structure with the highest aggregated score. Our method outperforms previous systems in various few-shot settings on the CLINC and SNIPS benchmarks.},
  urldate   = {2023-10-08},
  booktitle = {Proceedings of the 2021 {Conference} of the {North} {American} {Chapter} of the {Association} for {Computational} {Linguistics}: {Human} {Language} {Technologies}},
  publisher = {Association for Computational Linguistics},
  author    = {Yu, Dian and He, Luheng and Zhang, Yuan and Du, Xinya and Pasupat, Panupong and Li, Qi},
  month     = jun,
  year      = {2021},
  pages     = {734--749}
}

@inproceedings{krone_learning_2020,
  address   = {Online},
  title     = {Learning to {Classify} {Intents} and {Slot} {Labels} {Given} a {Handful} of {Examples}},
  url       = {https://aclanthology.org/2020.nlp4convai-1.12},
  doi       = {10.18653/v1/2020.nlp4convai-1.12},
  abstract  = {Intent classification (IC) and slot filling (SF) are core components in most goal-oriented dialogue systems. Current IC/SF models perform poorly when the number of training examples per class is small. We propose a new few-shot learning task, few-shot IC/SF, to study and improve the performance of IC and SF models on classes not seen at training time in ultra low resource scenarios. We establish a few-shot IC/SF benchmark by defining few-shot splits for three public IC/SF datasets, ATIS, TOP, and Snips. We show that two popular few-shot learning algorithms, model agnostic meta learning (MAML) and prototypical networks, outperform a fine-tuning baseline on this benchmark. Prototypical networks achieves significant gains in IC performance on the ATIS and TOP datasets, while both prototypical networks and MAML outperform the baseline with respect to SF on all three datasets. In addition, we demonstrate that joint training as well as the use of pre-trained language models, ELMo and BERT in our case, are complementary to these few-shot learning methods and yield further gains.},
  urldate   = {2023-10-08},
  booktitle = {Proceedings of the 2nd {Workshop} on {Natural} {Language} {Processing} for {Conversational} {AI}},
  publisher = {Association for Computational Linguistics},
  author    = {Krone, Jason and Zhang, Yi and Diab, Mona},
  month     = jul,
  year      = {2020},
  pages     = {96--108}
}

@inproceedings{schick_exploiting_2021,
  address   = {Online},
  title     = {Exploiting {Cloze}-{Questions} for {Few}-{Shot} {Text} {Classification} and {Natural} {Language} {Inference}},
  url       = {https://aclanthology.org/2021.eacl-main.20},
  doi       = {10.18653/v1/2021.eacl-main.20},
  abstract  = {Some NLP tasks can be solved in a fully unsupervised fashion by providing a pretrained language model with “task descriptions” in natural language (e.g., Radford et al., 2019). While this approach underperforms its supervised counterpart, we show in this work that the two ideas can be combined: We introduce Pattern-Exploiting Training (PET), a semi-supervised training procedure that reformulates input examples as cloze-style phrases to help language models understand a given task. These phrases are then used to assign soft labels to a large set of unlabeled examples. Finally, standard supervised training is performed on the resulting training set. For several tasks and languages, PET outperforms supervised training and strong semi-supervised approaches in low-resource settings by a large margin.},
  urldate   = {2023-10-08},
  booktitle = {Proceedings of the 16th {Conference} of the {European} {Chapter} of the {Association} for {Computational} {Linguistics}: {Main} {Volume}},
  publisher = {Association for Computational Linguistics},
  author    = {Schick, Timo and Schütze, Hinrich},
  month     = apr,
  year      = {2021},
  pages     = {255--269}
}

@inproceedings{ma_frustratingly_2021,
  address   = {Online},
  title     = {Frustratingly {Simple} {Few}-{Shot} {Slot} {Tagging}},
  url       = {https://aclanthology.org/2021.findings-acl.88},
  doi       = {10.18653/v1/2021.findings-acl.88},
  urldate   = {2023-10-08},
  booktitle = {Findings of the {Association} for {Computational} {Linguistics}: {ACL}-{IJCNLP} 2021},
  publisher = {Association for Computational Linguistics},
  author    = {Ma, Jianqiang and Yan, Zeyu and Li, Chang and Zhang, Yang},
  month     = aug,
  year      = {2021},
  pages     = {1028--1033}
}

@inproceedings{hou_learning_2021,
  address    = {Online},
  title      = {Learning to {Bridge} {Metric} {Spaces}: {Few}-shot {Joint} {Learning} of {Intent} {Detection} and {Slot} {Filling}},
  shorttitle = {Learning to {Bridge} {Metric} {Spaces}},
  url        = {https://aclanthology.org/2021.findings-acl.282},
  doi        = {10.18653/v1/2021.findings-acl.282},
  urldate    = {2023-10-08},
  booktitle  = {Findings of the {Association} for {Computational} {Linguistics}: {ACL}-{IJCNLP} 2021},
  publisher  = {Association for Computational Linguistics},
  author     = {Hou, Yutai and Lai, Yongkui and Chen, Cheng and Che, Wanxiang and Liu, Ting},
  month      = aug,
  year       = {2021},
  pages      = {3190--3200}
}

@inproceedings{cui_template-based_2021,
  address   = {Online},
  title     = {Template-{Based} {Named} {Entity} {Recognition} {Using} {BART}},
  url       = {https://aclanthology.org/2021.findings-acl.161},
  doi       = {10.18653/v1/2021.findings-acl.161},
  urldate   = {2023-10-08},
  booktitle = {Findings of the {Association} for {Computational} {Linguistics}: {ACL}-{IJCNLP} 2021},
  publisher = {Association for Computational Linguistics},
  author    = {Cui, Leyang and Wu, Yu and Liu, Jian and Yang, Sen and Zhang, Yue},
  month     = aug,
  year      = {2021},
  pages     = {1835--1845}
}


@inproceedings{kwon-etal-2023-sidlr,
  title     = {{SIDLR}: Slot and Intent Detection Models for Low-Resource Language Varieties},
  author    = {Kwon, Sang Yun  and
               Bhatia, Gagan  and
               Nagoudi, Elmoatez Billah  and
               Alcoba Inciarte, Alcides  and
               Abdul-mageed, Muhammad},
  booktitle = {Tenth Workshop on NLP for Similar Languages, Varieties and Dialects (VarDial 2023)},
  month     = may,
  year      = {2023},
  address   = {Dubrovnik, Croatia},
  publisher = {Association for Computational Linguistics},
  url       = {https://aclanthology.org/2023.vardial-1.24},
  doi       = {10.18653/v1/2023.vardial-1.24},
  pages     = {241--250},
  abstract  = {Intent detection and slot filling are two critical tasks in spoken and natural language understandingfor task-oriented dialog systems. In this work, we describe our participation in slot and intent detection for low-resource language varieties (SID4LR) (Aepli et al., 2023). We investigate the slot and intent detection (SID) tasks using a wide range of models and settings. Given the recent success of multitask promptedfinetuning of the large language models, we also test the generalization capability of the recent encoder-decoder model mT0 (Muennighoff et al., 2022) on new tasks (i.e., SID) in languages they have never intentionally seen. We show that our best model outperforms the baseline by a large margin (up to +30 F1 points) in both SID tasks.}
}


@misc{chang2023prompting,
  title         = {Prompting and Adapter Tuning for Self-supervised Encoder-Decoder Speech Model},
  author        = {Kai-Wei Chang and Ming-Hsin Chen and Yun-Ping Lin and Jing Neng Hsu and Paul Kuo-Ming Huang and Chien-yu Huang and Shang-Wen Li and Hung-yi Lee},
  year          = {2023},
  eprint        = {2310.02971},
  archiveprefix = {arXiv},
  primaryclass  = {eess.AS}
}

@inproceedings{wang_slot_2022,
  address   = {Gyeongju, Republic of Korea},
  title     = {Slot {Dependency} {Modeling} for {Zero}-{Shot} {Cross}-{Domain} {Dialogue} {State} {Tracking}},
  url       = {https://aclanthology.org/2022.coling-1.42},
  urldate   = {2023-08-04},
  booktitle = {Proceedings of the 29th {International} {Conference} on {Computational} {Linguistics}},
  publisher = {International Committee on Computational Linguistics},
  author    = {Wang, Qingyue and Cao, Yanan and Li, Piji and Fu, Yanhe and Lin, Zheng and Guo, Li},
  month     = oct,
  year      = {2022},
  pages     = {510--520}
}

@inproceedings{hu_-context_2022,
  address   = {Abu Dhabi, United Arab Emirates},
  title     = {In-{Context} {Learning} for {Few}-{Shot} {Dialogue} {State} {Tracking}},
  url       = {https://aclanthology.org/2022.findings-emnlp.193},
  doi       = {10.18653/v1/2022.findings-emnlp.193},
  abstract  = {Collecting and annotating task-oriented dialogues is time-consuming and costly. Thus, zero and few shot learning for dialogue tasks presents an exciting opportunity. In this work, we propose an in-context (IC) learning framework for zero-shot and few-shot learning dialogue state tracking (DST), where a large pretrained language model (LM) takes a test instance and a few exemplars as input, and directly decodes the dialogue state without any parameter updates. This approach is more flexible and scalable than prior DST work when adapting to new domains and scenarios. To better leverage a tabular domain description in the LM prompt, we reformulate DST into a text-to-SQL problem. We also propose a novel approach to retrieve annotated dialogues as exemplars. Empirical results on MultiWOZ show that our method IC-DST substantially outperforms previous fine-tuned state-of-the-art models in few-shot settings. In addition, we test IC-DST in zero-shot settings, in which the model only takes a fixed task instruction as input, finding that it outperforms previous zero-shot methods by a large margin.},
  urldate   = {2023-10-09},
  booktitle = {Findings of the {Association} for {Computational} {Linguistics}: {EMNLP} 2022},
  publisher = {Association for Computational Linguistics},
  author    = {Hu, Yushi and Lee, Chia-Hsuan and Xie, Tianbao and Yu, Tao and Smith, Noah A. and Ostendorf, Mari},
  month     = dec,
  year      = {2022},
  pages     = {2627--2643}
}

@inproceedings{gupta_show_2022,
  address    = {Seattle, United States},
  title      = {Show, {Don}'t {Tell}: {Demonstrations} {Outperform} {Descriptions} for {Schema}-{Guided} {Task}-{Oriented} {Dialogue}},
  shorttitle = {Show, {Don}'t {Tell}},
  url        = {https://aclanthology.org/2022.naacl-main.336},
  doi        = {10.18653/v1/2022.naacl-main.336},
  abstract   = {Building universal dialogue systems that operate across multiple domains/APIs and generalize to new ones with minimal overhead is a critical challenge. Recent works have leveraged natural language descriptions of schema elements to enable such systems; however, descriptions only indirectly convey schema semantics. In this work, we propose Show, Don't Tell, which prompts seq2seq models with a labeled example dialogue to show the semantics of schema elements rather than tell the model through descriptions. While requiring similar effort from service developers as generating descriptions, we show that using short examples as schema representations with large language models results in state-of-the-art performance on two popular dialogue state tracking benchmarks designed to measure zero-shot generalization - the Schema-Guided Dialogue dataset and the MultiWOZ leave-one-out benchmark.},
  urldate    = {2023-08-04},
  booktitle  = {Proceedings of the 2022 {Conference} of the {North} {American} {Chapter} of the {Association} for {Computational} {Linguistics}: {Human} {Language} {Technologies}},
  publisher  = {Association for Computational Linguistics},
  author     = {Gupta, Raghav and Lee, Harrison and Zhao, Jeffrey and Cao, Yuan and Rastogi, Abhinav and Wu, Yonghui},
  month      = jul,
  year       = {2022},
  pages      = {4541--4549}
}

@misc{hudecek_are_2023,
  title     = {Are {LLMs} {All} {You} {Need} for {Task}-{Oriented} {Dialogue}?},
  url       = {http://arxiv.org/abs/2304.06556},
  doi       = {10.48550/arXiv.2304.06556},
  abstract  = {Instructions-tuned Large Language Models (LLMs) gained recently huge popularity thanks to their ability to interact with users through conversation. In this work we aim to evaluate their ability to complete multi-turn tasks and interact with external databases in the context of established task-oriented dialogue benchmarks. We show that for explicit belief state tracking, LLMs underperform compared to specialized task-specific models. Nevertheless, they show ability to guide the dialogue to successful ending if given correct slot values. Furthermore this ability improves with access to true belief state distribution or in-domain examples.},
  urldate   = {2023-08-04},
  publisher = {arXiv},
  author    = {Hudeček, Vojtěch and Dušek, Ondřej},
  month     = aug,
  year      = {2023},
  note      = {arXiv:2304.06556 [cs]},
  keywords  = {Computer Science - Computation and Language}
}

@techreport{radford_language_2019,
  title       = {Language {Models} are {Unsupervised} {Multitask} {Learners}},
  abstract    = {Natural language processing tasks, such as question answering, machine translation, reading comprehension, and summarization, are typically approached with supervised learning on taskspeciﬁc datasets. We demonstrate that language models begin to learn these tasks without any explicit supervision when trained on a new dataset of millions of webpages called WebText. When conditioned on a document plus questions, the answers generated by the language model reach 55 F1 on the CoQA dataset - matching or exceeding the performance of 3 out of 4 baseline systems without using the 127,000+ training examples. The capacity of the language model is essential to the success of zero-shot task transfer and increasing it improves performance in a log-linear fashion across tasks. Our largest model, GPT-2, is a 1.5B parameter Transformer that achieves state of the art results on 7 out of 8 tested language modeling datasets in a zero-shot setting but still underﬁts WebText. Samples from the model reﬂect these improvements and contain coherent paragraphs of text. These ﬁndings suggest a promising path towards building language processing systems which learn to perform tasks from their naturally occurring demonstrations.},
  language    = {en},
  author      = {Radford, Alec and Wu, Jeffrey and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya},
  institution = {OpenAI},
  year        = {2019},
  url         = {https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf}
}


@misc{fuisz2022improved,
  title         = {Improved and Efficient Conversational Slot Labeling through Question Answering},
  author        = {Gabor Fuisz and Ivan Vulić and Samuel Gibbons and Inigo Casanueva and Paweł Budzianowski},
  year          = {2022},
  eprint        = {2204.02123},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL}
}


@misc{hung2022dstod,
  title         = {DS-TOD: Efficient Domain Specialization for Task Oriented Dialog},
  author        = {Chia-Chien Hung and Anne Lauscher and Simone Paolo Ponzetto and Goran Glavaš},
  year          = {2022},
  eprint        = {2110.08395},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL}
}

@inproceedings{Wang_2022,
  doi       = {10.1145/3477495.3531920},
  url       = {https://doi.org/10.1145%2F3477495.3531920},
  year      = 2022,
  month     = {jul},
  publisher = {{ACM}},
  author    = {Weizhi Wang and Zhirui Zhang and Junliang Guo and Yinpei Dai and Boxing Chen and Weihua Luo},
  title     = {Task-Oriented Dialogue System as Natural Language Generation},
  booktitle = {Proceedings of the 45th International {ACM} {SIGIR} Conference on Research and Development in Information Retrieval}
}


@misc{liu2022fewshot,
  title         = {Few-Shot Parameter-Efficient Fine-Tuning is Better and Cheaper than In-Context Learning},
  author        = {Haokun Liu and Derek Tam and Mohammed Muqeeth and Jay Mohta and Tenghao Huang and Mohit Bansal and Colin Raffel},
  year          = {2022},
  eprint        = {2205.05638},
  archiveprefix = {arXiv},
  primaryclass  = {cs.LG}
}


@misc{li2023robust,
  title         = {Robust Instruction Optimization for Large Language Models with Distribution Shifts},
  author        = {Moxin Li and Wenjie Wang and Fuli Feng and Jizhi Zhang and Tat-Seng Chua},
  year          = {2023},
  eprint        = {2305.13954},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL}
}


@misc{rawte_troubling_2023,
  title     = {The {Troubling} {Emergence} of {Hallucination} in {Large} {Language} {Models} -- {An} {Extensive} {Definition}, {Quantification}, and {Prescriptive} {Remediations}},
  url       = {http://arxiv.org/abs/2310.04988},
  doi       = {10.48550/arXiv.2310.04988},
  abstract  = {The recent advancements in Large Language Models (LLMs) have garnered widespread acclaim for their remarkable emerging capabilities. However, the issue of hallucination has parallelly emerged as a by-product, posing significant concerns. While some recent endeavors have been made to identify and mitigate different types of hallucination, there has been a limited emphasis on the nuanced categorization of hallucination and associated mitigation methods. To address this gap, we offer a fine-grained discourse on profiling hallucination based on its degree, orientation, and category, along with offering strategies for alleviation. As such, we define two overarching orientations of hallucination: (i) factual mirage (FM) and (ii) silver lining (SL). To provide a more comprehensive understanding, both orientations are further sub-categorized into intrinsic and extrinsic, with three degrees of severity - (i) mild, (ii) moderate, and (iii) alarming. We also meticulously categorize hallucination into six types: (i) acronym ambiguity, (ii) numeric nuisance, (iii) generated golem, (iv) virtual voice, (v) geographic erratum, and (vi) time wrap. Furthermore, we curate HallucInation eLiciTation (HILT), a publicly available dataset comprising of 75,000 samples generated using 15 contemporary LLMs along with human annotations for the aforementioned categories. Finally, to establish a method for quantifying and to offer a comparative spectrum that allows us to evaluate and rank LLMs based on their vulnerability to producing hallucinations, we propose Hallucination Vulnerability Index (HVI). We firmly believe that HVI holds significant value as a tool for the wider NLP community, with the potential to serve as a rubric in AI-related policy-making. In conclusion, we propose two solution strategies for mitigating hallucinations.},
  urldate   = {2023-10-19},
  publisher = {arXiv},
  author    = {Rawte, Vipula and Chakraborty, Swagata and Pathak, Agnibh and Sarkar, Anubhav and Tonmoy, S. M. Towhidul Islam and Chadha, Aman and Sheth, Amit P. and Das, Amitava},
  month     = oct,
  year      = {2023},
  note      = {arXiv:2310.04988 [cs]},
  keywords  = {Computer Science - Artificial Intelligence}
}

@inproceedings{su_multi-task_2022,
  address   = {Dublin, Ireland},
  title     = {Multi-{Task} {Pre}-{Training} for {Plug}-and-{Play} {Task}-{Oriented} {Dialogue} {System}},
  url       = {https://aclanthology.org/2022.acl-long.319},
  doi       = {10.18653/v1/2022.acl-long.319},
  abstract  = {Pre-trained language models have been recently shown to benefit task-oriented dialogue (TOD) systems. Despite their success, existing methods often formulate this task as a cascaded generation problem which can lead to error accumulation across different sub-tasks and greater data annotation overhead. In this study, we present PPTOD, a unified plug-and-play model for task-oriented dialogue. In addition, we introduce a new dialogue multi-task pre-training strategy that allows the model to learn the primary TOD task completion skills from heterogeneous dialog corpora. We extensively test our model on three benchmark TOD tasks, including end-to-end dialogue modelling, dialogue state tracking, and intent classification. Experimental results show that PPTOD achieves new state of the art on all evaluated tasks in both high-resource and low-resource scenarios. Furthermore, comparisons against previous SOTA methods show that the responses generated by PPTOD are more factually correct and semantically coherent as judged by human annotators.},
  urldate   = {2023-10-20},
  booktitle = {Proceedings of the 60th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics} ({Volume} 1: {Long} {Papers})},
  publisher = {Association for Computational Linguistics},
  author    = {Su, Yixuan and Shu, Lei and Mansimov, Elman and Gupta, Arshit and Cai, Deng and Lai, Yi-An and Zhang, Yi},
  month     = may,
  year      = {2022},
  pages     = {4661--4676}
}

@inproceedings{xie_unifiedskg_2022,
  address    = {Abu Dhabi, United Arab Emirates},
  title      = {{UnifiedSKG}: {Unifying} and {Multi}-{Tasking} {Structured} {Knowledge} {Grounding} with {Text}-to-{Text} {Language} {Models}},
  shorttitle = {{UnifiedSKG}},
  url        = {https://aclanthology.org/2022.emnlp-main.39},
  doi        = {10.18653/v1/2022.emnlp-main.39},
  abstract   = {Structured knowledge grounding (SKG) leverages structured knowledge to complete user requests, such as semantic parsing over databases and question answering over knowledge bases. Since the inputs and outputs of SKG tasks are heterogeneous, they have been studied separately by different communities, which limits systematic and compatible research on SKG. In this paper, we overcome this limitation by proposing the UnifiedSKG framework, which unifies 21 SKG tasks into a text-to-text format, aiming to promote systematic SKG research, instead of being exclusive to a single task, domain, or dataset. We use UnifiedSKG to benchmark T5 with different sizes and show that T5, with simple modifications when necessary, achieves state-of-the-art performance on almost all of the 21 tasks. We further demonstrate that multi-task prefix-tuning improves the performance on most tasks, largely improving the overall performance. UnifiedSKG also facilitates the investigation of zero-shot and few-shot learning, and we show that T0, GPT-3, and Codex struggle in zero-shot and few-shot learning for SKG. We also use UnifiedSKG to conduct a series of controlled experiments on structured knowledge encoding variants across SKG tasks. UnifiedSKG is easily extensible to more tasks, and it is open-sourced at https://github.com/hkunlp/unifiedskg.},
  urldate    = {2023-10-20},
  booktitle  = {Proceedings of the 2022 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing}},
  publisher  = {Association for Computational Linguistics},
  author     = {Xie, Tianbao and Wu, Chen Henry and Shi, Peng and Zhong, Ruiqi and Scholak, Torsten and Yasunaga, Michihiro and Wu, Chien-Sheng and Zhong, Ming and Yin, Pengcheng and Wang, Sida I. and Zhong, Victor and Wang, Bailin and Li, Chengzu and Boyle, Connor and Ni, Ansong and Yao, Ziyu and Radev, Dragomir and Xiong, Caiming and Kong, Lingpeng and Zhang, Rui and Smith, Noah A. and Zettlemoyer, Luke and Yu, Tao},
  month      = dec,
  year       = {2022},
  pages      = {602--631}
}

@misc{xue_mt5_2021,
  title      = {{mT5}: {A} massively multilingual pre-trained text-to-text transformer},
  shorttitle = {{mT5}},
  url        = {http://arxiv.org/abs/2010.11934},
  abstract   = {The recent "Text-to-Text Transfer Transformer" (T5) leveraged a unified text-to-text format and scale to attain state-of-the-art results on a wide variety of English-language NLP tasks. In this paper, we introduce mT5, a multilingual variant of T5 that was pre-trained on a new Common Crawl-based dataset covering 101 languages. We detail the design and modified training of mT5 and demonstrate its state-of-the-art performance on many multilingual benchmarks. We also describe a simple technique to prevent "accidental translation" in the zero-shot setting, where a generative model chooses to (partially) translate its prediction into the wrong language. All of the code and model checkpoints used in this work are publicly available.},
  urldate    = {2024-03-25},
  publisher  = {arXiv},
  author     = {Xue, Linting and Constant, Noah and Roberts, Adam and Kale, Mihir and Al-Rfou, Rami and Siddhant, Aditya and Barua, Aditya and Raffel, Colin},
  month      = mar,
  year       = {2021},
  doi        = {10.48550/arXiv.2010.11934},
  keywords   = {Computer Science - Computation and Language}
}


@languageresource{Speecon,
  author       = {{Speecon Consortium}},
  title        = {Catalan Speecon database},
  publisher    = {Speecon Project, distributed via ELRA: ELRA-Id S0327},
  year         = {2011},
  series       = {Speecon resources},
  edition      = {1.0},
  islrn        = {935-211-147-357-5},
  organization = {SpeeCon}
}

@languageresource{EMILLE,
  author       = {{Anthony McEnery and others}},
  title        = {The EMILLE/CIIL Corpus},
  publisher    = {distributed via ELRA: ELRA-Id W0037},
  year         = {2004},
  islrn        = {039-846-040-604-0},
  organization = {EMILLE (Enabling Minority Language Engineering) Project}
}

@languageresource{Orientel,
  author    = {{Khalid Choukri and Niklas Paullson}},
  islrn     = {613-578-868-832-2},
  publisher = {distributed via ELRA: ELRA-Id ELRA-S0183},
  title     = {The OrienTel Moroccan MCA (Modern Colloquial Arabic) database},
  year      = {2004}
}

@languageresource{bigscience/xP3,
  title     = {xP3: Crosslingual generalization through multitask finetuning},
  author    = {Muennighoff, Niklas and Wang, Thomas and Sutawika, Lintang and Roberts, Adam and Biderman, Stella and Scao, Teven Le and Bari, M Saiful and Shen, Sheng and Yong, Zheng-Xin and Schoelkopf, Hailey and others},
  publisher = {BigScience, distributed via Hugging Face},
  url       = {https://huggingface.co/datasets/bigscience/xP3},
  year      = {2022}
}

@languageresource{longpre_flan_2023,
  title      = {The {Flan} {Collection}: {Designing} {Data} and {Methods} for {Effective} {Instruction} {Tuning}},
  copyright  = {Creative Commons Attribution 4.0 International},
  shorttitle = {The {Flan} {Collection}},
  url        = {https://arxiv.org/abs/2301.13688},
  doi        = {10.48550/ARXIV.2301.13688},
  author     = {Longpre, Shayne and Hou, Le and Vu, Tu and Webson, Albert and Chung, Hyung Won and Tay, Yi and Zhou, Denny and Le, Quoc V. and Zoph, Barret and Wei, Jason and Roberts, Adam},
  year       = {2023}
}

@languageresource{xu2023wizardlm,
  title     = {Wizardlm: Empowering large language models to follow complex instructions},
  author    = {Xu, Can and Sun, Qingfeng and Zheng, Kai and Geng, Xiubo and Zhao, Pu and Feng, Jiazhan and Tao, Chongyang and Jiang, Daxin},
  publisher = {WizardLM, distributed via Hugging Face},
  url       = {https://huggingface.co/datasets/WizardLM/WizardLM_evol_instruct_70k},
  year      = {2023}
}

@languageresource{penedo_refinedweb_2023,
  title      = {The {RefinedWeb} {Dataset} for {Falcon} {LLM}: {Outperforming} {Curated} {Corpora} with {Web} {Data}, and {Web} {Data} {Only}},
  shorttitle = {The {RefinedWeb} {Dataset} for {Falcon} {LLM}},
  url        = {http://arxiv.org/abs/2306.01116},
  doi        = {10.48550/arXiv.2306.01116},
  abstract   = {Large language models are commonly trained on a mixture of filtered web data and curated high-quality corpora, such as social media conversations, books, or technical papers. This curation process is believed to be necessary to produce performant models with broad zero-shot generalization abilities. However, as larger models requiring pretraining on trillions of tokens are considered, it is unclear how scalable is curation and whether we will run out of unique high-quality data soon. At variance with previous beliefs, we show that properly filtered and deduplicated web data alone can lead to powerful models; even significantly outperforming models from the state-of-the-art trained on The Pile. Despite extensive filtering, the high-quality data we extract from the web is still plentiful, and we are able to obtain five trillion tokens from CommonCrawl. We publicly release an extract of 600 billion tokens from our RefinedWeb dataset, and 1.3/7.5B parameters language models trained on it.},
  urldate    = {2023-09-29},
  publisher  = {arXiv},
  author     = {Penedo, Guilherme and Malartic, Quentin and Hesslow, Daniel and Cojocaru, Ruxandra and Cappelli, Alessandro and Alobeidli, Hamza and Pannier, Baptiste and Almazrouei, Ebtesam and Launay, Julien},
  month      = jun,
  year       = {2023},
  note       = {arXiv:2306.01116 [cs]},
  keywords   = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language}
}

@languageresource{xu_baize_2023,
  title      = {Baize: {An} {Open}-{Source} {Chat} {Model} with {Parameter}-{Efficient} {Tuning} on {Self}-{Chat} {Data}},
  shorttitle = {Baize},
  url        = {http://arxiv.org/abs/2304.01196},
  doi        = {10.48550/arXiv.2304.01196},
  abstract   = {Chat models, such as ChatGPT, have shown impressive capabilities and have been rapidly adopted across numerous domains. However, these models are only accessible through a restricted API, creating barriers for new research and progress in the field. We propose a pipeline that can automatically generate a high-quality multi-turn chat corpus by leveraging ChatGPT to engage in a conversation with itself. Subsequently, we employ parameter-efficient tuning to enhance LLaMA, an open-source large language model. The resulting model, named Baize, demonstrates good performance in multi-turn dialogues with guardrails that minimize potential risks. Furthermore, we propose a new technique called Self-Distill with Feedback, to further improve the performance of the Baize models with feedback from ChatGPT. The Baize models and data are released for research purposes only at https://github.com/project-baize/baize-chatbot. An online demo is also available at https://huggingface.co/spaces/project-baize/chat-with-baize.},
  urldate    = {2023-09-29},
  publisher  = {arXiv},
  author     = {Xu, Canwen and Guo, Daya and Duan, Nan and McAuley, Julian},
  month      = may,
  year       = {2023},
  note       = {arXiv:2304.01196 [cs]},
  keywords   = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language}
}

@article{Rafiepour_2023,
  title     = {CTRAN: CNN-Transformer-based network for natural language understanding},
  volume    = {126},
  issn      = {0952-1976},
  url       = {http://dx.doi.org/10.1016/j.engappai.2023.107013},
  doi       = {10.1016/j.engappai.2023.107013},
  journal   = {Engineering Applications of Artificial Intelligence},
  publisher = {Elsevier BV},
  author    = {Rafiepour, Mehrdad and Sartakhti, Javad Salimi},
  year      = {2023},
  month     = nov,
  pages     = {107013}
}


@languageresource{fitzgerald2022massive,
  title   = {Massive: A 1m-example multilingual natural language understanding dataset with 51 typologically-diverse languages},
  author  = {FitzGerald, Jack and Hench, Christopher and Peris, Charith and Mackie, Scott and Rottmann, Kay and Sanchez, Ana and Nash, Aaron and Urbach, Liam and Kakarala, Vishesh and Singh, Richa and others},
  journal = {arXiv preprint arXiv:2204.08582},
  year    = {2022}
}

@languageresource{budzianowski2018multiwoz,
  title   = {Multiwoz--a large-scale multi-domain wizard-of-oz dataset for task-oriented dialogue modelling},
  author  = {Budzianowski, Pawe{\l} and Wen, Tsung-Hsien and Tseng, Bo-Hsiang and Casanueva, Inigo and Ultes, Stefan and Ramadan, Osman and Ga{\v{s}}i{\'c}, Milica},
  journal = {arXiv preprint arXiv:1810.00278},
  year    = {2018}
}


@misc{xue_mt5_2021,
  title      = {{mT5}: {A} massively multilingual pre-trained text-to-text transformer},
  shorttitle = {{mT5}},
  url        = {http://arxiv.org/abs/2010.11934},
  abstract   = {The recent "Text-to-Text Transfer Transformer" (T5) leveraged a unified text-to-text format and scale to attain state-of-the-art results on a wide variety of English-language NLP tasks. In this paper, we introduce mT5, a multilingual variant of T5 that was pre-trained on a new Common Crawl-based dataset covering 101 languages. We detail the design and modified training of mT5 and demonstrate its state-of-the-art performance on many multilingual benchmarks. We also describe a simple technique to prevent "accidental translation" in the zero-shot setting, where a generative model chooses to (partially) translate its prediction into the wrong language. All of the code and model checkpoints used in this work are publicly available.},
  urldate    = {2024-03-25},
  publisher  = {arXiv},
  author     = {Xue, Linting and Constant, Noah and Roberts, Adam and Kale, Mihir and Al-Rfou, Rami and Siddhant, Aditya and Barua, Aditya and Raffel, Colin},
  month      = mar,
  year       = {2021},
  doi        = {10.48550/arXiv.2010.11934},
  keywords   = {Computer Science - Computation and Language}
}

@misc{xue_mt5_2021-1,
  title      = {{mT5}: {A} massively multilingual pre-trained text-to-text transformer},
  shorttitle = {{mT5}},
  url        = {http://arxiv.org/abs/2010.11934},
  doi        = {10.48550/arXiv.2010.11934},
  abstract   = {The recent "Text-to-Text Transfer Transformer" (T5) leveraged a unified text-to-text format and scale to attain state-of-the-art results on a wide variety of English-language NLP tasks. In this paper, we introduce mT5, a multilingual variant of T5 that was pre-trained on a new Common Crawl-based dataset covering 101 languages. We detail the design and modified training of mT5 and demonstrate its state-of-the-art performance on many multilingual benchmarks. We also describe a simple technique to prevent "accidental translation" in the zero-shot setting, where a generative model chooses to (partially) translate its prediction into the wrong language. All of the code and model checkpoints used in this work are publicly available.},
  urldate    = {2024-03-25},
  publisher  = {arXiv},
  author     = {Xue, Linting and Constant, Noah and Roberts, Adam and Kale, Mihir and Al-Rfou, Rami and Siddhant, Aditya and Barua, Aditya and Raffel, Colin},
  month      = mar,
  year       = {2021},
  note       = {arXiv:2010.11934 [cs]},
  keywords   = {Computer Science - Computation and Language}
}

@misc{xue_mt5_2021-2,
  title      = {{mT5}: {A} massively multilingual pre-trained text-to-text transformer},
  shorttitle = {{mT5}},
  url        = {http://arxiv.org/abs/2010.11934},
  doi        = {10.48550/arXiv.2010.11934},
  abstract   = {The recent "Text-to-Text Transfer Transformer" (T5) leveraged a unified text-to-text format and scale to attain state-of-the-art results on a wide variety of English-language NLP tasks. In this paper, we introduce mT5, a multilingual variant of T5 that was pre-trained on a new Common Crawl-based dataset covering 101 languages. We detail the design and modified training of mT5 and demonstrate its state-of-the-art performance on many multilingual benchmarks. We also describe a simple technique to prevent "accidental translation" in the zero-shot setting, where a generative model chooses to (partially) translate its prediction into the wrong language. All of the code and model checkpoints used in this work are publicly available.},
  urldate    = {2024-03-25},
  publisher  = {arXiv},
  author     = {Xue, Linting and Constant, Noah and Roberts, Adam and Kale, Mihir and Al-Rfou, Rami and Siddhant, Aditya and Barua, Aditya and Raffel, Colin},
  month      = mar,
  year       = {2021},
  note       = {arXiv:2010.11934 [cs]},
  keywords   = {Computer Science - Computation and Language}
}

@misc{bang_task-optimized_2023,
  title    = {Task-{Optimized} {Adapters} for an {End}-to-{End} {Task}-{Oriented} {Dialogue} {System}},
  url      = {https://arxiv.org/abs/2305.02468v3},
  abstract = {Task-Oriented Dialogue (TOD) systems are designed to carry out specific tasks by tracking dialogue states and generating appropriate responses to help users achieve defined goals. Recently, end-to-end dialogue models pre-trained based on large datasets have shown promising performance in the conversational system. However, they share the same parameters to train tasks of the dialogue system (NLU, DST, NLG), so debugging each task is challenging. Also, they require a lot of effort to fine-tune large parameters to create a task-oriented chatbot, making it difficult for non-experts to handle. Therefore, we intend to train relatively lightweight and fast models compared to PLM. In this paper, we propose an End-to-end TOD system with Task-Optimized Adapters which learn independently per task, adding only small number of parameters after fixed layers of pre-trained network. We also enhance the performance of the DST and NLG modules through reinforcement learning, overcoming the learning curve that has lacked at the adapter learning and enabling the natural and consistent response generation that is appropriate for the goal. Our method is a model-agnostic approach and does not require prompt-tuning as only input data without a prompt. As results of the experiment, our method shows competitive performance on the MultiWOZ benchmark compared to the existing end-to-end models. In particular, we attain state-of-the-art performance on the DST task of 2.2 dataset.},
  language = {en},
  urldate  = {2023-10-30},
  journal  = {arXiv.org},
  author   = {Bang, Namo and Lee, Jeehyun and Koo, Myoung-Wan},
  month    = may,
  year     = {2023},
  keywords = {Important}
}

@misc{zhang_enhancing_2023,
  title     = {Enhancing {Performance} on {Seen} and {Unseen} {Dialogue} {Scenarios} using {Retrieval}-{Augmented} {End}-to-{End} {Task}-{Oriented} {System}},
  url       = {http://arxiv.org/abs/2308.08169},
  doi       = {10.48550/arXiv.2308.08169},
  abstract  = {End-to-end task-oriented dialogue (TOD) systems have achieved promising performance by leveraging sophisticated natural language understanding and natural language generation capabilities of pre-trained models. This work enables the TOD systems with more flexibility through a simple cache. The cache provides the flexibility to dynamically update the TOD systems and handle both existing and unseen dialogue scenarios. Towards this end, we first fine-tune a retrieval module to effectively retrieve the most relevant information entries from the cache. We then train end-to-end TOD models that can refer to and ground on both dialogue history and retrieved information during TOD generation. The cache is straightforward to construct, and the backbone models of TOD systems are compatible with existing pre-trained generative models. Extensive experiments demonstrate the superior performance of our framework, with a notable improvement in non-empty joint goal accuracy by 6.7\% compared to strong baselines.},
  urldate   = {2023-10-30},
  publisher = {arXiv},
  author    = {Zhang, Jianguo and Roller, Stephen and Qian, Kun and Liu, Zhiwei and Meng, Rui and Heinecke, Shelby and Wang, Huan and Savarese, Silvio and Xiong, Caiming},
  month     = aug,
  year      = {2023},
  note      = {arXiv:2308.08169 [cs]},
  keywords  = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language}
}

@misc{feng_towards_2023,
  title     = {Towards {LLM}-driven {Dialogue} {State} {Tracking}},
  url       = {http://arxiv.org/abs/2310.14970},
  doi       = {10.48550/arXiv.2310.14970},
  abstract  = {Dialogue State Tracking (DST) is of paramount importance in ensuring accurate tracking of user goals and system actions within task-oriented dialogue systems. The emergence of large language models (LLMs) such as GPT3 and ChatGPT has sparked considerable interest in assessing their efficacy across diverse applications. In this study, we conduct an initial examination of ChatGPT's capabilities in DST. Our evaluation uncovers the exceptional performance of ChatGPT in this task, offering valuable insights to researchers regarding its capabilities and providing useful directions for designing and enhancing dialogue systems. Despite its impressive performance, ChatGPT has significant limitations including its closed-source nature, request restrictions, raising data privacy concerns, and lacking local deployment capabilities. To address these concerns, we present LDST, an LLM-driven DST framework based on smaller, open-source foundation models. By utilizing a novel domain-slot instruction tuning method, LDST achieves performance on par with ChatGPT. Comprehensive evaluations across three distinct experimental settings, we find that LDST exhibits remarkable performance improvements in both zero-shot and few-shot setting compared to previous SOTA methods. The source code is provided for reproducibility.},
  urldate   = {2023-10-30},
  publisher = {arXiv},
  author    = {Feng, Yujie and Lu, Zexin and Liu, Bo and Zhan, Liming and Wu, Xiao-Ming},
  month     = oct,
  year      = {2023},
  note      = {arXiv:2310.14970 [cs]},
  keywords  = {Computer Science - Computation and Language}
}

@inproceedings{gupta_show_2022,
  address    = {Seattle, United States},
  title      = {Show, {Don}'t {Tell}: {Demonstrations} {Outperform} {Descriptions} for {Schema}-{Guided} {Task}-{Oriented} {Dialogue}},
  shorttitle = {Show, {Don}'t {Tell}},
  url        = {https://aclanthology.org/2022.naacl-main.336},
  doi        = {10.18653/v1/2022.naacl-main.336},
  abstract   = {Building universal dialogue systems that operate across multiple domains/APIs and generalize to new ones with minimal overhead is a critical challenge. Recent works have leveraged natural language descriptions of schema elements to enable such systems; however, descriptions only indirectly convey schema semantics. In this work, we propose Show, Don't Tell, which prompts seq2seq models with a labeled example dialogue to show the semantics of schema elements rather than tell the model through descriptions. While requiring similar effort from service developers as generating descriptions, we show that using short examples as schema representations with large language models results in state-of-the-art performance on two popular dialogue state tracking benchmarks designed to measure zero-shot generalization - the Schema-Guided Dialogue dataset and the MultiWOZ leave-one-out benchmark.},
  urldate    = {2023-10-30},
  booktitle  = {Proceedings of the 2022 {Conference} of the {North} {American} {Chapter} of the {Association} for {Computational} {Linguistics}: {Human} {Language} {Technologies}},
  publisher  = {Association for Computational Linguistics},
  author     = {Gupta, Raghav and Lee, Harrison and Zhao, Jeffrey and Cao, Yuan and Rastogi, Abhinav and Wu, Yonghui},
  month      = jul,
  year       = {2022},
  keywords   = {Important},
  pages      = {4541--4549}
}

@misc{feng_fantastic_2023,
  title      = {Fantastic {Rewards} and {How} to {Tame} {Them}: {A} {Case} {Study} on {Reward} {Learning} for {Task}-oriented {Dialogue} {Systems}},
  shorttitle = {Fantastic {Rewards} and {How} to {Tame} {Them}},
  url        = {https://arxiv.org/abs/2302.10342v1},
  abstract   = {When learning task-oriented dialogue (ToD) agents, reinforcement learning (RL) techniques can naturally be utilized to train dialogue strategies to achieve user-specific goals. Prior works mainly focus on adopting advanced RL techniques to train the ToD agents, while the design of the reward function is not well studied. This paper aims at answering the question of how to efficiently learn and leverage a reward function for training end-to-end (E2E) ToD agents. Specifically, we introduce two generalized objectives for reward-function learning, inspired by the classical learning-to-rank literature. Further, we utilize the learned reward function to guide the training of the E2E ToD agent. With the proposed techniques, we achieve competitive results on the E2E response-generation task on the Multiwoz 2.0 dataset. Source code and checkpoints are publicly released at https://github.com/Shentao-YANG/Fantastic\_Reward\_ICLR2023.},
  language   = {en},
  urldate    = {2023-10-30},
  journal    = {arXiv.org},
  author     = {Feng, Yihao and Yang, Shentao and Zhang, Shujian and Zhang, Jianguo and Xiong, Caiming and Zhou, Mingyuan and Wang, Huan},
  month      = feb,
  year       = {2023}
}

@article{qin_modularized_2023,
  title    = {Modularized {Pre}-{Training} for {End}-to-{End} {Task}-{Oriented} {Dialogue}},
  volume   = {31},
  issn     = {2329-9304},
  url      = {https://ieeexplore.ieee.org/abstract/document/10043710},
  doi      = {10.1109/TASLP.2023.3244503},
  abstract = {Pre-training for end-to-end task-oriented dialogue systems (EToDs) is a challenging task due to its unique knowledge base query (accuracy) need and lack of sufficient training data (fluency). In this paper, we try to mitigate the above challenges by introducing a modularized pre-training framework for EToDs, which achieves to effectively improve both accuracy and fluency of EToDs through a pre-training paradigm. The core insight is a modular design by decomposing EToDs into a generation (fluency) module and a knowledge-retriever (accuracy) module, which allows us to optimize each module by pre-training these two sub-modules with different well-designed pre-training tasks, respectively. In addition, such a modularized paradigm enables us to make full use of large amounts of KB-free dialogue corpus for the pre-training generation module, which can alleviate the insufficient training problem. Furthermore, we introduce a new consistency-guided data augmentation (CGDA) strategy to cope with the data scarcity problem to better pre-train the knowledge-retriever module. Finally, we fine-tune the pre-trained generation module and knowledge-retriever module jointly. Experimental results on three datasets show that our model achieve superior performance in terms of both fluency and accuracy. To our knowledge, this is the first work to explore modularized pre-training methods for EToDs.},
  urldate  = {2023-10-30},
  journal  = {IEEE/ACM Transactions on Audio, Speech, and Language Processing},
  author   = {Qin, Libo and Xu, Xiao and Wang, Lehan and Zhang, Yue and Che, Wanxiang},
  year     = {2023},
  note     = {Conference Name: IEEE/ACM Transactions on Audio, Speech, and Language Processing},
  pages    = {1601--1610}
}

@article{chen_opal_2023,
  title      = {{OPAL}: {Ontology}-{Aware} {Pretrained} {Language} {Model} for {End}-to-{End} {Task}-{Oriented} {Dialogue}},
  volume     = {11},
  shorttitle = {{OPAL}},
  url        = {https://aclanthology.org/2023.tacl-1.5},
  doi        = {10.1162/tacl_a_00534},
  abstract   = {This paper presents an ontology-aware pretrained language model (OPAL) for end-to-end task-oriented dialogue (TOD). Unlike chit-chat dialogue models, task-oriented dialogue models fulfill at least two task-specific modules: Dialogue state tracker (DST) and response generator (RG). The dialogue state consists of the domain-slot-value triples, which are regarded as the user's constraints to search the domain-related databases. The large-scale task-oriented dialogue data with the annotated structured dialogue state usually are inaccessible. It prevents the development of the pretrained language model for the task-oriented dialogue. We propose a simple yet effective pretraining method to alleviate this problem, which consists of two pretraining phases. The first phase is to pretrain on large-scale contextual text data, where the structured information of the text is extracted by the information extracting tool. To bridge the gap between the pretraining method and downstream tasks, we design two pretraining tasks: ontology-like triple recovery and next-text generation, which simulates the DST and RG, respectively. The second phase is to fine-tune the pretrained model on the TOD data. The experimental results show that our proposed method achieves an exciting boost and obtains competitive performance even without any TOD data on CamRest676 and MultiWOZ benchmarks.},
  urldate    = {2023-10-30},
  journal    = {Transactions of the Association for Computational Linguistics},
  author     = {Chen, Zhi and Liu, Yuncong and Chen, Lu and Zhu, Su and Wu, Mengyue and Yu, Kai},
  year       = {2023},
  note       = {Place: Cambridge, MA
                Publisher: MIT Press},
  pages      = {68--84}
}

@inproceedings{xie_unifiedskg_2022,
  address    = {Abu Dhabi, United Arab Emirates},
  title      = {{UnifiedSKG}: {Unifying} and {Multi}-{Tasking} {Structured} {Knowledge} {Grounding} with {Text}-to-{Text} {Language} {Models}},
  shorttitle = {{UnifiedSKG}},
  url        = {https://aclanthology.org/2022.emnlp-main.39},
  doi        = {10.18653/v1/2022.emnlp-main.39},
  abstract   = {Structured knowledge grounding (SKG) leverages structured knowledge to complete user requests, such as semantic parsing over databases and question answering over knowledge bases. Since the inputs and outputs of SKG tasks are heterogeneous, they have been studied separately by different communities, which limits systematic and compatible research on SKG. In this paper, we overcome this limitation by proposing the UnifiedSKG framework, which unifies 21 SKG tasks into a text-to-text format, aiming to promote systematic SKG research, instead of being exclusive to a single task, domain, or dataset. We use UnifiedSKG to benchmark T5 with different sizes and show that T5, with simple modifications when necessary, achieves state-of-the-art performance on almost all of the 21 tasks. We further demonstrate that multi-task prefix-tuning improves the performance on most tasks, largely improving the overall performance. UnifiedSKG also facilitates the investigation of zero-shot and few-shot learning, and we show that T0, GPT-3, and Codex struggle in zero-shot and few-shot learning for SKG. We also use UnifiedSKG to conduct a series of controlled experiments on structured knowledge encoding variants across SKG tasks. UnifiedSKG is easily extensible to more tasks, and it is open-sourced at https://github.com/hkunlp/unifiedskg.},
  urldate    = {2023-10-20},
  booktitle  = {Proceedings of the 2022 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing}},
  publisher  = {Association for Computational Linguistics},
  author     = {Xie, Tianbao and Wu, Chen Henry and Shi, Peng and Zhong, Ruiqi and Scholak, Torsten and Yasunaga, Michihiro and Wu, Chien-Sheng and Zhong, Ming and Yin, Pengcheng and Wang, Sida I. and Zhong, Victor and Wang, Bailin and Li, Chengzu and Boyle, Connor and Ni, Ansong and Yao, Ziyu and Radev, Dragomir and Xiong, Caiming and Kong, Lingpeng and Zhang, Rui and Smith, Noah A. and Zettlemoyer, Luke and Yu, Tao},
  month      = dec,
  year       = {2022},
  pages      = {602--631}
}

@inproceedings{su_multi-task_2022,
  address   = {Dublin, Ireland},
  title     = {Multi-{Task} {Pre}-{Training} for {Plug}-and-{Play} {Task}-{Oriented} {Dialogue} {System}},
  url       = {https://aclanthology.org/2022.acl-long.319},
  doi       = {10.18653/v1/2022.acl-long.319},
  abstract  = {Pre-trained language models have been recently shown to benefit task-oriented dialogue (TOD) systems. Despite their success, existing methods often formulate this task as a cascaded generation problem which can lead to error accumulation across different sub-tasks and greater data annotation overhead. In this study, we present PPTOD, a unified plug-and-play model for task-oriented dialogue. In addition, we introduce a new dialogue multi-task pre-training strategy that allows the model to learn the primary TOD task completion skills from heterogeneous dialog corpora. We extensively test our model on three benchmark TOD tasks, including end-to-end dialogue modelling, dialogue state tracking, and intent classification. Experimental results show that PPTOD achieves new state of the art on all evaluated tasks in both high-resource and low-resource scenarios. Furthermore, comparisons against previous SOTA methods show that the responses generated by PPTOD are more factually correct and semantically coherent as judged by human annotators.},
  urldate   = {2023-10-20},
  booktitle = {Proceedings of the 60th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics} ({Volume} 1: {Long} {Papers})},
  publisher = {Association for Computational Linguistics},
  author    = {Su, Yixuan and Shu, Lei and Mansimov, Elman and Gupta, Arshit and Cai, Deng and Lai, Yi-An and Zhang, Yi},
  month     = may,
  year      = {2022},
  pages     = {4661--4676}
}

@misc{rawte_troubling_2023,
  title     = {The {Troubling} {Emergence} of {Hallucination} in {Large} {Language} {Models} -- {An} {Extensive} {Definition}, {Quantification}, and {Prescriptive} {Remediations}},
  url       = {http://arxiv.org/abs/2310.04988},
  doi       = {10.48550/arXiv.2310.04988},
  abstract  = {The recent advancements in Large Language Models (LLMs) have garnered widespread acclaim for their remarkable emerging capabilities. However, the issue of hallucination has parallelly emerged as a by-product, posing significant concerns. While some recent endeavors have been made to identify and mitigate different types of hallucination, there has been a limited emphasis on the nuanced categorization of hallucination and associated mitigation methods. To address this gap, we offer a fine-grained discourse on profiling hallucination based on its degree, orientation, and category, along with offering strategies for alleviation. As such, we define two overarching orientations of hallucination: (i) factual mirage (FM) and (ii) silver lining (SL). To provide a more comprehensive understanding, both orientations are further sub-categorized into intrinsic and extrinsic, with three degrees of severity - (i) mild, (ii) moderate, and (iii) alarming. We also meticulously categorize hallucination into six types: (i) acronym ambiguity, (ii) numeric nuisance, (iii) generated golem, (iv) virtual voice, (v) geographic erratum, and (vi) time wrap. Furthermore, we curate HallucInation eLiciTation (HILT), a publicly available dataset comprising of 75,000 samples generated using 15 contemporary LLMs along with human annotations for the aforementioned categories. Finally, to establish a method for quantifying and to offer a comparative spectrum that allows us to evaluate and rank LLMs based on their vulnerability to producing hallucinations, we propose Hallucination Vulnerability Index (HVI). We firmly believe that HVI holds significant value as a tool for the wider NLP community, with the potential to serve as a rubric in AI-related policy-making. In conclusion, we propose two solution strategies for mitigating hallucinations.},
  urldate   = {2023-10-19},
  publisher = {arXiv},
  author    = {Rawte, Vipula and Chakraborty, Swagata and Pathak, Agnibh and Sarkar, Anubhav and Tonmoy, S. M. Towhidul Islam and Chadha, Aman and Sheth, Amit P. and Das, Amitava},
  month     = oct,
  year      = {2023},
  note      = {arXiv:2310.04988 [cs]},
  keywords  = {Computer Science - Artificial Intelligence}
}

@article{ouyang_training_2022,
  title    = {Training language models to follow instructions with human feedback},
  volume   = {35},
  url      = {https://proceedings.neurips.cc/paper_files/paper/2022/hash/b1efde53be364a73914f58805a001731-Abstract-Conference.html},
  language = {en},
  urldate  = {2023-10-09},
  journal  = {Advances in Neural Information Processing Systems},
  author   = {Ouyang, Long and Wu, Jeffrey and Jiang, Xu and Almeida, Diogo and Wainwright, Carroll and Mishkin, Pamela and Zhang, Chong and Agarwal, Sandhini and Slama, Katarina and Ray, Alex and Schulman, John and Hilton, Jacob and Kelton, Fraser and Miller, Luke and Simens, Maddie and Askell, Amanda and Welinder, Peter and Christiano, Paul F. and Leike, Jan and Lowe, Ryan},
  month    = dec,
  year     = {2022},
  pages    = {27730--27744}
}

@inproceedings{muennighoff_crosslingual_2023,
  address   = {Toronto, Canada},
  title     = {Crosslingual {Generalization} through {Multitask} {Finetuning}},
  url       = {https://aclanthology.org/2023.acl-long.891},
  doi       = {10.18653/v1/2023.acl-long.891},
  abstract  = {Multitask prompted finetuning (MTF) has been shown to help large language models generalize to new tasks in a zero-shot setting, but so far explorations of MTF have focused on English data and models. We apply MTF to the pretrained multilingual BLOOM and mT5 model families to produce finetuned variants called BLOOMZ and mT0. We find finetuning large multilingual language models on English tasks with English prompts allows for task genrealization to non-English languages that appear only in the pretraining corpus. Finetuning on multilingual tasks with English prompts further improves performance on English and non-English tasks leading to various state-of-the-art zero-shot results. We also investigate finetuning on multilingual tasks with prompts that have been machine-translated from English to match the language of each dataset. We find training on these machine-translated prompts leads to better performance on human-written prompts in the respective languages. Surprisingly, we find models are capable of zero-shot generalization to tasks in languages they have never intentionally seen. We conjecture that the models are learning higher-level capabilities that are both task- and language-agnostic. In addition, we introduce xP3, a composite of supervised datasets in 46 languages with English and machine-translated prompts. Our code, datasets and models are freely available at https://github.com/bigscience-workshop/xmtf.},
  urldate   = {2023-10-09},
  booktitle = {Proceedings of the 61st {Annual} {Meeting} of the {Association} for {Computational} {Linguistics} ({Volume} 1: {Long} {Papers})},
  publisher = {Association for Computational Linguistics},
  author    = {Muennighoff, Niklas and Wang, Thomas and Sutawika, Lintang and Roberts, Adam and Biderman, Stella and Le Scao, Teven and Bari, M Saiful and Shen, Sheng and Yong, Zheng Xin and Schoelkopf, Hailey and Tang, Xiangru and Radev, Dragomir and Aji, Alham Fikri and Almubarak, Khalid and Albanie, Samuel and Alyafeai, Zaid and Webson, Albert and Raff, Edward and Raffel, Colin},
  month     = jul,
  year      = {2023},
  pages     = {15991--16111}
}

@misc{zheng_judging_2023,
  title    = {Judging {LLM}-as-a-judge with {MT}-{Bench} and {Chatbot} {Arena}},
  url      = {https://arxiv.org/abs/2306.05685v2},
  abstract = {Evaluating large language model (LLM) based chat assistants is challenging due to their broad capabilities and the inadequacy of existing benchmarks in measuring human preferences. To address this, we explore using strong LLMs as judges to evaluate these models on more open-ended questions. We examine the usage and limitations of LLM-as-a-judge, including position, verbosity, and self-enhancement biases, as well as limited reasoning ability, and propose solutions to mitigate some of them. We then verify the agreement between LLM judges and human preferences by introducing two benchmarks: MT-bench, a multi-turn question set; and Chatbot Arena, a crowdsourced battle platform. Our results reveal that strong LLM judges like GPT-4 can match both controlled and crowdsourced human preferences well, achieving over 80{\textbackslash}\% agreement, the same level of agreement between humans. Hence, LLM-as-a-judge is a scalable and explainable way to approximate human preferences, which are otherwise very expensive to obtain. Additionally, we show our benchmark and traditional benchmarks complement each other by evaluating several variants of LLaMA and Vicuna. We will publicly release MT-bench questions, 3K expert votes, and 30K conversations with human preferences from Chatbot Arena.},
  language = {en},
  urldate  = {2023-10-09},
  journal  = {arXiv.org},
  author   = {Zheng, Lianmin and Chiang, Wei-Lin and Sheng, Ying and Zhuang, Siyuan and Wu, Zhanghao and Zhuang, Yonghao and Lin, Zi and Li, Zhuohan and Li, Dacheng and Xing, Eric P. and Zhang, Hao and Gonzalez, Joseph E. and Stoica, Ion},
  month    = jun,
  year     = {2023}
}

@misc{wang_task-oriented_2021,
  title    = {Task-{Oriented} {Dialogue} {System} as {Natural} {Language} {Generation}},
  url      = {https://arxiv.org/abs/2108.13679v3},
  abstract = {In this paper, we propose to formulate the task-oriented dialogue system as the purely natural language generation task, so as to fully leverage the large-scale pre-trained models like GPT-2 and simplify complicated delexicalization prepossessing. However, directly applying this method heavily suffers from the dialogue entity inconsistency caused by the removal of delexicalized tokens, as well as the catastrophic forgetting problem of the pre-trained model during fine-tuning, leading to unsatisfactory performance. To alleviate these problems, we design a novel GPT-Adapter-CopyNet network, which incorporates the lightweight adapter and CopyNet modules into GPT-2 to achieve better performance on transfer learning and dialogue entity generation. Experimental results conducted on the DSTC8 Track 1 benchmark and MultiWOZ dataset demonstrate that our proposed approach significantly outperforms baseline models with a remarkable performance on automatic and human evaluations.},
  language = {en},
  urldate  = {2023-10-09},
  journal  = {arXiv.org},
  author   = {Wang, Weizhi and Zhang, Zhirui and Guo, Junliang and Dai, Yinpei and Chen, Boxing and Luo, Weihua},
  month    = aug,
  year     = {2021},
  doi      = {10.1145/3477495.3531920}
}

@misc{hung_ds-tod_2021,
  title      = {{DS}-{TOD}: {Efficient} {Domain} {Specialization} for {Task} {Oriented} {Dialog}},
  shorttitle = {{DS}-{TOD}},
  url        = {https://arxiv.org/abs/2110.08395v2},
  abstract   = {Recent work has shown that self-supervised dialog-specific pretraining on large conversational datasets yields substantial gains over traditional language modeling (LM) pretraining in downstream task-oriented dialog (TOD). These approaches, however, exploit general dialogic corpora (e.g., Reddit) and thus presumably fail to reliably embed domain-specific knowledge useful for concrete downstream TOD domains. In this work, we investigate the effects of domain specialization of pretrained language models (PLMs) for TOD. Within our DS-TOD framework, we first automatically extract salient domain-specific terms, and then use them to construct DomainCC and DomainReddit -- resources that we leverage for domain-specific pretraining, based on (i) masked language modeling (MLM) and (ii) response selection (RS) objectives, respectively. We further propose a resource-efficient and modular domain specialization by means of domain adapters -- additional parameter-light layers in which we encode the domain knowledge. Our experiments with prominent TOD tasks -- dialog state tracking (DST) and response retrieval (RR) -- encompassing five domains from the MultiWOZ benchmark demonstrate the effectiveness of DS-TOD. Moreover, we show that the light-weight adapter-based specialization (1) performs comparably to full fine-tuning in single domain setups and (2) is particularly suitable for multi-domain specialization, where besides advantageous computational footprint, it can offer better TOD performance.},
  language   = {en},
  urldate    = {2023-10-09},
  journal    = {arXiv.org},
  author     = {Hung, Chia-Chien and Lauscher, Anne and Ponzetto, Simone Paolo and Glavaš, Goran},
  month      = oct,
  year       = {2021}
}

@inproceedings{li_prefix-tuning_2021,
  address    = {Online},
  title      = {Prefix-{Tuning}: {Optimizing} {Continuous} {Prompts} for {Generation}},
  shorttitle = {Prefix-{Tuning}},
  url        = {https://aclanthology.org/2021.acl-long.353},
  doi        = {10.18653/v1/2021.acl-long.353},
  abstract   = {Fine-tuning is the de facto way of leveraging large pretrained language models for downstream tasks. However, fine-tuning modifies all the language model parameters and therefore necessitates storing a full copy for each task. In this paper, we propose prefix-tuning, a lightweight alternative to fine-tuning for natural language generation tasks, which keeps language model parameters frozen and instead optimizes a sequence of continuous task-specific vectors, which we call the prefix. Prefix-tuning draws inspiration from prompting for language models, allowing subsequent tokens to attend to this prefix as if it were “virtual tokens”. We apply prefix-tuning to GPT-2 for table-to-text generation and to BART for summarization. We show that by learning only 0.1\% of the parameters, prefix-tuning obtains comparable performance in the full data setting, outperforms fine-tuning in low-data settings, and extrapolates better to examples with topics that are unseen during training.},
  urldate    = {2023-10-09},
  booktitle  = {Proceedings of the 59th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics} and the 11th {International} {Joint} {Conference} on {Natural} {Language} {Processing} ({Volume} 1: {Long} {Papers})},
  publisher  = {Association for Computational Linguistics},
  author     = {Li, Xiang Lisa and Liang, Percy},
  month      = aug,
  year       = {2021},
  pages      = {4582--4597}
}

@inproceedings{lester_power_2021,
  address   = {Online and Punta Cana, Dominican Republic},
  title     = {The {Power} of {Scale} for {Parameter}-{Efficient} {Prompt} {Tuning}},
  url       = {https://aclanthology.org/2021.emnlp-main.243},
  doi       = {10.18653/v1/2021.emnlp-main.243},
  abstract  = {In this work, we explore “prompt tuning,” a simple yet effective mechanism for learning “soft prompts” to condition frozen language models to perform specific downstream tasks. Unlike the discrete text prompts used by GPT-3, soft prompts are learned through backpropagation and can be tuned to incorporate signals from any number of labeled examples. Our end-to-end learned approach outperforms GPT-3's few-shot learning by a large margin. More remarkably, through ablations on model size using T5, we show that prompt tuning becomes more competitive with scale: as models exceed billions of parameters, our method “closes the gap” and matches the strong performance of model tuning (where all model weights are tuned). This finding is especially relevant because large models are costly to share and serve and the ability to reuse one frozen model for multiple downstream tasks can ease this burden. Our method can be seen as a simplification of the recently proposed “prefix tuning” of Li and Liang (2021) and we provide a comparison to this and other similar approaches. Finally, we show that conditioning a frozen model with soft prompts confers benefits in robustness to domain transfer and enables efficient “prompt ensembling.” We release code and model checkpoints to reproduce our experiments.},
  urldate   = {2023-10-09},
  booktitle = {Proceedings of the 2021 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing}},
  publisher = {Association for Computational Linguistics},
  author    = {Lester, Brian and Al-Rfou, Rami and Constant, Noah},
  month     = nov,
  year      = {2021},
  pages     = {3045--3059}
}

@inproceedings{hu_lora_2021,
  title      = {{LoRA}: {Low}-{Rank} {Adaptation} of {Large} {Language} {Models}},
  shorttitle = {{LoRA}},
  url        = {https://openreview.net/forum?id=nZeVKeeFYf9},
  abstract   = {An important paradigm of natural language processing consists of large-scale pre-training on general domain data and adaptation to particular tasks or domains. As we pre-train larger models, full fine-tuning, which retrains all model parameters, becomes less feasible. Using GPT-3 175B as an example -- deploying independent instances of fine-tuned models, each with 175B parameters, is prohibitively expensive. We propose Low-Rank Adaptation, or LoRA, which freezes the pre-trained model weights and injects trainable rank decomposition matrices into each layer of the Transformer architecture, greatly reducing the number of trainable parameters for downstream tasks. Compared to GPT-3 175B fine-tuned with Adam, LoRA can reduce the number of trainable parameters by a factor of 10,000 and the GPU memory requirement by a factor of 3. LoRA performs on-par or better than fine-tuning in model quality on RoBERTa, DeBERTa, GPT-2, and GPT-3, despite having fewer trainable parameters, a higher training throughput, and, unlike adapters, no additional inference latency. We also provide an empirical investigation into rank-deficiency in language model adaptation, which sheds light on the efficacy of LoRA. We release a package that facilitates the integration of LoRA with PyTorch models and provide our implementations and model checkpoints for RoBERTa, DeBERTa, and GPT-2 at https://github.com/microsoft/LoRA.},
  language   = {en},
  urldate    = {2023-10-09},
  author     = {Hu, Edward J. and Shen, Yelong and Wallis, Phillip and Allen-Zhu, Zeyuan and Li, Yuanzhi and Wang, Shean and Wang, Lu and Chen, Weizhu},
  month      = oct,
  year       = {2021}
}

@inproceedings{houlsby_parameter-efficient_2019,
  title     = {Parameter-{Efficient} {Transfer} {Learning} for {NLP}},
  url       = {https://proceedings.mlr.press/v97/houlsby19a.html},
  abstract  = {Fine-tuning large pretrained models is an effective transfer mechanism in NLP. However, in the presence of many downstream tasks, fine-tuning is parameter inefficient: an entire new model is required for every task. As an alternative, we propose transfer with adapter modules. Adapter modules yield a compact and extensible model; they add only a few trainable parameters per task, and new tasks can be added without revisiting previous ones. The parameters of the original network remain fixed, yielding a high degree of parameter sharing. To demonstrate adapter’s effectiveness, we transfer the recently proposed BERT Transformer model to 262626 diverse text classification tasks, including the GLUE benchmark. Adapters attain near state-of-the-art performance, whilst adding only a few parameters per task. On GLUE, we attain within 0.80.80.8\% of the performance of full fine-tuning, adding only 3.63.63.6\% parameters per task. By contrast, fine-tuning trains 100100100\% of the parameters per task.},
  language  = {en},
  urldate   = {2023-10-09},
  booktitle = {Proceedings of the 36th {International} {Conference} on {Machine} {Learning}},
  publisher = {PMLR},
  author    = {Houlsby, Neil and Giurgiu, Andrei and Jastrzebski, Stanislaw and Morrone, Bruna and Laroussilhe, Quentin De and Gesmundo, Andrea and Attariyan, Mona and Gelly, Sylvain},
  month     = may,
  year      = {2019},
  note      = {ISSN: 2640-3498},
  pages     = {2790--2799}
}

@misc{chen2019bert,
  title         = {BERT for Joint Intent Classification and Slot Filling},
  author        = {Qian Chen and Zhu Zhuo and Wen Wang},
  year          = {2019},
  eprint        = {1902.10909},
  archiveprefix = {arXiv},
  primaryclass  = {id='cs.CL' full_name='Computation and Language' is_active=True alt_name='cmp-lg' in_archive='cs' is_general=False description='Covers natural language processing. Roughly includes material in ACM Subject Class I.2.7. Note that work on artificial languages (programming languages, logics, formal systems) that does not explicitly address natural-language issues broadly construed (natural-language processing, computational linguistics, speech, text retrieval, etc.) is not appropriate for this area.'}
}

@inproceedings{Zhu2024,
  author    = {Zhu, Zhihong and Cheng, Xuxin and An, Hao and Wang, Zhichang and Chen, Dongsheng and Huang, Zhiqi},
  booktitle = {Proceedings of the 2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation (LREC-COLING 2024)},
  title     = {Zero-Shot Spoken Language Understanding via Large Language Models: A Preliminary Study},
  year      = {2024},
  address   = {Torino, Italia},
  editor    = {Calzolari, Nicoletta and Kan, Min-Yen and Hoste, Veronique and Lenci, Alessandro and Sakti, Sakriani and Xue, Nianwen},
  month     = may,
  pages     = {17877--17883},
  publisher = {ELRA and ICCL},
  abstract  = {Zero-shot Spoken Language Understanding (SLU) aims to enable task-oriented dialogue systems to understand user needs without training data. Challenging but worthwhile, zero-shot SLU reduces the time and effort that data labeling takes. Recent advancements in large language models (LLMs), such as GPT3.5 and ChatGPT, have shown promising results in zero-shot settings, which motivates us to explore prompt-based methods. In this study, we investigate whether strong SLU models can be constructed by directly prompting LLMs. Specifically, we propose a simple yet effective two-stage framework dubbed GPT-SLU, which transforms the SLU task into a question-answering problem. Powered by multi-stage mutual guided prompts, GPT-SLU can leverage the correlations between two subtasks in SLU to achieve better predictions, which is greatly explored in the traditional fine-tuning paradigm. Experimental results on three SLU benchmark datasets demonstrate the significant potential of LLMs for zero-shot SLU. Comprehensive analyses validate the effectiveness of our proposed framework and also indicate that there is still room for further improvement of LLMs in SLU scenarios.},
  url       = {https://aclanthology.org/2024.lrec-main.1554}
}



@article{radford_language_nodate,
  title    = {Language {Models} are {Unsupervised} {Multitask} {Learners}},
  abstract = {Natural language processing tasks, such as question answering, machine translation, reading comprehension, and summarization, are typically approached with supervised learning on taskspeciﬁc datasets. We demonstrate that language models begin to learn these tasks without any explicit supervision when trained on a new dataset of millions of webpages called WebText. When conditioned on a document plus questions, the answers generated by the language model reach 55 F1 on the CoQA dataset - matching or exceeding the performance of 3 out of 4 baseline systems without using the 127,000+ training examples. The capacity of the language model is essential to the success of zero-shot task transfer and increasing it improves performance in a log-linear fashion across tasks. Our largest model, GPT-2, is a 1.5B parameter Transformer that achieves state of the art results on 7 out of 8 tested language modeling datasets in a zero-shot setting but still underﬁts WebText. Samples from the model reﬂect these improvements and contain coherent paragraphs of text. These ﬁndings suggest a promising path towards building language processing systems which learn to perform tasks from their naturally occurring demonstrations.},
  language = {en},
  author   = {Radford, Alec and Wu, Jeffrey and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya}
}

@inproceedings{hu_-context_2022,
  address   = {Abu Dhabi, United Arab Emirates},
  title     = {In-{Context} {Learning} for {Few}-{Shot} {Dialogue} {State} {Tracking}},
  url       = {https://aclanthology.org/2022.findings-emnlp.193},
  doi       = {10.18653/v1/2022.findings-emnlp.193},
  abstract  = {Collecting and annotating task-oriented dialogues is time-consuming and costly. Thus, zero and few shot learning for dialogue tasks presents an exciting opportunity. In this work, we propose an in-context (IC) learning framework for zero-shot and few-shot learning dialogue state tracking (DST), where a large pretrained language model (LM) takes a test instance and a few exemplars as input, and directly decodes the dialogue state without any parameter updates. This approach is more flexible and scalable than prior DST work when adapting to new domains and scenarios. To better leverage a tabular domain description in the LM prompt, we reformulate DST into a text-to-SQL problem. We also propose a novel approach to retrieve annotated dialogues as exemplars. Empirical results on MultiWOZ show that our method IC-DST substantially outperforms previous fine-tuned state-of-the-art models in few-shot settings. In addition, we test IC-DST in zero-shot settings, in which the model only takes a fixed task instruction as input, finding that it outperforms previous zero-shot methods by a large margin.},
  urldate   = {2023-10-09},
  booktitle = {Findings of the {Association} for {Computational} {Linguistics}: {EMNLP} 2022},
  publisher = {Association for Computational Linguistics},
  author    = {Hu, Yushi and Lee, Chia-Hsuan and Xie, Tianbao and Yu, Tao and Smith, Noah A. and Ostendorf, Mari},
  month     = dec,
  year      = {2022},
  pages     = {2627--2643}
}

@misc{fuisz_improved_2022,
  title    = {Improved and {Efficient} {Conversational} {Slot} {Labeling} through {Question} {Answering}},
  url      = {https://arxiv.org/abs/2204.02123v1},
  abstract = {Transformer-based pretrained language models (PLMs) offer unmatched performance across the majority of natural language understanding (NLU) tasks, including a body of question answering (QA) tasks. We hypothesize that improvements in QA methodology can also be directly exploited in dialog NLU; however, dialog tasks must be {\textbackslash}textit\{reformatted\} into QA tasks. In particular, we focus on modeling and studying {\textbackslash}textit\{slot labeling\} (SL), a crucial component of NLU for dialog, through the QA optics, aiming to improve both its performance and efficiency, and make it more effective and resilient to working with limited task data. To this end, we make a series of contributions: 1) We demonstrate how QA-tuned PLMs can be applied to the SL task, reaching new state-of-the-art performance, with large gains especially pronounced in such low-data regimes. 2) We propose to leverage contextual information, required to tackle ambiguous values, simply through natural language. 3) Efficiency and compactness of QA-oriented fine-tuning are boosted through the use of lightweight yet effective adapter modules. 4) Trading-off some of the quality of QA datasets for their size, we experiment with larger automatically generated QA datasets for QA-tuning, arriving at even higher performance. Finally, our analysis suggests that our novel QA-based slot labeling models, supported by the PLMs, reach a performance ceiling in high-data regimes, calling for more challenging and more nuanced benchmarks in future work.},
  language = {en},
  urldate  = {2023-10-08},
  journal  = {arXiv.org},
  author   = {Fuisz, Gabor and Vulić, Ivan and Gibbons, Samuel and Casanueva, Inigo and Budzianowski, Paweł},
  month    = apr,
  year     = {2022}
}

@inproceedings{shafi_fine-tuned_2023,
  title     = {Fine-{Tuned} {BERT} with {Attention}-{Based} {Bi}-{GRU}-{CapsNet} {Framework} for {Joint} {Intent} {Recognition} and {Slot} {Filing}},
  url       = {https://ieeexplore.ieee.org/abstract/document/10141744?casa_token=l3NnhH4Ab6QAAAAA:FEneagq56UQwaY-V3Als9BFaJzLxdrLgxgBuQO6ZgjjAXwoVuslWFpkS3AeEHxds7bYUnBWjzwo},
  doi       = {10.1109/InCACCT57535.2023.10141744},
  abstract  = {In the field of natural language processing (NLP), the two most prominent research areas are slot tagging and intent recognition. Modern joint learning strategies examine the link between slot-tag identification and intent classification, utilising the shared knowledge between the two tasks for their collective advantage.However, methods that combine various variants of pre-trained Bidirectional Encoder Representations from Transformers [BERT] models with attention based capsule networks for joint slot-tag identification and intent detection have not been fully explored.This study proposes a multi-stage framework trained on different versions of BERT models ({\textbackslash}mathrmB{\textbackslash}mathrmE{\textbackslash}mathrmR{\textbackslash}mathrmT\_{\textbackslash}mathrmb{\textbackslash}mathrma{\textbackslash}mathrms{\textbackslash}mathrme and {\textbackslash}mathrmB{\textbackslash}mathrmE{\textbackslash}mathrmR{\textbackslash}mathrmT\_{\textbackslash}mathrml{\textbackslash}mathrma{\textbackslash}mathrmr{\textbackslash}mathrmg{\textbackslash}mathrme) with Bidirectional Gated Recurrent Unit [Bi-GRU] and self attention mechanism as intent detection decoder to capture the underlying information and to discover the explicit links.While the capsule network, in accordance with the dynamic routing algorithm, acts as a slot filler decoder in predicting intents and slots and representing the semantic and syntactic relationships. The experimental findings demonstrate that the proposed approach enhances semantic frame accuracy at the sentence level, outperforming various baseline methodologies by a significant margin with a 1.2\% improvement in the intent Flscore and 3.24\% in the slot Fl-score, relative to the previous state-of-the-art models on the SNIPS datasets.},
  urldate   = {2023-10-08},
  booktitle = {2023 {International} {Conference} on {Advancement} in {Computation} \& {Computer} {Technologies} ({InCACCT})},
  author    = {Shafi, Nahida and Chachoo, Manzoor Ahmed},
  month     = may,
  year      = {2023},
  pages     = {369--374}
}

@inproceedings{kwon_sidlr_2023,
  address    = {Dubrovnik, Croatia},
  title      = {{SIDLR}: {Slot} and {Intent} {Detection} {Models} for {Low}-{Resource} {Language} {Varieties}},
  shorttitle = {{SIDLR}},
  url        = {https://aclanthology.org/2023.vardial-1.24},
  doi        = {10.18653/v1/2023.vardial-1.24},
  abstract   = {Intent detection and slot filling are two critical tasks in spoken and natural language understandingfor task-oriented dialog systems. In this work, we describe our participation in slot and intent detection for low-resource language varieties (SID4LR) (Aepli et al., 2023). We investigate the slot and intent detection (SID) tasks using a wide range of models and settings. Given the recent success of multitask promptedfinetuning of the large language models, we also test the generalization capability of the recent encoder-decoder model mT0 (Muennighoff et al., 2022) on new tasks (i.e., SID) in languages they have never intentionally seen. We show that our best model outperforms the baseline by a large margin (up to +30 F1 points) in both SID tasks.},
  urldate    = {2023-10-08},
  booktitle  = {Tenth {Workshop} on {NLP} for {Similar} {Languages}, {Varieties} and {Dialects} ({VarDial} 2023)},
  publisher  = {Association for Computational Linguistics},
  author     = {Kwon, Sang Yun and Bhatia, Gagan and Nagoudi, Elmoatez Billah and Alcoba Inciarte, Alcides and Abdul-mageed, Muhammad},
  month      = may,
  year       = {2023},
  keywords   = {Important},
  pages      = {241--250}
}

@misc{chang_prompting_2023,
  title    = {Prompting and {Adapter} {Tuning} for {Self}-supervised {Encoder}-{Decoder} {Speech} {Model}},
  url      = {https://arxiv.org/abs/2310.02971v1},
  abstract = {Prompting and adapter tuning have emerged as efficient alternatives to fine-tuning (FT) methods. However, existing studies on speech prompting focused on classification tasks and failed on more complex sequence generation tasks. Besides, adapter tuning is primarily applied with a focus on encoder-only self-supervised models. Our experiments show that prompting on Wav2Seq, a self-supervised encoder-decoder model, surpasses previous works in sequence generation tasks. It achieves a remarkable 53\% relative improvement in word error rate for ASR and a 27\% in F1 score for slot filling. Additionally, prompting competes with the FT method in the low-resource scenario. Moreover, we show the transferability of prompting and adapter tuning on Wav2Seq in cross-lingual ASR. When limited trainable parameters are involved, prompting and adapter tuning consistently outperform conventional FT across 7 languages. Notably, in the low-resource scenario, prompting consistently outperforms adapter tuning.},
  language = {en},
  urldate  = {2023-10-08},
  journal  = {arXiv.org},
  author   = {Chang, Kai-Wei and Chen, Ming-Hsin and Lin, Yun-Ping and Hsu, Jing Neng and Huang, Paul Kuo-Ming and Huang, Chien-yu and Li, Shang-Wen and Lee, Hung-yi},
  month    = oct,
  year     = {2023}
}

@inproceedings{fitzgerald_alexa_2022,
  address    = {New York, NY, USA},
  series     = {{KDD} '22},
  title      = {Alexa {Teacher} {Model}: {Pretraining} and {Distilling} {Multi}-{Billion}-{Parameter} {Encoders} for {Natural} {Language} {Understanding} {Systems}},
  isbn       = {978-1-4503-9385-0},
  shorttitle = {Alexa {Teacher} {Model}},
  url        = {https://dl.acm.org/doi/10.1145/3534678.3539173},
  doi        = {10.1145/3534678.3539173},
  abstract   = {We present results from a large-scale experiment on pretraining encoders with non-embedding parameter counts ranging from 700M to 9.3B, their subsequent distillation into smaller models ranging from 17M-170M parameters, and their application to the Natural Language Understanding (NLU) component of a virtual assistant system. Though we train using 70\% spoken-form data, our teacher models perform comparably to XLM-R and mT5 when evaluated on the written-form Cross-lingual Natural Language Inference (XNLI) corpus. We perform a second stage of pretraining on our teacher models using in-domain data from our system, improving error rates by 3.86\% relative for intent classification and 7.01\% relative for slot filling. We find that even a 170M-parameter model distilled from our Stage 2 teacher model has 2.88\% better intent classification and 7.69\% better slot filling error rates when compared to the 2.3B-parameter teacher trained only on public data (Stage 1), emphasizing the importance of in-domain data for pretraining. When evaluated offline using labeled NLU data, our 17M-parameter Stage 2 distilled model outperforms both XLM-R Base (85M params) and DistillBERT (42M params) by 4.23\% to 6.14\%, respectively. Finally, we present results from a full virtual assistant experimentation platform, where we find that models trained using our pretraining and distillation pipeline outperform models distilled from 85M-parameter teachers by 3.74\%-4.91\% on an automatic measurement of full-system user dissatisfaction.},
  urldate    = {2023-10-08},
  booktitle  = {Proceedings of the 28th {ACM} {SIGKDD} {Conference} on {Knowledge} {Discovery} and {Data} {Mining}},
  publisher  = {Association for Computing Machinery},
  author     = {FitzGerald, Jack and Ananthakrishnan, Shankar and Arkoudas, Konstantine and Bernardi, Davide and Bhagia, Abhishek and Delli Bovi, Claudio and Cao, Jin and Chada, Rakesh and Chauhan, Amit and Chen, Luoxin and Dwarakanath, Anurag and Dwivedi, Satyam and Gojayev, Turan and Gopalakrishnan, Karthik and Gueudre, Thomas and Hakkani-Tur, Dilek and Hamza, Wael and Hüser, Jonathan J. and Jose, Kevin Martin and Khan, Haidar and Liu, Beiye and Lu, Jianhua and Manzotti, Alessandro and Natarajan, Pradeep and Owczarzak, Karolina and Oz, Gokmen and Palumbo, Enrico and Peris, Charith and Prakash, Chandana Satya and Rawls, Stephen and Rosenbaum, Andy and Shenoy, Anjali and Soltan, Saleh and Sridhar, Mukund Harakere and Tan, Lizhen and Triefenbach, Fabian and Wei, Pan and Yu, Haiyang and Zheng, Shuai and Tur, Gokhan and Natarajan, Prem},
  month      = aug,
  year       = {2022},
  keywords   = {distributed training, knowledge distillation, model pretraining, natural language understanding, self-attention, transformers, virtual assistant, voice a.i.},
  pages      = {2893--2902}
}

@article{han_ptr_2022,
  title      = {{PTR}: {Prompt} {Tuning} with {Rules} for {Text} {Classification}},
  volume     = {3},
  issn       = {2666-6510},
  shorttitle = {{PTR}},
  url        = {https://www.sciencedirect.com/science/article/pii/S2666651022000183},
  doi        = {10.1016/j.aiopen.2022.11.003},
  abstract   = {Recently, prompt tuning has been widely applied to stimulate the rich knowledge in pre-trained language models (PLMs) to serve NLP tasks. Although prompt tuning has achieved promising results on some few-class classification tasks, such as sentiment classification and natural language inference, manually designing prompts is cumbersome. Meanwhile, generating prompts automatically is also difficult and time-consuming. Therefore, obtaining effective prompts for complex many-class classification tasks still remains a challenge. In this paper, we propose to encode the prior knowledge of a classification task into rules, then design sub-prompts according to the rules, and finally combine the sub-prompts to handle the task. We name this Prompt Tuning method with Rules “PTR”. Compared with existing prompt-based methods, PTR achieves a good trade-off between effectiveness and efficiency in building prompts. We conduct experiments on three many-class classification tasks, including relation classification, entity typing, and intent classification. The results show that PTR outperforms both vanilla and prompt tuning baselines, indicating the effectiveness of utilizing rules for prompt tuning. The source code of PTR is available at https://github.com/thunlp/PTR.},
  urldate    = {2023-10-08},
  journal    = {AI Open},
  author     = {Han, Xu and Zhao, Weilin and Ding, Ning and Liu, Zhiyuan and Sun, Maosong},
  month      = jan,
  year       = {2022},
  keywords   = {Pre-trained language models, Prompt tuning},
  pages      = {182--192}
}

@misc{budzianowski_multiwoz_2020,
  title     = {{MultiWOZ} -- {A} {Large}-{Scale} {Multi}-{Domain} {Wizard}-of-{Oz} {Dataset} for {Task}-{Oriented} {Dialogue} {Modelling}},
  url       = {http://arxiv.org/abs/1810.00278},
  doi       = {10.48550/arXiv.1810.00278},
  abstract  = {Even though machine learning has become the major scene in dialogue research community, the real breakthrough has been blocked by the scale of data available. To address this fundamental obstacle, we introduce the Multi-Domain Wizard-of-Oz dataset (MultiWOZ), a fully-labeled collection of human-human written conversations spanning over multiple domains and topics. At a size of \$10\$k dialogues, it is at least one order of magnitude larger than all previous annotated task-oriented corpora. The contribution of this work apart from the open-sourced dataset labelled with dialogue belief states and dialogue actions is two-fold: firstly, a detailed description of the data collection procedure along with a summary of data structure and analysis is provided. The proposed data-collection pipeline is entirely based on crowd-sourcing without the need of hiring professional annotators; secondly, a set of benchmark results of belief tracking, dialogue act and response generation is reported, which shows the usability of the data and sets a baseline for future studies.},
  urldate   = {2023-10-08},
  publisher = {arXiv},
  author    = {Budzianowski, Paweł and Wen, Tsung-Hsien and Tseng, Bo-Hsiang and Casanueva, Iñigo and Ultes, Stefan and Ramadan, Osman and Gašić, Milica},
  month     = apr,
  year      = {2020},
  note      = {arXiv:1810.00278 [cs]},
  keywords  = {Computer Science - Computation and Language}
}

% and : and Teven Le Scao and Angela Fan and Christopher Akiki and Ellie Pavlick and Suzana Ilić and Daniel Hesslow and Roman Castagné and Alexandra Sasha Luccioni and François Yvon and Matthias Gallé and Jonathan Tow and Alexander M. Rush and Stella Biderman and Albert Webson and Pawan Sasanka Ammanamanchi and Thomas Wang and Benoît Sagot and Niklas Muennighoff and Albert Villanova del Moral and Olatunji Ruwase and Rachel Bawden and Stas Bekman and Angelina McMillan-Major and Iz Beltagy and Huu Nguyen and Lucile Saulnier and Samson Tan and Pedro Ortiz Suarez and Victor Sanh and Hugo Laurençon and Yacine Jernite and Julien Launay and Margaret Mitchell and Colin Raffel and Aaron Gokaslan and Adi Simhi and Aitor Soroa and Alham Fikri Aji and Amit Alfassy and Anna Rogers and Ariel Kreisberg Nitzav and Canwen Xu and Chenghao Mou and Chris Emezue and Christopher Klamm and Colin Leong and Daniel van Strien and David Ifeoluwa Adelani and Dragomir Radev and Eduardo González Ponferrada and Efrat Levkovizh and Ethan Kim and Eyal Bar Natan and Francesco De Toni and Gérard Dupont and Germán Kruszewski and Giada Pistilli and Hady Elsahar and Hamza Benyamina and Hieu Tran and Ian Yu and Idris Abdulmumin and Isaac Johnson and Itziar Gonzalez-Dios and Javier de la Rosa and Jenny Chim and Jesse Dodge and Jian Zhu and Jonathan Chang and Jörg Frohberg and Joseph Tobing and Joydeep Bhattacharjee and Khalid Almubarak and Kimbo Chen and Kyle Lo and Leandro Von Werra and Leon Weber and Long Phan and Loubna Ben allal and Ludovic Tanguy and Manan Dey and Manuel Romero Muñoz and Maraim Masoud and María Grandury and Mario Šaško and Max Huang and Maximin Coavoux and Mayank Singh and Mike Tian-Jian Jiang and Minh Chien Vu and Mohammad A. Jauhar and Mustafa Ghaleb and Nishant Subramani and Nora Kassner and Nurulaqilla Khamis and Olivier Nguyen and Omar Espejel and Ona de Gibert and Paulo Villegas and Peter Henderson and Pierre Colombo and Priscilla Amuok and Quentin Lhoest and Rheza Harliman and Rishi Bommasani and Roberto Luis López and Rui Ribeiro and Salomey Osei and Sampo Pyysalo and Sebastian Nagel and Shamik Bose and Shamsuddeen Hassan Muhammad and Shanya Sharma and Shayne Longpre and Somaieh Nikpoor and Stanislav Silberberg and Suhas Pai and Sydney Zink and Tiago Timponi Torrent and Timo Schick and Tristan Thrush and Valentin Danchev and Vassilina Nikoulina and Veronika Laippala and Violette Lepercq and Vrinda Prabhu and Zaid Alyafeai and Zeerak Talat and Arun Raja and Benjamin Heinzerling and Chenglei Si and Davut Emre Taşar and Elizabeth Salesky and Sabrina J. Mielke and Wilson Y. Lee and Abheesht Sharma and Andrea Santilli and Antoine Chaffin and Arnaud Stiegler and Debajyoti Datta and Eliza Szczechla and Gunjan Chhablani and Han Wang and Harshit Pandey and Hendrik Strobelt and Jason Alan Fries and Jos Rozen and Leo Gao and Lintang Sutawika and M Saiful Bari and Maged S. Al-shaibani and Matteo Manica and Nihal Nayak and Ryan Teehan and Samuel Albanie and Sheng Shen and Srulik Ben-David and Stephen H. Bach and Taewoon Kim and Tali Bers and Thibault Fevry and Trishala Neeraj and Urmish Thakker and Vikas Raunak and Xiangru Tang and Zheng-Xin Yong and Zhiqing Sun and Shaked Brody and Yallow Uri and Hadar Tojarieh and Adam Roberts and Hyung Won Chung and Jaesung Tae and Jason Phang and Ofir Press and Conglong Li and Deepak Narayanan and Hatim Bourfoune and Jared Casper and Jeff Rasley and Max Ryabinin and Mayank Mishra and Minjia Zhang and Mohammad Shoeybi and Myriam Peyrounette and Nicolas Patry and Nouamane Tazi and Omar Sanseviero and Patrick von Platen and Pierre Cornette and Pierre François Lavallée and Rémi Lacroix and Samyam Rajbhandari and Sanchit Gandhi and Shaden Smith and Stéphane Requena and Suraj Patil and Tim Dettmers and Ahmed Baruwa and Amanpreet Singh and Anastasia Cheveleva and Anne-Laure Ligozat and Arjun Subramonian and Aurélie Névéol and Charles Lovering and Dan Garrette and Deepak Tunuguntla and Ehud Reiter and Ekaterina Taktasheva and Ekaterina Voloshina and Eli Bogdanov and Genta Indra Winata and Hailey Schoelkopf and Jan-Christoph Kalo and Jekaterina Novikova and Jessica Zosa Forde and Jordan Clive and Jungo Kasai and Ken Kawamura and Liam Hazan and Marine Carpuat and Miruna Clinciu and Najoung Kim and Newton Cheng and Oleg Serikov and Omer Antverg and Oskar van der Wal and Rui Zhang and Ruochen Zhang and Sebastian Gehrmann and Shachar Mirkin and Shani Pais and Tatiana Shavrina and Thomas Scialom and Tian Yun and Tomasz Limisiewicz and Verena Rieser and Vitaly Protasov and Vladislav Mikhailov and Yada Pruksachatkun and Yonatan Belinkov and Zachary Bamberger and Zdeněk Kasner and Alice Rueda and Amanda Pestana and Amir Feizpour and Ammar Khan and Amy Faranak and Ana Santos and Anthony Hevia and Antigona Unldreaj and Arash Aghagol and Arezoo Abdollahi and Aycha Tammour and Azadeh HajiHosseini and Bahareh Behroozi and Benjamin Ajibade and Bharat Saxena and Carlos Muñoz Ferrandis and Daniel McDuff and Danish Contractor and David Lansky and Davis David and Douwe Kiela and Duong A. Nguyen and Edward Tan and Emi Baylor and Ezinwanne Ozoani and Fatima Mirza and Frankline Ononiwu and Habib Rezanejad and Hessie Jones and Indrani Bhattacharya and Irene Solaiman and Irina Sedenko and Isar Nejadgholi and Jesse Passmore and Josh Seltzer and Julio Bonis Sanz and Livia Dutra and Mairon Samagaio and Maraim Elbadri and Margot Mieskes and Marissa Gerchick and Martha Akinlolu and Michael McKenna and Mike Qiu and Muhammed Ghauri and Mykola Burynok and Nafis Abrar and Nazneen Rajani and Nour Elkott and Nour Fahmy and Olanrewaju Samuel and Ran An and Rasmus Kromann and Ryan Hao and Samira Alizadeh and Sarmad Shubber and Silas Wang and Sourav Roy and Sylvain Viguier and Thanh Le and Tobi Oyebade and Trieu Le and Yoyo Yang and Zach Nguyen and Abhinav Ramesh Kashyap and Alfredo Palasciano and Alison Callahan and Anima Shukla and Antonio Miranda-Escalada and Ayush Singh and Benjamin Beilharz and Bo Wang and Caio Brito and Chenxi Zhou and Chirag Jain and Chuxin Xu and Clémentine Fourrier and Daniel León Periñán and Daniel Molano and Dian Yu and Enrique Manjavacas and Fabio Barth and Florian Fuhrimann and Gabriel Altay and Giyaseddin Bayrak and Gully Burns and Helena U. Vrabec and Imane Bello and Ishani Dash and Jihyun Kang and John Giorgi and Jonas Golde and Jose David Posada and Karthik Rangasai Sivaraman and Lokesh Bulchandani and Lu Liu and Luisa Shinzato and Madeleine Hahn de Bykhovetz and Maiko Takeuchi and Marc Pàmies and Maria A Castillo and Marianna Nezhurina and Mario Sänger and Matthias Samwald and Michael Cullan and Michael Weinberg and Michiel De Wolf and Mina Mihaljcic and Minna Liu and Moritz Freidank and Myungsun Kang and Natasha Seelam and Nathan Dahlberg and Nicholas Michio Broad and Nikolaus Muellner and Pascale Fung and Patrick Haller and Ramya Chandrasekhar and Renata Eisenberg and Robert Martin and Rodrigo Canalli and Rosaline Su and Ruisi Su and Samuel Cahyawijaya and Samuele Garda and Shlok S Deshmukh and Shubhanshu Mishra and Sid Kiblawi and Simon Ott and Sinee Sang-aroonsiri and Srishti Kumar and Stefan Schweter and Sushil Bharati and Tanmay Laud and Théo Gigant and Tomoya Kainuma and Wojciech Kusa and Yanis Labrak and Yash Shailesh Bajaj and Yash Venkatraman and Yifan Xu and Yingxin Xu and Yu Xu and Zhe Tan and Zhongli Xie and Zifan Ye and Mathilde Bras and Younes Belkada and Thomas Wolf
@misc{workshop2023bloom,
  title         = {BLOOM: A 176B-Parameter Open-Access Multilingual Language Model},
  author        = {BigScience Workshop},
  year          = {2023},
  eprint        = {2211.05100},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL}
}

@article{muennighoff2022crosslingual,
  title   = {Crosslingual generalization through multitask finetuning},
  author  = {Muennighoff, Niklas and Wang, Thomas and Sutawika, Lintang and Roberts, Adam and Biderman, Stella and Scao, Teven Le and Bari, M Saiful and Shen, Sheng and Yong, Zheng-Xin and Schoelkopf, Hailey and others},
  journal = {arXiv preprint arXiv:2211.01786},
  year    = {2022}
}

@misc{abdin2024phi3,
  title         = {Phi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone},
  author        = {Microsoft research Team},
  year          = {2024},
  eprint        = {2404.14219},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL}
}

@article{llama3modelcard,
  title  = {Llama 3 Model Card},
  author = {AI@Meta},
  year   = {2024},
  url    = {https://github.com/meta-llama/llama3/blob/main/MODEL_CARD.md}
}

@inproceedings{alavoine:hal-04523286,
  title       = {{New Semantic Task for the French Spoken Language Understanding MEDIA Benchmark}},
  author      = {Alavoine, Nad{\`e}ge and Laperriere, Ga{\"e}lle and Servan, Christophe and Ghannay, Sahar and Rosset, Sophie},
  url         = {https://hal.science/hal-04523286},
  booktitle   = {{The 2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation (LREC-COLING 2024)}},
  address     = {Torino, Italy},
  series      = {The 2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation (LREC-COLING 2024)},
  year        = {2024},
  month       = May,
  keywords    = {Benchmark Dataset ; Spoken Language Understanding ; Joint Intent Detection And Slot-filling ; Tri-training},
  pdf         = {https://hal.science/hal-04523286/file/main.pdf},
  hal_id      = {hal-04523286},
  hal_version = {v1}
}

@inproceedings{bonneaumaynard05_interspeech,
  author    = {H. Bonneau-Maynard and Sophie Rosset and C. Ayache and A. Kuhn and Djamel Mostefa},
  title     = {{Semantic annotation of the French media dialog corpus}},
  year      = 2005,
  booktitle = {Proc. Interspeech 2005},
  pages     = {3457--3460},
  doi       = {10.21437/Interspeech.2005-312},
  issn      = {2308-457X}
}

@misc{qin2024cropromptcrosstaskinteractiveprompting,
  title         = {CroPrompt: Cross-task Interactive Prompting for Zero-shot Spoken Language Understanding},
  author        = {Libo Qin and Fuxuan Wei and Qiguang Chen and Jingxuan Zhou and Shijue Huang and Jiasheng Si and Wenpeng Lu and Wanxiang Che},
  year          = {2024},
  eprint        = {2406.10505},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL},
  url           = {https://arxiv.org/abs/2406.10505}
}
@misc{fitzgerald_massive_2022,
  title      = {{MASSIVE}: {A} {1M}-{Example} {Multilingual} {Natural} {Language} {Understanding} {Dataset} with 51 {Typologically}-{Diverse} {Languages}},
  shorttitle = {{MASSIVE}},
  url        = {https://arxiv.org/abs/2204.08582v2},
  abstract   = {We present the MASSIVE dataset--Multilingual Amazon Slu resource package (SLURP) for Slot-filling, Intent classification, and Virtual assistant Evaluation. MASSIVE contains 1M realistic, parallel, labeled virtual assistant utterances spanning 51 languages, 18 domains, 60 intents, and 55 slots. MASSIVE was created by tasking professional translators to localize the English-only SLURP dataset into 50 typologically diverse languages from 29 genera. We also present modeling results on XLM-R and mT5, including exact match accuracy, intent classification accuracy, and slot-filling F1 score. We have released our dataset, modeling code, and models publicly.},
  language   = {en},
  urldate    = {2023-10-08},
  journal    = {arXiv.org},
  author     = {FitzGerald, Jack and Hench, Christopher and Peris, Charith and Mackie, Scott and Rottmann, Kay and Sanchez, Ana and Nash, Aaron and Urbach, Liam and Kakarala, Vishesh and Singh, Richa and Ranganath, Swetha and Crist, Laurie and Britan, Misha and Leeuwis, Wouter and Tur, Gokhan and Natarajan, Prem},
  month      = apr,
  year       = {2022}
}

@misc{coucke_snips_2018,
  title      = {Snips {Voice} {Platform}: an embedded {Spoken} {Language} {Understanding} system for private-by-design voice interfaces},
  shorttitle = {Snips {Voice} {Platform}},
  url        = {https://arxiv.org/abs/1805.10190v3},
  abstract   = {This paper presents the machine learning architecture of the Snips Voice Platform, a software solution to perform Spoken Language Understanding on microprocessors typical of IoT devices. The embedded inference is fast and accurate while enforcing privacy by design, as no personal user data is ever collected. Focusing on Automatic Speech Recognition and Natural Language Understanding, we detail our approach to training high-performance Machine Learning models that are small enough to run in real-time on small devices. Additionally, we describe a data generation procedure that provides sufficient, high-quality training data without compromising user privacy.},
  language   = {en},
  urldate    = {2023-10-08},
  journal    = {arXiv.org},
  author     = {Coucke, Alice and Saade, Alaa and Ball, Adrien and Bluche, Théodore and Caulier, Alexandre and Leroy, David and Doumouro, Clément and Gisselbrecht, Thibault and Caltagirone, Francesco and Lavril, Thibaut and Primet, Maël and Dureau, Joseph},
  month      = may,
  year       = {2018}
}


@inproceedings{krone_learning_2020,
  address   = {Online},
  title     = {Learning to {Classify} {Intents} and {Slot} {Labels} {Given} a {Handful} of {Examples}},
  url       = {https://aclanthology.org/2020.nlp4convai-1.12},
  doi       = {10.18653/v1/2020.nlp4convai-1.12},
  abstract  = {Intent classification (IC) and slot filling (SF) are core components in most goal-oriented dialogue systems. Current IC/SF models perform poorly when the number of training examples per class is small. We propose a new few-shot learning task, few-shot IC/SF, to study and improve the performance of IC and SF models on classes not seen at training time in ultra low resource scenarios. We establish a few-shot IC/SF benchmark by defining few-shot splits for three public IC/SF datasets, ATIS, TOP, and Snips. We show that two popular few-shot learning algorithms, model agnostic meta learning (MAML) and prototypical networks, outperform a fine-tuning baseline on this benchmark. Prototypical networks achieves significant gains in IC performance on the ATIS and TOP datasets, while both prototypical networks and MAML outperform the baseline with respect to SF on all three datasets. In addition, we demonstrate that joint training as well as the use of pre-trained language models, ELMo and BERT in our case, are complementary to these few-shot learning methods and yield further gains.},
  urldate   = {2023-10-08},
  booktitle = {Proceedings of the 2nd {Workshop} on {Natural} {Language} {Processing} for {Conversational} {AI}},
  publisher = {Association for Computational Linguistics},
  author    = {Krone, Jason and Zhang, Yi and Diab, Mona},
  month     = jul,
  year      = {2020},
  pages     = {96--108}
}

@inproceedings{gupta_simple_2019,
  address   = {Stockholm, Sweden},
  title     = {Simple, {Fast}, {Accurate} {Intent} {Classification} and {Slot} {Labeling} for {Goal}-{Oriented} {Dialogue} {Systems}},
  url       = {https://aclanthology.org/W19-5906},
  doi       = {10.18653/v1/W19-5906},
  abstract  = {With the advent of conversational assistants, like Amazon Alexa, Google Now, etc., dialogue systems are gaining a lot of traction, especially in industrial setting. These systems typically consist of Spoken Language understanding component which, in turn, consists of two tasks - Intent Classification (IC) and Slot Labeling (SL). Generally, these two tasks are modeled together jointly to achieve best performance. However, this joint modeling adds to model obfuscation. In this work, we first design framework for a modularization of joint IC-SL task to enhance architecture transparency. Then, we explore a number of self-attention, convolutional, and recurrent models, contributing a large-scale analysis of modeling paradigms for IC+SL across two datasets. Finally, using this framework, we propose a class of `label-recurrent' models that otherwise non-recurrent, with a 10-dimensional representation of the label history, and show that our proposed systems are easy to interpret, highly accurate (achieving over 30\% error reduction in SL over the state-of-the-art on the Snips dataset), as well as fast, at 2x the inference and 2/3 to 1/2 the training time of comparable recurrent models, thus giving an edge in critical real-world systems.},
  urldate   = {2023-10-06},
  booktitle = {Proceedings of the 20th {Annual} {SIGdial} {Meeting} on {Discourse} and {Dialogue}},
  publisher = {Association for Computational Linguistics},
  author    = {Gupta, Arshit and Hewitt, John and Kirchhoff, Katrin},
  month     = sep,
  year      = {2019},
  pages     = {46--55}
}

@misc{han_bi-directional_2022,
  title     = {Bi-directional {Joint} {Neural} {Networks} for {Intent} {Classification} and {Slot} {Filling}},
  url       = {http://arxiv.org/abs/2202.13079},
  doi       = {10.48550/arXiv.2202.13079},
  abstract  = {Intent classification and slot filling are two critical tasks for natural language understanding. Traditionally the two tasks proceeded independently. However, more recently joint models for intent classification and slot filling have achieved state-of-the-art performance, and have proved that there exists a strong relationship between the two tasks. In this paper, we propose a bi-directional joint model for intent classification and slot filling, which includes a multi-stage hierarchical process via BERT and bi-directional joint natural language understanding mechanisms, including intent2slot and slot2intent, to obtain mutual performance enhancement between intent classification and slot filling. The evaluations show that our model achieves state-of-the-art results on intent classification accuracy, slot filling F1, and significantly improves sentence-level semantic frame accuracy when applied to publicly available benchmark datasets, ATIS (88.6\%) and SNIPS (92.8\%).},
  urldate   = {2023-10-06},
  publisher = {arXiv},
  author    = {Han, Soyeon Caren and Long, Siqu and Li, Huichun and Weld, Henry and Poon, Josiah},
  month     = feb,
  year      = {2022},
  note      = {arXiv:2202.13079 [cs]},
  keywords  = {Computer Science - Computation and Language}
}

@misc{workshop_bloom_2022,
  title      = {{BLOOM}: {A} {176B}-{Parameter} {Open}-{Access} {Multilingual} {Language} {Model}},
  shorttitle = {{BLOOM}},
  url        = {https://arxiv.org/abs/2211.05100v4},
  abstract   = {Large language models (LLMs) have been shown to be able to perform new tasks based on a few demonstrations or natural language instructions. While these capabilities have led to widespread adoption, most LLMs are developed by resource-rich organizations and are frequently kept from the public. As a step towards democratizing this powerful technology, we present BLOOM, a 176B-parameter open-access language model designed and built thanks to a collaboration of hundreds of researchers. BLOOM is a decoder-only Transformer language model that was trained on the ROOTS corpus, a dataset comprising hundreds of sources in 46 natural and 13 programming languages (59 in total). We find that BLOOM achieves competitive performance on a wide variety of benchmarks, with stronger results after undergoing multitask prompted finetuning. To facilitate future research and applications using LLMs, we publicly release our models and code under the Responsible AI License.},
  language   = {en},
  urldate    = {2023-09-30},
  journal    = {arXiv.org},
  author     = {Workshop, BigScience and Scao, Teven Le and Fan, Angela and Akiki, Christopher and Pavlick, Ellie and Ilić, Suzana and Hesslow, Daniel and Castagné, Roman and Luccioni, Alexandra Sasha and Yvon, François and Gallé, Matthias and Tow, Jonathan and Rush, Alexander M. and Biderman, Stella and Webson, Albert and Ammanamanchi, Pawan Sasanka and Wang, Thomas and Sagot, Benoît and Muennighoff, Niklas and del Moral, Albert Villanova and Ruwase, Olatunji and Bawden, Rachel and Bekman, Stas and McMillan-Major, Angelina and Beltagy, Iz and Nguyen, Huu and Saulnier, Lucile and Tan, Samson and Suarez, Pedro Ortiz and Sanh, Victor and Laurençon, Hugo and Jernite, Yacine and Launay, Julien and Mitchell, Margaret and Raffel, Colin and Gokaslan, Aaron and Simhi, Adi and Soroa, Aitor and Aji, Alham Fikri and Alfassy, Amit and Rogers, Anna and Nitzav, Ariel Kreisberg and Xu, Canwen and Mou, Chenghao and Emezue, Chris and Klamm, Christopher and Leong, Colin and van Strien, Daniel and Adelani, David Ifeoluwa and Radev, Dragomir and Ponferrada, Eduardo González and Levkovizh, Efrat and Kim, Ethan and Natan, Eyal Bar and De Toni, Francesco and Dupont, Gérard and Kruszewski, Germán and Pistilli, Giada and Elsahar, Hady and Benyamina, Hamza and Tran, Hieu and Yu, Ian and Abdulmumin, Idris and Johnson, Isaac and Gonzalez-Dios, Itziar and de la Rosa, Javier and Chim, Jenny and Dodge, Jesse and Zhu, Jian and Chang, Jonathan and Frohberg, Jörg and Tobing, Joseph and Bhattacharjee, Joydeep and Almubarak, Khalid and Chen, Kimbo and Lo, Kyle and Von Werra, Leandro and Weber, Leon and Phan, Long and Allal, Loubna Ben and Tanguy, Ludovic and Dey, Manan and Muñoz, Manuel Romero and Masoud, Maraim and Grandury, María and Šaško, Mario and Huang, Max and Coavoux, Maximin and Singh, Mayank and Jiang, Mike Tian-Jian and Vu, Minh Chien and Jauhar, Mohammad A. and Ghaleb, Mustafa and Subramani, Nishant and Kassner, Nora and Khamis, Nurulaqilla and Nguyen, Olivier and Espejel, Omar and de Gibert, Ona and Villegas, Paulo and Henderson, Peter and Colombo, Pierre and Amuok, Priscilla and Lhoest, Quentin and Harliman, Rheza and Bommasani, Rishi and López, Roberto Luis and Ribeiro, Rui and Osei, Salomey and Pyysalo, Sampo and Nagel, Sebastian and Bose, Shamik and Muhammad, Shamsuddeen Hassan and Sharma, Shanya and Longpre, Shayne and Nikpoor, Somaieh and Silberberg, Stanislav and Pai, Suhas and Zink, Sydney and Torrent, Tiago Timponi and Schick, Timo and Thrush, Tristan and Danchev, Valentin and Nikoulina, Vassilina and Laippala, Veronika and Lepercq, Violette and Prabhu, Vrinda and Alyafeai, Zaid and Talat, Zeerak and Raja, Arun and Heinzerling, Benjamin and Si, Chenglei and Taşar, Davut Emre and Salesky, Elizabeth and Mielke, Sabrina J. and Lee, Wilson Y. and Sharma, Abheesht and Santilli, Andrea and Chaffin, Antoine and Stiegler, Arnaud and Datta, Debajyoti and Szczechla, Eliza and Chhablani, Gunjan and Wang, Han and Pandey, Harshit and Strobelt, Hendrik and Fries, Jason Alan and Rozen, Jos and Gao, Leo and Sutawika, Lintang and Bari, M. Saiful and Al-shaibani, Maged S. and Manica, Matteo and Nayak, Nihal and Teehan, Ryan and Albanie, Samuel and Shen, Sheng and Ben-David, Srulik and Bach, Stephen H. and Kim, Taewoon and Bers, Tali and Fevry, Thibault and Neeraj, Trishala and Thakker, Urmish and Raunak, Vikas and Tang, Xiangru and Yong, Zheng-Xin and Sun, Zhiqing and Brody, Shaked and Uri, Yallow and Tojarieh, Hadar and Roberts, Adam and Chung, Hyung Won and Tae, Jaesung and Phang, Jason and Press, Ofir and Li, Conglong and Narayanan, Deepak and Bourfoune, Hatim and Casper, Jared and Rasley, Jeff and Ryabinin, Max and Mishra, Mayank and Zhang, Minjia and Shoeybi, Mohammad and Peyrounette, Myriam and Patry, Nicolas and Tazi, Nouamane and Sanseviero, Omar and von Platen, Patrick and Cornette, Pierre and Lavallée, Pierre François and Lacroix, Rémi and Rajbhandari, Samyam and Gandhi, Sanchit and Smith, Shaden and Requena, Stéphane and Patil, Suraj and Dettmers, Tim and Baruwa, Ahmed and Singh, Amanpreet and Cheveleva, Anastasia and Ligozat, Anne-Laure and Subramonian, Arjun and Névéol, Aurélie and Lovering, Charles and Garrette, Dan and Tunuguntla, Deepak and Reiter, Ehud and Taktasheva, Ekaterina and Voloshina, Ekaterina and Bogdanov, Eli and Winata, Genta Indra and Schoelkopf, Hailey and Kalo, Jan-Christoph and Novikova, Jekaterina and Forde, Jessica Zosa and Clive, Jordan and Kasai, Jungo and Kawamura, Ken and Hazan, Liam and Carpuat, Marine and Clinciu, Miruna and Kim, Najoung and Cheng, Newton and Serikov, Oleg and Antverg, Omer and van der Wal, Oskar and Zhang, Rui and Zhang, Ruochen and Gehrmann, Sebastian and Mirkin, Shachar and Pais, Shani and Shavrina, Tatiana and Scialom, Thomas and Yun, Tian and Limisiewicz, Tomasz and Rieser, Verena and Protasov, Vitaly and Mikhailov, Vladislav and Pruksachatkun, Yada and Belinkov, Yonatan and Bamberger, Zachary and Kasner, Zdeněk and Rueda, Alice and Pestana, Amanda and Feizpour, Amir and Khan, Ammar and Faranak, Amy and Santos, Ana and Hevia, Anthony and Unldreaj, Antigona and Aghagol, Arash and Abdollahi, Arezoo and Tammour, Aycha and HajiHosseini, Azadeh and Behroozi, Bahareh and Ajibade, Benjamin and Saxena, Bharat and Ferrandis, Carlos Muñoz and McDuff, Daniel and Contractor, Danish and Lansky, David and David, Davis and Kiela, Douwe and Nguyen, Duong A. and Tan, Edward and Baylor, Emi and Ozoani, Ezinwanne and Mirza, Fatima and Ononiwu, Frankline and Rezanejad, Habib and Jones, Hessie and Bhattacharya, Indrani and Solaiman, Irene and Sedenko, Irina and Nejadgholi, Isar and Passmore, Jesse and Seltzer, Josh and Sanz, Julio Bonis and Dutra, Livia and Samagaio, Mairon and Elbadri, Maraim and Mieskes, Margot and Gerchick, Marissa and Akinlolu, Martha and McKenna, Michael and Qiu, Mike and Ghauri, Muhammed and Burynok, Mykola and Abrar, Nafis and Rajani, Nazneen and Elkott, Nour and Fahmy, Nour and Samuel, Olanrewaju and An, Ran and Kromann, Rasmus and Hao, Ryan and Alizadeh, Samira and Shubber, Sarmad and Wang, Silas and Roy, Sourav and Viguier, Sylvain and Le, Thanh and Oyebade, Tobi and Le, Trieu and Yang, Yoyo and Nguyen, Zach and Kashyap, Abhinav Ramesh and Palasciano, Alfredo and Callahan, Alison and Shukla, Anima and Miranda-Escalada, Antonio and Singh, Ayush and Beilharz, Benjamin and Wang, Bo and Brito, Caio and Zhou, Chenxi and Jain, Chirag and Xu, Chuxin and Fourrier, Clémentine and Periñán, Daniel León and Molano, Daniel and Yu, Dian and Manjavacas, Enrique and Barth, Fabio and Fuhrimann, Florian and Altay, Gabriel and Bayrak, Giyaseddin and Burns, Gully and Vrabec, Helena U. and Bello, Imane and Dash, Ishani and Kang, Jihyun and Giorgi, John and Golde, Jonas and Posada, Jose David and Sivaraman, Karthik Rangasai and Bulchandani, Lokesh and Liu, Lu and Shinzato, Luisa and de Bykhovetz, Madeleine Hahn and Takeuchi, Maiko and Pàmies, Marc and Castillo, Maria A. and Nezhurina, Marianna and Sänger, Mario and Samwald, Matthias and Cullan, Michael and Weinberg, Michael and De Wolf, Michiel and Mihaljcic, Mina and Liu, Minna and Freidank, Moritz and Kang, Myungsun and Seelam, Natasha and Dahlberg, Nathan and Broad, Nicholas Michio and Muellner, Nikolaus and Fung, Pascale and Haller, Patrick and Chandrasekhar, Ramya and Eisenberg, Renata and Martin, Robert and Canalli, Rodrigo and Su, Rosaline and Su, Ruisi and Cahyawijaya, Samuel and Garda, Samuele and Deshmukh, Shlok S. and Mishra, Shubhanshu and Kiblawi, Sid and Ott, Simon and Sang-aroonsiri, Sinee and Kumar, Srishti and Schweter, Stefan and Bharati, Sushil and Laud, Tanmay and Gigant, Théo and Kainuma, Tomoya and Kusa, Wojciech and Labrak, Yanis and Bajaj, Yash Shailesh and Venkatraman, Yash and Xu, Yifan and Xu, Yingxin and Xu, Yu and Tan, Zhe and Xie, Zhongli and Ye, Zifan and Bras, Mathilde and Belkada, Younes and Wolf, Thomas},
  month      = nov,
  year       = {2022}
}

@misc{goodfellow_empirical_2015,
  title     = {An {Empirical} {Investigation} of {Catastrophic} {Forgetting} in {Gradient}-{Based} {Neural} {Networks}},
  url       = {http://arxiv.org/abs/1312.6211},
  doi       = {10.48550/arXiv.1312.6211},
  abstract  = {Catastrophic forgetting is a problem faced by many machine learning models and algorithms. When trained on one task, then trained on a second task, many machine learning models "forget" how to perform the first task. This is widely believed to be a serious problem for neural networks. Here, we investigate the extent to which the catastrophic forgetting problem occurs for modern neural networks, comparing both established and recent gradient-based training algorithms and activation functions. We also examine the effect of the relationship between the first task and the second task on catastrophic forgetting. We find that it is always best to train using the dropout algorithm--the dropout algorithm is consistently best at adapting to the new task, remembering the old task, and has the best tradeoff curve between these two extremes. We find that different tasks and relationships between tasks result in very different rankings of activation function performance. This suggests the choice of activation function should always be cross-validated.},
  urldate   = {2023-09-29},
  publisher = {arXiv},
  author    = {Goodfellow, Ian J. and Mirza, Mehdi and Xiao, Da and Courville, Aaron and Bengio, Yoshua},
  month     = mar,
  year      = {2015},
  note      = {arXiv:1312.6211 [cs, stat]},
  keywords  = {Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, Statistics - Machine Learning}
}

@article{radford_improving_nodate,
  title    = {Improving {Language} {Understanding} by {Generative} {Pre}-{Training}},
  abstract = {Natural language understanding comprises a wide range of diverse tasks such as textual entailment, question answering, semantic similarity assessment, and document classiﬁcation. Although large unlabeled text corpora are abundant, labeled data for learning these speciﬁc tasks is scarce, making it challenging for discriminatively trained models to perform adequately. We demonstrate that large gains on these tasks can be realized by generative pre-training of a language model on a diverse corpus of unlabeled text, followed by discriminative ﬁne-tuning on each speciﬁc task. In contrast to previous approaches, we make use of task-aware input transformations during ﬁne-tuning to achieve effective transfer while requiring minimal changes to the model architecture. We demonstrate the effectiveness of our approach on a wide range of benchmarks for natural language understanding. Our general task-agnostic model outperforms discriminatively trained models that use architectures speciﬁcally crafted for each task, signiﬁcantly improving upon the state of the art in 9 out of the 12 tasks studied. For instance, we achieve absolute improvements of 8.9\% on commonsense reasoning (Stories Cloze Test), 5.7\% on question answering (RACE), and 1.5\% on textual entailment (MultiNLI).},
  language = {en},
  author   = {Radford, Alec and Narasimhan, Karthik and Salimans, Tim and Sutskever, Ilya}
}

@misc{tunstall_efficient_2022,
  title     = {Efficient {Few}-{Shot} {Learning} {Without} {Prompts}},
  url       = {http://arxiv.org/abs/2209.11055},
  doi       = {10.48550/arXiv.2209.11055},
  abstract  = {Recent few-shot methods, such as parameter-efficient fine-tuning (PEFT) and pattern exploiting training (PET), have achieved impressive results in label-scarce settings. However, they are difficult to employ since they are subject to high variability from manually crafted prompts, and typically require billion-parameter language models to achieve high accuracy. To address these shortcomings, we propose SetFit (Sentence Transformer Fine-tuning), an efficient and prompt-free framework for few-shot fine-tuning of Sentence Transformers (ST). SetFit works by first fine-tuning a pretrained ST on a small number of text pairs, in a contrastive Siamese manner. The resulting model is then used to generate rich text embeddings, which are used to train a classification head. This simple framework requires no prompts or verbalizers, and achieves high accuracy with orders of magnitude less parameters than existing techniques. Our experiments show that SetFit obtains comparable results with PEFT and PET techniques, while being an order of magnitude faster to train. We also show that SetFit can be applied in multilingual settings by simply switching the ST body. Our code is available at https://github.com/huggingface/setfit and our datasets at https://huggingface.co/setfit .},
  urldate   = {2023-09-29},
  publisher = {arXiv},
  author    = {Tunstall, Lewis and Reimers, Nils and Jo, Unso Eun Seo and Bates, Luke and Korat, Daniel and Wasserblat, Moshe and Pereg, Oren},
  month     = sep,
  year      = {2022},
  note      = {arXiv:2209.11055 [cs]},
  keywords  = {Computer Science - Computation and Language}
}

@misc{penedo_refinedweb_2023,
  title      = {The {RefinedWeb} {Dataset} for {Falcon} {LLM}: {Outperforming} {Curated} {Corpora} with {Web} {Data}, and {Web} {Data} {Only}},
  shorttitle = {The {RefinedWeb} {Dataset} for {Falcon} {LLM}},
  url        = {http://arxiv.org/abs/2306.01116},
  doi        = {10.48550/arXiv.2306.01116},
  abstract   = {Large language models are commonly trained on a mixture of filtered web data and curated high-quality corpora, such as social media conversations, books, or technical papers. This curation process is believed to be necessary to produce performant models with broad zero-shot generalization abilities. However, as larger models requiring pretraining on trillions of tokens are considered, it is unclear how scalable is curation and whether we will run out of unique high-quality data soon. At variance with previous beliefs, we show that properly filtered and deduplicated web data alone can lead to powerful models; even significantly outperforming models from the state-of-the-art trained on The Pile. Despite extensive filtering, the high-quality data we extract from the web is still plentiful, and we are able to obtain five trillion tokens from CommonCrawl. We publicly release an extract of 600 billion tokens from our RefinedWeb dataset, and 1.3/7.5B parameters language models trained on it.},
  urldate    = {2023-09-29},
  publisher  = {arXiv},
  author     = {Penedo, Guilherme and Malartic, Quentin and Hesslow, Daniel and Cojocaru, Ruxandra and Cappelli, Alessandro and Alobeidli, Hamza and Pannier, Baptiste and Almazrouei, Ebtesam and Launay, Julien},
  month      = jun,
  year       = {2023},
  note       = {arXiv:2306.01116 [cs]},
  keywords   = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language}
}

@misc{instruct-llm-survey,
  title      = {Instruction {Tuning} for {Large} {Language} {Models}: {A} {Survey}},
  shorttitle = {Instruction {Tuning} for {Large} {Language} {Models}},
  url        = {http://arxiv.org/abs/2308.10792},
  abstract   = {This paper surveys research works in the quickly advancing field of instruction tuning (IT), a crucial technique to enhance the capabilities and controllability of large language models (LLMs). Instruction tuning refers to the process of further training LLMs on a dataset consisting of {\textbackslash}textsc\{(instruction, output)\} pairs in a supervised fashion, which bridges the gap between the next-word prediction objective of LLMs and the users' objective of having LLMs adhere to human instructions. In this work, we make a systematic review of the literature, including the general methodology of IT, the construction of IT datasets, the training of IT models, and applications to different modalities, domains and applications, along with an analysis on aspects that influence the outcome of IT (e.g., generation of instruction outputs, size of the instruction dataset, etc). We also review the potential pitfalls of IT along with criticism against it, along with efforts pointing out current deficiencies of existing strategies and suggest some avenues for fruitful research.},
  urldate    = {2023-09-18},
  publisher  = {arXiv},
  author     = {Zhang, Shengyu and Dong, Linfeng and Li, Xiaoya and Zhang, Sen and Sun, Xiaofei and Wang, Shuhe and Li, Jiwei and Hu, Runyi and Zhang, Tianwei and Wu, Fei and Wang, Guoyin},
  month      = aug,
  year       = {2023},
  note       = {arXiv:2308.10792 [cs]},
  keywords   = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning}
}

@misc{xu_baize_2023,
  title      = {Baize: {An} {Open}-{Source} {Chat} {Model} with {Parameter}-{Efficient} {Tuning} on {Self}-{Chat} {Data}},
  shorttitle = {Baize},
  url        = {http://arxiv.org/abs/2304.01196},
  doi        = {10.48550/arXiv.2304.01196},
  abstract   = {Chat models, such as ChatGPT, have shown impressive capabilities and have been rapidly adopted across numerous domains. However, these models are only accessible through a restricted API, creating barriers for new research and progress in the field. We propose a pipeline that can automatically generate a high-quality multi-turn chat corpus by leveraging ChatGPT to engage in a conversation with itself. Subsequently, we employ parameter-efficient tuning to enhance LLaMA, an open-source large language model. The resulting model, named Baize, demonstrates good performance in multi-turn dialogues with guardrails that minimize potential risks. Furthermore, we propose a new technique called Self-Distill with Feedback, to further improve the performance of the Baize models with feedback from ChatGPT. The Baize models and data are released for research purposes only at https://github.com/project-baize/baize-chatbot. An online demo is also available at https://huggingface.co/spaces/project-baize/chat-with-baize.},
  urldate    = {2023-09-29},
  publisher  = {arXiv},
  author     = {Xu, Canwen and Guo, Daya and Duan, Nan and McAuley, Julian},
  month      = may,
  year       = {2023},
  note       = {arXiv:2304.01196 [cs]},
  keywords   = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language}
}


@misc{longpre_flan_2023,
  title      = {The {Flan} {Collection}: {Designing} {Data} and {Methods} for {Effective} {Instruction} {Tuning}},
  shorttitle = {The {Flan} {Collection}},
  url        = {http://arxiv.org/abs/2301.13688},
  doi        = {10.48550/arXiv.2301.13688},
  abstract   = {We study the design decisions of publicly available instruction tuning methods, and break down the development of Flan 2022 (Chung et al., 2022). Through careful ablation studies on the Flan Collection of tasks and methods, we tease apart the effect of design decisions which enable Flan-T5 to outperform prior work by 3-17\%+ across evaluation settings. We find task balancing and enrichment techniques are overlooked but critical to effective instruction tuning, and in particular, training with mixed prompt settings (zero-shot, few-shot, and chain-of-thought) actually yields stronger (2\%+) performance in all settings. In further experiments, we show Flan-T5 requires less finetuning to converge higher and faster than T5 on single downstream tasks, motivating instruction-tuned models as more computationally-efficient starting checkpoints for new tasks. Finally, to accelerate research on instruction tuning, we make the Flan 2022 collection of datasets, templates, and methods publicly available at https://github.com/google-research/FLAN/tree/main/flan/v2.},
  urldate    = {2023-09-29},
  publisher  = {arXiv},
  author     = {Longpre, Shayne and Hou, Le and Vu, Tu and Webson, Albert and Chung, Hyung Won and Tay, Yi and Zhou, Denny and Le, Quoc V. and Zoph, Barret and Wei, Jason and Roberts, Adam},
  month      = feb,
  year       = {2023},
  note       = {arXiv:2301.13688 [cs]},
  keywords   = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning}
}

@article{Groeneveld2023OLMo,
  title   = {OLMo: Accelerating the Science of Language Models},
  author  = {Groeneveld, Dirk and Beltagy, Iz and Walsh, Pete and Bhagia, Akshita and Kinney, Rodney and Tafjord, Oyvind and Jha, Ananya Harsh and Ivison, Hamish and Magnusson, Ian and Wang, Yizhong and Arora, Shane and Atkinson, David and Authur, Russell and Chandu, Khyathi and Cohan, Arman and Dumas, Jennifer and Elazar, Yanai and Gu, Yuling and Hessel, Jack and Khot, Tushar and Merrill, William and Morrison, Jacob and Muennighoff, Niklas and Naik, Aakanksha and Nam, Crystal and Peters, Matthew E. and Pyatkin, Valentina and Ravichander, Abhilasha and Schwenk, Dustin and Shah, Saurabh and Smith, Will and Subramani, Nishant and Wortsman, Mitchell and Dasigi, Pradeep and Lambert, Nathan and Richardson, Kyle and Dodge, Jesse and Lo, Kyle and Soldaini, Luca and Smith, Noah A. and Hajishirzi, Hannaneh},
  journal = {Preprint},
  year    = {2024}
}

@misc{noauthor_38993608_nodate,
  title    = {38993608 · {Efficient} {Few}-{Shot} {Learning} {Without} {Prompts}},
  url      = {https://slideslive.com/embed/presentation/38993608?embed_parent_url=https%3A%2F%2Fnips.cc%2Fvirtual%2F2022%2F59466&embed_origin=https%3A%2F%2Fnips.cc&embed_container_id=presentation-embed-38993608&auto_load=true&auto_play=false&zoom_ratio=&disable_fullscreen=false&locale=en&vertical_enabled=true&vertical_enabled_on_mobile=false&allow_hidden_controls_when_paused=true&fit_to_viewport=true&custom_user_id=&user_uuid=421bd679-642d-4ed8-b63d-1163e03f52fc},
  abstract = {Professional Conference Recording},
  language = {en-US},
  urldate  = {2023-09-29},
  journal  = {SlidesLive}
}

@misc{faysse2024croissantllm,
  title         = {CroissantLLM: A Truly Bilingual French-English Language Model},
  author        = {Manuel Faysse and Patrick Fernandes and Nuno M. Guerreiro and António Loison and Duarte M. Alves and Caio Corro and Nicolas Boizard and João Alves and Ricardo Rei and Pedro H. Martins and Antoni Bigata Casademunt and François Yvon and André F. T. Martins and Gautier Viaud and Céline Hudelot and Pierre Colombo},
  year          = {2024},
  eprint        = {2402.00786},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL}
}

@article{weld_survey_2023,
  title    = {A {Survey} of {Joint} {Intent} {Detection} and {Slot} {Filling} {Models} in {Natural} {Language} {Understanding}},
  volume   = {55},
  issn     = {0360-0300, 1557-7341},
  url      = {https://dl.acm.org/doi/10.1145/3547138},
  doi      = {10.1145/3547138},
  abstract = {Intent classification, to identify the speaker’s intention, and slot filling, to label each token with a semantic type, are critical tasks in natural language understanding. Traditionally the two tasks have been addressed independently. More recently joint models that address the two tasks together have achieved state-of-the-art performance for each task and have shown there exists a strong relationship between the two. In this survey, we bring the coverage of methods up to 2021 including the many applications of deep learning in the field. As well as a technological survey, we look at issues addressed in the joint task and the approaches designed to address these issues. We cover datasets, evaluation metrics, and experiment design and supply a summary of reported performance on the standard datasets.},
  language = {en},
  number   = {8},
  urldate  = {2023-09-28},
  journal  = {ACM Computing Surveys},
  author   = {Weld, Henry and Huang, Xiaoqi and Long, Siqu and Poon, Josiah and Han, Soyeon Caren},
  month    = aug,
  year     = {2023},
  pages    = {1--38}
}
@misc{liu_ia3_2022,
  title     = {{IA3}: {Few}-{Shot} {Parameter}-{Efficient} {Fine}-{Tuning} is {Better} and {Cheaper} than {In}-{Context} {Learning}},
  url       = {http://arxiv.org/abs/2205.05638},
  doi       = {10.48550/arXiv.2205.05638},
  abstract  = {Few-shot in-context learning (ICL) enables pre-trained language models to perform a previously-unseen task without any gradient-based training by feeding a small number of training examples as part of the input. ICL incurs substantial computational, memory, and storage costs because it involves processing all of the training examples every time a prediction is made. Parameter-efficient fine-tuning (PEFT) (e.g. adapter modules, prompt tuning, sparse update methods, etc.) offers an alternative paradigm where a small set of parameters are trained to enable a model to perform the new task. In this paper, we rigorously compare few-shot ICL and PEFT and demonstrate that the latter offers better accuracy as well as dramatically lower computational costs. Along the way, we introduce a new PEFT method called (IA)\${\textasciicircum}3\$ that scales activations by learned vectors, attaining stronger performance while only introducing a relatively tiny amount of new parameters. We also propose a simple recipe based on the T0 model called T-Few that can be applied to new tasks without task-specific tuning or modifications. We validate the effectiveness of T-Few on completely unseen tasks by applying it to the RAFT benchmark, attaining super-human performance for the first time and outperforming the state-of-the-art by 6\% absolute. All of the code used in our experiments is publicly available.},
  urldate   = {2023-09-25},
  publisher = {arXiv},
  author    = {Liu, Haokun and Tam, Derek and Muqeeth, Mohammed and Mohta, Jay and Huang, Tenghao and Bansal, Mohit and Raffel, Colin},
  month     = aug,
  year      = {2022},
  note      = {arXiv:2205.05638 [cs]},
  keywords  = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning}
}

@misc{hosseini-asl_simple_2022,
  title     = {A {Simple} {Language} {Model} for {Task}-{Oriented} {Dialogue}},
  url       = {http://arxiv.org/abs/2005.00796},
  abstract  = {Task-oriented dialogue is often decomposed into three tasks: understanding user input, deciding actions, and generating a response. While such decomposition might suggest a dedicated model for each sub-task, we find a simple, unified approach leads to state-of-the-art performance on the MultiWOZ dataset. SimpleTOD is a simple approach to task-oriented dialogue that uses a single, causal language model trained on all sub-tasks recast as a single sequence prediction problem. This allows SimpleTOD to fully leverage transfer learning from pre-trained, open domain, causal language models such as GPT-2. SimpleTOD improves over the prior state-of-the-art in joint goal accuracy for dialogue state tracking, and our analysis reveals robustness to noisy annotations in this setting. SimpleTOD also improves the main metrics used to evaluate action decisions and response generation in an end-to-end setting: inform rate by 8.1 points, success rate by 9.7 points, and combined score by 7.2 points.},
  urldate   = {2023-09-19},
  publisher = {arXiv},
  author    = {Hosseini-Asl, Ehsan and McCann, Bryan and Wu, Chien-Sheng and Yavuz, Semih and Socher, Richard},
  month     = apr,
  year      = {2022},
  note      = {arXiv:2005.00796 [cs]},
  keywords  = {Computer Science - Computation and Language}
}

@misc{instruct-llm-survey,
  title      = {Instruction {Tuning} for {Large} {Language} {Models}: {A} {Survey}},
  shorttitle = {Instruction {Tuning} for {Large} {Language} {Models}},
  url        = {http://arxiv.org/abs/2308.10792},
  abstract   = {This paper surveys research works in the quickly advancing field of instruction tuning (IT), a crucial technique to enhance the capabilities and controllability of large language models (LLMs). Instruction tuning refers to the process of further training LLMs on a dataset consisting of {\textbackslash}textsc\{(instruction, output)\} pairs in a supervised fashion, which bridges the gap between the next-word prediction objective of LLMs and the users' objective of having LLMs adhere to human instructions. In this work, we make a systematic review of the literature, including the general methodology of IT, the construction of IT datasets, the training of IT models, and applications to different modalities, domains and applications, along with an analysis on aspects that influence the outcome of IT (e.g., generation of instruction outputs, size of the instruction dataset, etc). We also review the potential pitfalls of IT along with criticism against it, along with efforts pointing out current deficiencies of existing strategies and suggest some avenues for fruitful research.},
  urldate    = {2023-09-18},
  publisher  = {arXiv},
  author     = {Zhang, Shengyu and Dong, Linfeng and Li, Xiaoya and Zhang, Sen and Sun, Xiaofei and Wang, Shuhe and Li, Jiwei and Hu, Runyi and Zhang, Tianwei and Wu, Fei and Wang, Guoyin},
  month      = aug,
  year       = {2023},
  note       = {arXiv:2308.10792 [cs]},
  keywords   = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning}
}


@article{ramakrishnan_long-term_2022,
  title      = {Long-term {Control} for {Dialogue} {Generation}: {Methods} and {Evaluation}},
  copyright  = {Creative Commons Attribution 4.0 International},
  shorttitle = {Long-term {Control} for {Dialogue} {Generation}},
  url        = {https://arxiv.org/abs/2205.07352},
  doi        = {10.48550/ARXIV.2205.07352},
  abstract   = {Current approaches for controlling dialogue response generation are primarily focused on high-level attributes like style, sentiment, or topic. In this work, we focus on constrained long-term dialogue generation, which involves more fine-grained control and requires a given set of control words to appear in generated responses. This setting requires a model to not only consider the generation of these control words in the immediate context, but also produce utterances that will encourage the generation of the words at some time in the (possibly distant) future. We define the problem of constrained long-term control for dialogue generation, identify gaps in current methods for evaluation, and propose new metrics that better measure long-term control. We also propose a retrieval-augmented method that improves performance of long-term controlled generation via logit modification techniques. We show through experiments on three task-oriented dialogue datasets that our metrics better assess dialogue control relative to current alternatives and that our method outperforms state-of-the-art constrained generation baselines.},
  urldate    = {2023-09-11},
  author     = {Ramakrishnan, Ramya and Narangodage, Hashan Buddhika and Schilman, Mauro and Weinberger, Kilian Q. and McDonald, Ryan},
  year       = {2022},
  keywords   = {Artificial Intelligence (cs.AI), Computation and Language (cs.CL), FOS: Computer and information sciences}
}

@article{chen_action-based_2021,
  title      = {Action-{Based} {Conversations} {Dataset}: {A} {Corpus} for {Building} {More} {In}-{Depth} {Task}-{Oriented} {Dialogue} {Systems}},
  copyright  = {Creative Commons Attribution 4.0 International},
  shorttitle = {Action-{Based} {Conversations} {Dataset}},
  url        = {https://arxiv.org/abs/2104.00783},
  doi        = {10.48550/ARXIV.2104.00783},
  abstract   = {Existing goal-oriented dialogue datasets focus mainly on identifying slots and values. However, customer support interactions in reality often involve agents following multi-step procedures derived from explicitly-defined company policies as well. To study customer service dialogue systems in more realistic settings, we introduce the Action-Based Conversations Dataset (ABCD), a fully-labeled dataset with over 10K human-to-human dialogues containing 55 distinct user intents requiring unique sequences of actions constrained by policies to achieve task success. We propose two additional dialog tasks, Action State Tracking and Cascading Dialogue Success, and establish a series of baselines involving large-scale, pre-trained language models on this dataset. Empirical results demonstrate that while more sophisticated networks outperform simpler models, a considerable gap (50.8\% absolute accuracy) still exists to reach human-level performance on ABCD.},
  urldate    = {2023-09-11},
  author     = {Chen, Derek and Chen, Howard and Yang, Yi and Lin, Alex and Yu, Zhou},
  year       = {2021},
  keywords   = {Computation and Language (cs.CL), FOS: Computer and information sciences}
}

@misc{pan_preliminary_2023,
  title     = {A {Preliminary} {Evaluation} of {ChatGPT} for {Zero}-shot {Dialogue} {Understanding}},
  url       = {http://arxiv.org/abs/2304.04256},
  doi       = {10.48550/arXiv.2304.04256},
  abstract  = {Zero-shot dialogue understanding aims to enable dialogue to track the user's needs without any training data, which has gained increasing attention. In this work, we investigate the understanding ability of ChatGPT for zero-shot dialogue understanding tasks including spoken language understanding (SLU) and dialogue state tracking (DST). Experimental results on four popular benchmarks reveal the great potential of ChatGPT for zero-shot dialogue understanding. In addition, extensive analysis shows that ChatGPT benefits from the multi-turn interactive prompt in the DST task but struggles to perform slot filling for SLU. Finally, we summarize several unexpected behaviors of ChatGPT in dialogue understanding tasks, hoping to provide some insights for future research on building zero-shot dialogue understanding systems with Large Language Models (LLMs).},
  urldate   = {2023-08-27},
  publisher = {arXiv},
  author    = {Pan, Wenbo and Chen, Qiguang and Xu, Xiao and Che, Wanxiang and Qin, Libo},
  month     = apr,
  year      = {2023},
  note      = {arXiv:2304.04256 [cs]},
  keywords  = {Computer Science - Computation and Language, To Read}
}

@misc{weld_tri-level_2023,
  title     = {Tri-level {Joint} {Natural} {Language} {Understanding} for {Multi}-turn {Conversational} {Datasets}},
  url       = {http://arxiv.org/abs/2305.17729},
  doi       = {10.48550/arXiv.2305.17729},
  abstract  = {Natural language understanding typically maps single utterances to a dual level semantic frame, sentence level intent and slot labels at the word level. The best performing models force explicit interaction between intent detection and slot filling. We present a novel tri-level joint natural language understanding approach, adding domain, and explicitly exchange semantic information between all levels. This approach enables the use of multi-turn datasets which are a more natural conversational environment than single utterance. We evaluate our model on two multi-turn datasets for which we are the first to conduct joint slot-filling and intent detection. Our model outperforms state-of-the-art joint models in slot filling and intent detection on multi-turn data sets. We provide an analysis of explicit interaction locations between the layers. We conclude that including domain information improves model performance.},
  urldate   = {2023-08-27},
  publisher = {arXiv},
  author    = {Weld, Henry and Hu, Sijia and Long, Siqu and Poon, Josiah and Han, Soyeon Caren},
  month     = may,
  year      = {2023},
  note      = {arXiv:2305.17729 [cs]},
  keywords  = {Computer Science - Computation and Language}
}

@misc{tavares_task_2023,
  title     = {Task {Conditioned} {BERT} for {Joint} {Intent} {Detection} and {Slot}-filling},
  url       = {http://arxiv.org/abs/2308.06165},
  doi       = {10.48550/arXiv.2308.06165},
  abstract  = {Dialogue systems need to deal with the unpredictability of user intents to track dialogue state and the heterogeneity of slots to understand user preferences. In this paper we investigate the hypothesis that solving these challenges as one unified model will allow the transfer of parameter support data across the different tasks. The proposed principled model is based on a Transformer encoder, trained on multiple tasks, and leveraged by a rich input that conditions the model on the target inferences. Conditioning the Transformer encoder on multiple target inferences over the same corpus, i.e., intent and multiple slot types, allows learning richer language interactions than a single-task model would be able to. In fact, experimental results demonstrate that conditioning the model on an increasing number of dialogue inference tasks leads to improved results: on the MultiWOZ dataset, the joint intent and slot detection can be improved by 3.2{\textbackslash}\% by conditioning on intent, 10.8{\textbackslash}\% by conditioning on slot and 14.4{\textbackslash}\% by conditioning on both intent and slots. Moreover, on real conversations with Farfetch costumers, the proposed conditioned BERT can achieve high joint-goal and intent detection performance throughout a dialogue.},
  urldate   = {2023-08-27},
  publisher = {arXiv},
  author    = {Tavares, Diogo and Azevedo, Pedro and Semedo, David and Sousa, Ricardo and Magalhães, João},
  month     = aug,
  year      = {2023},
  note      = {arXiv:2308.06165 [cs]},
  keywords  = {Computer Science - Computation and Language, To Read}
}

@inproceedings{won_break_2023,
  address    = {Toronto, Canada},
  title      = {{BREAK}: {Breaking} the {Dialogue} {State} {Tracking} {Barrier} with {Beam} {Search} and {Re}-ranking},
  shorttitle = {{BREAK}},
  url        = {https://aclanthology.org/2023.acl-long.159},
  abstract   = {Despite the recent advances in dialogue state tracking (DST), the joint goal accuracy (JGA) of the existing methods on MultiWOZ 2.1 still remains merely 60\%. In our preliminary error analysis, we find that beam search produces a pool of candidates that is likely to include the correct dialogue state. Motivated by this observation, we introduce a novel framework, called BREAK (Beam search and RE-rAnKing), that achieves outstanding performance on DST. BREAK performs DST in two stages: (i) generating k-best dialogue state candidates with beam search and (ii) re-ranking the candidates to select the correct dialogue state. This simple yet powerful framework shows state-of-the-art performance on all versions of MultiWOZ and M2M datasets. Most notably, we push the joint goal accuracy to 80-90\% on MultiWOZ 2.1-2.4, which is an improvement of 23.6\%, 26.3\%, 21.7\%, and 10.8\% over the previous best-performing models, respectively. The data and code will be available at https://github.com/tony-won/DST-BREAK},
  urldate    = {2023-08-04},
  booktitle  = {Proceedings of the 61st {Annual} {Meeting} of the {Association} for {Computational} {Linguistics} ({Volume} 1: {Long} {Papers})},
  publisher  = {Association for Computational Linguistics},
  author     = {Won, Seungpil and Kwak, Heeyoung and Shin, Joongbo and Han, Janghoon and Jung, Kyomin},
  month      = jul,
  year       = {2023},
  keywords   = {To Read},
  pages      = {2832--2846}
}

@misc{wang_luna_2022,
  title      = {{LUNA}: {Learning} {Slot}-{Turn} {Alignment} for {Dialogue} {State} {Tracking}},
  shorttitle = {{LUNA}},
  url        = {http://arxiv.org/abs/2205.02550},
  doi        = {10.48550/arXiv.2205.02550},
  abstract   = {Dialogue state tracking (DST) aims to predict the current dialogue state given the dialogue history. Existing methods generally exploit the utterances of all dialogue turns to assign value for each slot. This could lead to suboptimal results due to the information introduced from irrelevant utterances in the dialogue history, which may be useless and can even cause confusion. To address this problem, we propose LUNA, a sLot-tUrN Alignment enhanced approach. It first explicitly aligns each slot with its most relevant utterance, then further predicts the corresponding value based on this aligned utterance instead of all dialogue utterances. Furthermore, we design a slot ranking auxiliary task to learn the temporal correlation among slots which could facilitate the alignment. Comprehensive experiments are conducted on multi-domain task-oriented dialogue datasets, i.e., MultiWOZ 2.0, MultiWOZ 2.1, and MultiWOZ 2.2. The results show that LUNA achieves new state-of-the-art results on these datasets.},
  urldate    = {2023-08-27},
  publisher  = {arXiv},
  author     = {Wang, Yifan and Zhao, Jing and Bao, Junwei and Duan, Chaoqun and Wu, Youzheng and He, Xiaodong},
  month      = may,
  year       = {2022},
  note       = {arXiv:2205.02550 [cs]},
  keywords   = {Computer Science - Computation and Language, To Read}
}

@inproceedings{mosharrof_toward_2023,
  title     = {Toward {Open}-domain {Slot} {Filling} via {Self}-supervised {Co}-training},
  url       = {http://arxiv.org/abs/2303.13801},
  doi       = {10.1145/3543507.3583541},
  abstract  = {Slot filling is one of the critical tasks in modern conversational systems. The majority of existing literature employs supervised learning methods, which require labeled training data for each new domain. Zero-shot learning and weak supervision approaches, among others, have shown promise as alternatives to manual labeling. Nonetheless, these learning paradigms are significantly inferior to supervised learning approaches in terms of performance. To minimize this performance gap and demonstrate the possibility of open-domain slot filling, we propose a Self-supervised Co-training framework, called SCot, that requires zero in-domain manually labeled training examples and works in three phases. Phase one acquires two sets of complementary pseudo labels automatically. Phase two leverages the power of the pre-trained language model BERT, by adapting it for the slot filling task using these sets of pseudo labels. In phase three, we introduce a self-supervised cotraining mechanism, where both models automatically select highconfidence soft labels to further improve the performance of the other in an iterative fashion. Our thorough evaluations show that SCot outperforms state-of-the-art models by 45.57\% and 37.56\% on SGD and MultiWoZ datasets, respectively. Moreover, our proposed framework SCot achieves comparable performance when compared to state-of-the-art fully supervised models.},
  urldate   = {2023-08-27},
  booktitle = {Proceedings of the {ACM} {Web} {Conference} 2023},
  author    = {Mosharrof, Adib and Fereidouni, Moghis and Siddique, A. B.},
  month     = apr,
  year      = {2023},
  note      = {arXiv:2303.13801 [cs]},
  keywords  = {Computer Science - Computation and Language, Computer Science - Machine Learning, To Read},
  pages     = {1928--1937}
}

@article{luo_zero-shot_2023,
  title     = {Zero-{Shot} {Slot} {Filling} with {Slot}-{Prefix} {Prompting} and {Attention} {Relationship} {Descriptor}},
  volume    = {37},
  copyright = {Copyright (c) 2023 Association for the Advancement of Artificial Intelligence},
  issn      = {2374-3468},
  url       = {https://ojs.aaai.org/index.php/AAAI/article/view/26566},
  doi       = {10.1609/aaai.v37i11.26566},
  abstract  = {This paper addresses zero-shot slot filling, which tries to build a system that can generalize to unseen slot types without any training data. The key to zero-shot slot-filling is to match the tokens from the utterance with the semantic definition of the slot without training data in the target domain. This paper tackles this problem by devising a scheme to fully leverage pre-trained language models (PLMs). To this end, we propose a new prompting scheme that utilizes both learnable tokens and slot names to guide the model to focus on the relevant text spans for a given slot. Furthermore, we use attention values between tokens to form a feature descriptor for each token, which is motivated by the fact that the attention value in a PLM naturally characterizes various relationships, e.g., syntactic or semantic, between tokens. By further consolidating those features with an additional transformer-based aggregation module, we create a simple-but-effective zero-shot slot filling system that can achieve significantly better performance than the previous methods, as demonstrated by our experimental studies.},
  language  = {en},
  number    = {11},
  urldate   = {2023-08-27},
  journal   = {Proceedings of the AAAI Conference on Artificial Intelligence},
  author    = {Luo, Qiaoyang and Liu, Lingqiao},
  month     = jun,
  year      = {2023},
  note      = {Number: 11},
  keywords  = {SNLP: Language Models, To Read},
  pages     = {13344--13352}
}

@inproceedings{tavares_learning_2023,
  address   = {New York, NY, USA},
  series    = {{SIGIR} '23},
  title     = {Learning to {Ask} {Questions} for {Zero}-shot {Dialogue} {State} {Tracking}},
  isbn      = {978-1-4503-9408-6},
  url       = {https://dl.acm.org/doi/10.1145/3539618.3592010},
  doi       = {10.1145/3539618.3592010},
  abstract  = {We present a method for performing zero-shot Dialogue State Tracking (DST) by casting the task as a learning-to-ask-questions framework. The framework learns to pair the best question generation (QG) strategy with in-domain question answering (QA) methods to extract slot values from a dialogue without any human intervention. A novel self-supervised QA pretraining step using in-domain data is essential to learn the structure without requiring any slot-filling annotations. Moreover, we show that QG methods need to be aligned with the same grammatical person used in the dialogue. Empirical evaluation on the MultiWOZ 2.1 dataset demonstrates that our approach, when used alongside robust QA models, outperforms existing zero-shot methods in the challenging task of zero-shot cross domain adaptation-given a comparable amount of domain knowledge during data creation. Finally, we analyze the impact of the types of questions used, and demonstrate that the algorithmic approach outperforms template-based question generation.},
  urldate   = {2023-08-27},
  booktitle = {Proceedings of the 46th {International} {ACM} {SIGIR} {Conference} on {Research} and {Development} in {Information} {Retrieval}},
  publisher = {Association for Computing Machinery},
  author    = {Tavares, Diogo and Semedo, David and Rudnicky, Alexander and Magalhaes, Joao},
  month     = jul,
  year      = {2023},
  keywords  = {To Read, dialogue state tracking, question answering, zero-shot},
  pages     = {2118--2122}
}

@article{ji_survey_2023,
  title    = {Survey of {Hallucination} in {Natural} {Language} {Generation}},
  volume   = {55},
  issn     = {0360-0300, 1557-7341},
  url      = {http://arxiv.org/abs/2202.03629},
  doi      = {10.1145/3571730},
  abstract = {Natural Language Generation (NLG) has improved exponentially in recent years thanks to the development of sequence-to-sequence deep learning technologies such as Transformer-based language models. This advancement has led to more fluent and coherent NLG, leading to improved development in downstream tasks such as abstractive summarization, dialogue generation and data-to-text generation. However, it is also apparent that deep learning based generation is prone to hallucinate unintended text, which degrades the system performance and fails to meet user expectations in many real-world scenarios. To address this issue, many studies have been presented in measuring and mitigating hallucinated texts, but these have never been reviewed in a comprehensive manner before. In this survey, we thus provide a broad overview of the research progress and challenges in the hallucination problem in NLG. The survey is organized into two parts: (1) a general overview of metrics, mitigation methods, and future directions; and (2) an overview of task-specific research progress on hallucinations in the following downstream tasks, namely abstractive summarization, dialogue generation, generative question answering, data-to-text generation, machine translation, and visual-language generation. This survey serves to facilitate collaborative efforts among researchers in tackling the challenge of hallucinated texts in NLG.},
  number   = {12},
  urldate  = {2023-08-04},
  journal  = {ACM Computing Surveys},
  author   = {Ji, Ziwei and Lee, Nayeon and Frieske, Rita and Yu, Tiezheng and Su, Dan and Xu, Yan and Ishii, Etsuko and Bang, Yejin and Dai, Wenliang and Madotto, Andrea and Fung, Pascale},
  month    = dec,
  year     = {2023},
  note     = {arXiv:2202.03629 [cs]},
  keywords = {A.1, Computer Science - Computation and Language},
  pages    = {1--38}
}

@article{yang_domain-transfer_2023,
  title     = {A {Domain}-{Transfer} {Meta} {Task} {Design} {Paradigm} for {Few}-{Shot} {Slot} {Tagging}},
  volume    = {37},
  copyright = {Copyright (c) 2023 Association for the Advancement of Artificial Intelligence},
  issn      = {2374-3468},
  url       = {https://ojs.aaai.org/index.php/AAAI/article/view/26626},
  doi       = {10.1609/aaai.v37i11.26626},
  abstract  = {Few-shot slot tagging is an important task in dialogue systems and attracts much attention of researchers. Most previous few-shot slot tagging methods utilize meta-learning procedure for training and strive to construct a large number of different meta tasks to simulate the testing situation of insufficient data. However, there is a widespread phenomenon of overlap slot between two domains in slot tagging. Traditional meta tasks ignore this special phenomenon and cannot simulate such realistic few-shot slot tagging scenarios. It violates the basic principle of meta-learning which the meta task is consistent with the real testing task, leading to historical information forgetting problem. In this paper, we introduce a novel domain-transfer meta task design paradigm to tackle this problem. We distribute a basic domain to each target domain based on the coincidence degree of slot labels between these two domains. Unlike classic meta tasks which only rely on small samples of target domain, our meta tasks aim to correctly infer the class of target domain query samples based on both abundant data in basic domain and scarce data in target domain. To accomplish our meta task, we propose a Task Adaptation Network to effectively transfer the historical information from the basic domain to the target domain. We carry out sufficient experiments on the benchmark slot tagging dataset SNIPS and the name entity recognition dataset NER. Results demonstrate that our proposed model outperforms previous methods and achieves the state-of-the-art performance.},
  language  = {en},
  number    = {11},
  urldate   = {2023-08-04},
  journal   = {Proceedings of the AAAI Conference on Artificial Intelligence},
  author    = {Yang, Fengyi and Zhou, Xi and Yang, Yating and Ma, Bo and Dong, Rui and Atawulla, Abibulla},
  month     = jun,
  year      = {2023},
  note      = {Number: 11},
  keywords  = {SNLP: Text Mining},
  pages     = {13887--13895}
}

@inproceedings{cheng_is_2022,
  address    = {Abu Dhabi, United Arab Emirates},
  title      = {Is {MultiWOZ} a {Solved} {Task}? {An} {Interactive} {TOD} {Evaluation} {Framework} with {User} {Simulator}},
  shorttitle = {Is {MultiWOZ} a {Solved} {Task}?},
  url        = {https://aclanthology.org/2022.findings-emnlp.90},
  abstract   = {Task-Oriented Dialogue (TOD) systems are drawing more and more attention in recent studies.Current methods focus on constructing pre-trained models or fine-tuning strategies while the evaluation of TOD is limited by a policy mismatch problem.That is, during evaluation, the user utterances are from the annotated dataset while these utterances should interact with previous responses which can have many alternatives besides annotated texts.Therefore, in this work, we propose an interactive evaluation framework for TOD. We first build a goal-oriented user simulator based on pre-trained models and then use the user simulator to interact with the dialogue system to generate dialogues.Besides, we introduce a sentence-level and a session-level score to measure the sentence fluency and session coherence in the interactive evaluation. Experimental results show that RL-based TOD systems trained by our proposed user simulator can achieve nearly 98\% inform and success rates in the interactive evaluation of MultiWOZ dataset and the proposed scores measure the response quality besides the inform and success rates.We are hoping that our work will encourage simulator-based interactive evaluations in the TOD task.},
  urldate    = {2023-08-04},
  booktitle  = {Findings of the {Association} for {Computational} {Linguistics}: {EMNLP} 2022},
  publisher  = {Association for Computational Linguistics},
  author     = {Cheng, Qinyuan and Li, Linyang and Quan, Guofeng and Gao, Feng and Mou, Xiaofeng and Qiu, Xipeng},
  month      = dec,
  year       = {2022},
  pages      = {1248--1259}
}
@misc{beurerkellner2024guidingllmsrightway,
  title         = {Guiding LLMs The Right Way: Fast, Non-Invasive Constrained Generation},
  author        = {Luca Beurer-Kellner and Marc Fischer and Martin Vechev},
  year          = {2024},
  eprint        = {2403.06988},
  archiveprefix = {arXiv},
  primaryclass  = {cs.LG},
  url           = {https://arxiv.org/abs/2403.06988}
}
@misc{abercrombie_mirages_2023,
  title      = {Mirages: {On} {Anthropomorphism} in {Dialogue} {Systems}},
  shorttitle = {Mirages},
  url        = {http://arxiv.org/abs/2305.09800},
  doi        = {10.48550/arXiv.2305.09800},
  abstract   = {Automated dialogue or conversational systems are anthropomorphised by developers and personified by users. While a degree of anthropomorphism is inevitable, conscious and unconscious design choices can guide users to personify them to varying degrees. Encouraging users to relate to automated systems as if they were human can lead to transparency and trust issues, and high risk scenarios caused by over-reliance on their outputs. As a result, natural language processing researchers have begun to investigate factors that induce personification and develop resources to mitigate such effects. However, these efforts are fragmented, and many aspects of anthropomorphism have yet to be considered. In this paper, we discuss the linguistic factors that contribute to the anthropomorphism of dialogue systems and the harms that can arise, arguing that it can reinforce stereotypes of gender roles and notions of acceptable language. We recommend that future efforts towards developing dialogue systems take particular care in their design, development, release, and description; and attend to the many linguistic cues that can elicit personification by users.},
  urldate    = {2023-08-04},
  publisher  = {arXiv},
  author     = {Abercrombie, Gavin and Curry, Amanda Cercas and Dinkar, Tanvi and Talat, Zeerak},
  month      = may,
  year       = {2023},
  note       = {arXiv:2305.09800 [cs]},
  keywords   = {Computer Science - Computation and Language}
}

@misc{li_guiding_2023,
  title     = {Guiding {Large} {Language} {Models} via {Directional} {Stimulus} {Prompting}},
  url       = {http://arxiv.org/abs/2302.11520},
  doi       = {10.48550/arXiv.2302.11520},
  abstract  = {We introduce a novel prompting framework called Directional Stimulus Prompting for guiding black-box large language models (LLMs) toward desired outputs. The framework introduces a new component called directional stimulus into the prompt, providing more fine-grained guidance and control over LLMs. The directional stimulus serves as hints or cues for each input query to guide LLMs toward the desired output, such as keywords that the desired summary should include for summarization. We utilize a small tunable model (e.g., T5) to generate such directional stimulus for each query, allowing us to optimize black-box LLMs by optimizing a small policy model. This policy model can be trained through 1) supervised fine-tuning using labeled data and 2) reinforcement learning from offline or online rewards to explore directional stimulus that better aligns LLMs with desired behaviors. We evaluate our framework on summarization and dialogue response generation tasks. Experimental results show that our framework consistently improves ChatGPT's performance over standard prompting with a small collection of training data, and reinforcement learning further improves the performance. Notably, on the MultWOZ dataset, our framework enables ChatGPT to achieve a remarkable 41.4\% improvement in its combined score with only 80 dialogues, matching or even surpassing the performance of some fully trained state-of-the-art models. We have made our code publicly available.},
  urldate   = {2023-08-04},
  publisher = {arXiv},
  author    = {Li, Zekun and Peng, Baolin and He, Pengcheng and Galley, Michel and Gao, Jianfeng and Yan, Xifeng},
  month     = jul,
  year      = {2023},
  note      = {arXiv:2302.11520 [cs]},
  keywords  = {Computer Science - Computation and Language}
}

@inproceedings{basu_strategies_2022,
  address   = {Seattle, USA},
  title     = {Strategies to {Improve} {Few}-shot {Learning} for {Intent} {Classification} and {Slot}-{Filling}},
  url       = {https://aclanthology.org/2022.suki-1.3},
  doi       = {10.18653/v1/2022.suki-1.3},
  abstract  = {Intent classification (IC) and slot filling (SF) are two fundamental tasks in modern Natural Language Understanding (NLU) systems. Collecting and annotating large amounts of data to train deep learning models for such systems are not scalable. This problem can be addressed by learning from few examples using fast supervised meta-learning techniques such as prototypical networks. In this work, we systematically investigate how contrastive learning and data augmentation methods can benefit these existing meta-learning pipelines for jointly modelled IC/SF tasks. Through extensive experiments across standard IC/SF benchmarks (SNIPS and ATIS), we show that our proposed approaches outperform standard meta-learning methods: contrastive losses as a regularizer in conjunction with prototypical networks consistently outperform the existing state-of-the-art for both IC and SF tasks, while data augmentation strategies primarily improve few-shot IC by a significant margin},
  urldate   = {2023-08-04},
  booktitle = {Proceedings of the {Workshop} on {Structured} and {Unstructured} {Knowledge} {Integration} ({SUKI})},
  publisher = {Association for Computational Linguistics},
  author    = {Basu, Samyadeep and Sharaf, Amr and Ip Kiun Chong, Karine and Fischer, Alex and Rohra, Vishal and Amoake, Michael and El-Hammamy, Hazem and Nosakhare, Ehi and Ramani, Vijay and Han, Benjamin},
  month     = jul,
  year      = {2022},
  pages     = {17--25}
}

@inproceedings{aksu_prompter_2023,
  address    = {Toronto, Canada},
  title      = {Prompter: {Zero}-shot {Adaptive} {Prefixes} for {Dialogue} {State} {Tracking} {Domain} {Adaptation}},
  shorttitle = {Prompter},
  url        = {https://aclanthology.org/2023.acl-long.252},
  abstract   = {A challenge in the Dialogue State Tracking (DST) field is adapting models to new domains without using any supervised data — zero-shot domain adaptation. Parameter-Efficient Transfer Learning (PETL) has the potential to address this problem due to its robustness. However, it has yet to be applied to the zero-shot scenarios, as it is not clear how to apply it unsupervisedly. Our method, Prompter, uses descriptions of target domain slots to generate dynamic prefixes that are concatenated to the key and values at each layer's self-attention mechanism. This allows for the use of prefix-tuning in zero-shot. Prompter outperforms previous methods on both the MultiWOZ and SGD benchmarks. In generating prefixes, our analyses find that Prompter not only utilizes the semantics of slot descriptions but also how often the slots appear together in conversation. Moreover, Prompter's gains are due to its improved ability to distinguish ”none”-valued dialogue slots, compared against baselines.},
  urldate    = {2023-08-04},
  booktitle  = {Proceedings of the 61st {Annual} {Meeting} of the {Association} for {Computational} {Linguistics} ({Volume} 1: {Long} {Papers})},
  publisher  = {Association for Computational Linguistics},
  author     = {Aksu, Ibrahim Taha and Kan, Min-Yen and Chen, Nancy},
  month      = jul,
  year       = {2023},
  pages      = {4588--4603}
}

@misc{bang_task-optimized_2023-1,
  title     = {Task-{Optimized} {Adapters} for an {End}-to-{End} {Task}-{Oriented} {Dialogue} {System}},
  url       = {http://arxiv.org/abs/2305.02468},
  doi       = {10.48550/arXiv.2305.02468},
  abstract  = {Task-Oriented Dialogue (TOD) systems are designed to carry out specific tasks by tracking dialogue states and generating appropriate responses to help users achieve defined goals. Recently, end-to-end dialogue models pre-trained based on large datasets have shown promising performance in the conversational system. However, they share the same parameters to train tasks of the dialogue system (NLU, DST, NLG), so debugging each task is challenging. Also, they require a lot of effort to fine-tune large parameters to create a task-oriented chatbot, making it difficult for non-experts to handle. Therefore, we intend to train relatively lightweight and fast models compared to PLM. In this paper, we propose an End-to-end TOD system with Task-Optimized Adapters which learn independently per task, adding only small number of parameters after fixed layers of pre-trained network. We also enhance the performance of the DST and NLG modules through reinforcement learning, overcoming the learning curve that has lacked at the adapter learning and enabling the natural and consistent response generation that is appropriate for the goal. Our method is a model-agnostic approach and does not require prompt-tuning as only input data without a prompt. As results of the experiment, our method shows competitive performance on the MultiWOZ benchmark compared to the existing end-to-end models. In particular, we attain state-of-the-art performance on the DST task of 2.2 dataset.},
  urldate   = {2023-08-04},
  publisher = {arXiv},
  author    = {Bang, Namo and Lee, Jeehyun and Koo, Myoung-Wan},
  month     = may,
  year      = {2023},
  note      = {arXiv:2305.02468 [cs]},
  keywords  = {Computer Science - Computation and Language}
}

@misc{hudecek_are_2023,
  title     = {Are {LLMs} {All} {You} {Need} for {Task}-{Oriented} {Dialogue}?},
  url       = {http://arxiv.org/abs/2304.06556},
  doi       = {10.48550/arXiv.2304.06556},
  abstract  = {Instructions-tuned Large Language Models (LLMs) gained recently huge popularity thanks to their ability to interact with users through conversation. In this work we aim to evaluate their ability to complete multi-turn tasks and interact with external databases in the context of established task-oriented dialogue benchmarks. We show that for explicit belief state tracking, LLMs underperform compared to specialized task-specific models. Nevertheless, they show ability to guide the dialogue to successful ending if given correct slot values. Furthermore this ability improves with access to true belief state distribution or in-domain examples.},
  urldate   = {2023-08-04},
  publisher = {arXiv},
  author    = {Hudeček, Vojtěch and Dušek, Ondřej},
  month     = aug,
  year      = {2023},
  note      = {arXiv:2304.06556 [cs]},
  keywords  = {Computer Science - Computation and Language}
}

@inproceedings{jacqmin_you_2022,
  address    = {Edinburgh, UK},
  title      = {“{Do} you follow me?”: {A} {Survey} of {Recent} {Approaches} in {Dialogue} {State} {Tracking}},
  shorttitle = {“{Do} you follow me?},
  url        = {https://aclanthology.org/2022.sigdial-1.33},
  abstract   = {While communicating with a user, a task-oriented dialogue system has to track the user's needs at each turn according to the conversation history. This process called dialogue state tracking (DST) is crucial because it directly informs the downstream dialogue policy. DST has received a lot of interest in recent years with the text-to-text paradigm emerging as the favored approach. In this review paper, we first present the task and its associated datasets. Then, considering a large number of recent publications, we identify highlights and advances of research in 2021-2022. Although neural approaches have enabled significant progress, we argue that some critical aspects of dialogue systems such as generalizability are still underexplored. To motivate future studies, we propose several research avenues.},
  urldate    = {2023-08-04},
  booktitle  = {Proceedings of the 23rd {Annual} {Meeting} of the {Special} {Interest} {Group} on {Discourse} and {Dialogue}},
  publisher  = {Association for Computational Linguistics},
  author     = {Jacqmin, Léo and Rojas Barahona, Lina M. and Favre, Benoit},
  month      = sep,
  year       = {2022},
  pages      = {336--350}
}

@inproceedings{gupta_show_2022-1,
  address    = {Seattle, United States},
  title      = {Show, {Don}'t {Tell}: {Demonstrations} {Outperform} {Descriptions} for {Schema}-{Guided} {Task}-{Oriented} {Dialogue}},
  shorttitle = {Show, {Don}'t {Tell}},
  url        = {https://aclanthology.org/2022.naacl-main.336},
  doi        = {10.18653/v1/2022.naacl-main.336},
  abstract   = {Building universal dialogue systems that operate across multiple domains/APIs and generalize to new ones with minimal overhead is a critical challenge. Recent works have leveraged natural language descriptions of schema elements to enable such systems; however, descriptions only indirectly convey schema semantics. In this work, we propose Show, Don't Tell, which prompts seq2seq models with a labeled example dialogue to show the semantics of schema elements rather than tell the model through descriptions. While requiring similar effort from service developers as generating descriptions, we show that using short examples as schema representations with large language models results in state-of-the-art performance on two popular dialogue state tracking benchmarks designed to measure zero-shot generalization - the Schema-Guided Dialogue dataset and the MultiWOZ leave-one-out benchmark.},
  urldate    = {2023-08-04},
  booktitle  = {Proceedings of the 2022 {Conference} of the {North} {American} {Chapter} of the {Association} for {Computational} {Linguistics}: {Human} {Language} {Technologies}},
  publisher  = {Association for Computational Linguistics},
  author     = {Gupta, Raghav and Lee, Harrison and Zhao, Jeffrey and Cao, Yuan and Rastogi, Abhinav and Wu, Yonghui},
  month      = jul,
  year       = {2022},
  pages      = {4541--4549}
}

@misc{su_selective_2022,
  title     = {Selective {Annotation} {Makes} {Language} {Models} {Better} {Few}-{Shot} {Learners}},
  url       = {http://arxiv.org/abs/2209.01975},
  doi       = {10.48550/arXiv.2209.01975},
  abstract  = {Many recent approaches to natural language tasks are built on the remarkable abilities of large language models. Large language models can perform in-context learning, where they learn a new task from a few task demonstrations, without any parameter updates. This work examines the implications of in-context learning for the creation of datasets for new natural language tasks. Departing from recent in-context learning methods, we formulate an annotation-efficient, two-step framework: selective annotation that chooses a pool of examples to annotate from unlabeled data in advance, followed by prompt retrieval that retrieves task examples from the annotated pool at test time. Based on this framework, we propose an unsupervised, graph-based selective annotation method, voke-k, to select diverse, representative examples to annotate. Extensive experiments on 10 datasets (covering classification, commonsense reasoning, dialogue, and text/code generation) demonstrate that our selective annotation method improves the task performance by a large margin. On average, vote-k achieves a 12.9\%/11.4\% relative gain under an annotation budget of 18/100, as compared to randomly selecting examples to annotate. Compared to state-of-the-art supervised finetuning approaches, it yields similar performance with 10-100x less annotation cost across 10 tasks. We further analyze the effectiveness of our framework in various scenarios: language models with varying sizes, alternative selective annotation methods, and cases where there is a test data domain shift. We hope that our studies will serve as a basis for data annotations as large language models are increasingly applied to new tasks. Our code is available at https://github.com/HKUNLP/icl-selective-annotation.},
  urldate   = {2023-08-04},
  publisher = {arXiv},
  author    = {Su, Hongjin and Kasai, Jungo and Wu, Chen Henry and Shi, Weijia and Wang, Tianlu and Xin, Jiayi and Zhang, Rui and Ostendorf, Mari and Zettlemoyer, Luke and Smith, Noah A. and Yu, Tao},
  month     = sep,
  year      = {2022},
  note      = {arXiv:2209.01975 [cs]},
  keywords  = {Computer Science - Computation and Language}
}

@inproceedings{sreedhar_prompt_2022,
  address   = {Abu Dhabi, Beijing (Hybrid)},
  title     = {Prompt {Learning} for {Domain} {Adaptation} in {Task}-{Oriented} {Dialogue}},
  url       = {https://aclanthology.org/2022.seretod-1.4},
  abstract  = {Conversation designers continue to face significant obstacles when creating productionquality task-oriented dialogue systems. The complexity and cost involved in schema development and data collection is often a major barrier for such designers, limiting their ability to create natural, user-friendly experiences. We frame the classification of user intent as the generation of a canonical form, a lightweight semantic representation using natural language. We show that canonical forms offer a promising alternative to traditional methods for intent classification. By tuning soft prompts for a frozen large language model, we show that canonical forms generalize very well to new, unseen domains in a zero- or few-shot setting. The method is also sample-efficient, reducing the complexity and effort of developing new task-oriented dialogue domains.},
  urldate   = {2023-08-04},
  booktitle = {Proceedings of the {Towards} {Semi}-{Supervised} and {Reinforced} {Task}-{Oriented} {Dialog} {Systems} ({SereTOD})},
  publisher = {Association for Computational Linguistics},
  author    = {Sreedhar, Makesh Narsimhan and Parisien, Christopher},
  month     = dec,
  year      = {2022},
  pages     = {24--30}
}

@misc{hu_-context_2022,
  title     = {In-{Context} {Learning} for {Few}-{Shot} {Dialogue} {State} {Tracking}},
  url       = {http://arxiv.org/abs/2203.08568},
  doi       = {10.48550/arXiv.2203.08568},
  abstract  = {Collecting and annotating task-oriented dialogues is time-consuming and costly; thus, zero and few shot learning could greatly benefit dialogue state tracking (DST). In this work, we propose an in-context learning (ICL) framework for zero-shot and few-shot learning DST, where a large pre-trained language model (LM) takes a test instance and a few exemplars as input, and directly decodes the dialogue state without any parameter updates. To better leverage a tabular domain description in the LM prompt, we reformulate DST into a text-to-SQL problem. We also propose a novel approach to retrieve annotated dialogues as exemplars. Empirical results on MultiWOZ show that our method IC-DST substantially outperforms previous fine-tuned state-of-the-art models in few-shot settings. In addition, we test IC-DST in zero-shot settings, in which the model only takes a fixed task instruction as input, finding that it outperforms previous zero-shot methods by a large margin.},
  urldate   = {2023-08-04},
  publisher = {arXiv},
  author    = {Hu, Yushi and Lee, Chia-Hsuan and Xie, Tianbao and Yu, Tao and Smith, Noah A. and Ostendorf, Mari},
  month     = oct,
  year      = {2022},
  note      = {arXiv:2203.08568 [cs]},
  keywords  = {Computer Science - Computation and Language}
}

@inproceedings{wang_slot_2022,
  address   = {Gyeongju, Republic of Korea},
  title     = {Slot {Dependency} {Modeling} for {Zero}-{Shot} {Cross}-{Domain} {Dialogue} {State} {Tracking}},
  url       = {https://aclanthology.org/2022.coling-1.42},
  urldate   = {2023-08-04},
  booktitle = {Proceedings of the 29th {International} {Conference} on {Computational} {Linguistics}},
  publisher = {International Committee on Computational Linguistics},
  author    = {Wang, Qingyue and Cao, Yanan and Li, Piji and Fu, Yanhe and Lin, Zheng and Guo, Li},
  month     = oct,
  year      = {2022},
  pages     = {510--520}
}

@inproceedings{dey_towards_2022,
  address   = {Dublin, Ireland},
  title     = {Towards {Fair} {Evaluation} of {Dialogue} {State} {Tracking} by {Flexible} {Incorporation} of {Turn}-level {Performances}},
  url       = {https://aclanthology.org/2022.acl-short.35},
  doi       = {10.18653/v1/2022.acl-short.35},
  abstract  = {Dialogue State Tracking (DST) is primarily evaluated using Joint Goal Accuracy (JGA) defined as the fraction of turns where the ground-truth dialogue state exactly matches the prediction. Generally in DST, the dialogue state or belief state for a given turn contain all the intents shown by the user till that turn. Due to this cumulative nature of the belief state, it is difficult to get a correct prediction once a misprediction has occurred. Thus, although being a useful metric, it can be harsh at times and underestimate the true potential of a DST model. Moreover, an improvement in JGA can sometimes decrease the performance of turn-level or non-cumulative belief state prediction due to inconsistency in annotations. So, using JGA as the only metric for model selection may not be ideal for all scenarios. In this work, we discuss various evaluation metrics used for DST along with their shortcomings. To address the existing issues, we propose a new evaluation metric named Flexible Goal Accuracy (FGA). FGA is a generalized version of JGA. But unlike JGA, it tries to give penalized rewards to mispredictions that are locally correct i.e. the root cause of the error is an earlier turn. By doing so, FGA considers the performance of both cumulative and turn-level prediction flexibly and provides a better insight than the existing metrics. We also show that FGA is a better discriminator of DST model performance.},
  urldate   = {2023-08-04},
  booktitle = {Proceedings of the 60th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics} ({Volume} 2: {Short} {Papers})},
  publisher = {Association for Computational Linguistics},
  author    = {Dey, Suvodip and Kummara, Ramamohan and Desarkar, Maunendra},
  month     = may,
  year      = {2022},
  pages     = {318--324}
}

@inproceedings{wang_deepstruct_2022,
  address    = {Dublin, Ireland},
  title      = {{DeepStruct}: {Pretraining} of {Language} {Models} for {Structure} {Prediction}},
  shorttitle = {{DeepStruct}},
  url        = {https://aclanthology.org/2022.findings-acl.67},
  doi        = {10.18653/v1/2022.findings-acl.67},
  abstract   = {We introduce a method for improving the structural understanding abilities of language models. Unlike previous approaches that finetune the models with task-specific augmentation, we pretrain language models to generate structures from the text on a collection of task-agnostic corpora. Our structure pretraining enables zero-shot transfer of the learned knowledge that models have about the structure tasks. We study the performance of this approach on 28 datasets, spanning 10 structure prediction tasks including open information extraction, joint entity and relation extraction, named entity recognition, relation classification, semantic role labeling, event extraction, coreference resolution, factual probe, intent detection, and dialogue state tracking. We further enhance the pretraining with the task-specific training sets. We show that a 10B parameter language model transfers non-trivially to most tasks and obtains state-of-the-art performance on 21 of 28 datasets that we evaluate. Our code and datasets will be made publicly available.},
  urldate    = {2023-08-04},
  booktitle  = {Findings of the {Association} for {Computational} {Linguistics}: {ACL} 2022},
  publisher  = {Association for Computational Linguistics},
  author     = {Wang, Chenguang and Liu, Xiao and Chen, Zui and Hong, Haoyun and Tang, Jie and Song, Dawn},
  month      = may,
  year       = {2022},
  pages      = {803--823}
}

@misc{rastogi_towards_2020,
  title      = {Towards {Scalable} {Multi}-domain {Conversational} {Agents}: {The} {Schema}-{Guided} {Dialogue} {Dataset}},
  shorttitle = {Towards {Scalable} {Multi}-domain {Conversational} {Agents}},
  url        = {http://arxiv.org/abs/1909.05855},
  doi        = {10.48550/arXiv.1909.05855},
  abstract   = {Virtual assistants such as Google Assistant, Alexa and Siri provide a conversational interface to a large number of services and APIs spanning multiple domains. Such systems need to support an ever-increasing number of services with possibly overlapping functionality. Furthermore, some of these services have little to no training data available. Existing public datasets for task-oriented dialogue do not sufficiently capture these challenges since they cover few domains and assume a single static ontology per domain. In this work, we introduce the the Schema-Guided Dialogue (SGD) dataset, containing over 16k multi-domain conversations spanning 16 domains. Our dataset exceeds the existing task-oriented dialogue corpora in scale, while also highlighting the challenges associated with building large-scale virtual assistants. It provides a challenging testbed for a number of tasks including language understanding, slot filling, dialogue state tracking and response generation. Along the same lines, we present a schema-guided paradigm for task-oriented dialogue, in which predictions are made over a dynamic set of intents and slots, provided as input, using their natural language descriptions. This allows a single dialogue system to easily support a large number of services and facilitates simple integration of new services without requiring additional training data. Building upon the proposed paradigm, we release a model for dialogue state tracking capable of zero-shot generalization to new APIs, while remaining competitive in the regular setting.},
  urldate    = {2023-08-04},
  publisher  = {arXiv},
  author     = {Rastogi, Abhinav and Zang, Xiaoxue and Sunkara, Srinivas and Gupta, Raghav and Khaitan, Pranav},
  month      = jan,
  year       = {2020},
  note       = {arXiv:1909.05855 [cs]},
  keywords   = {Computer Science - Computation and Language}
}

@Comment{jabref-meta: databaseType:bibtex;}

@inproceedings{Wu2022,
  author    = {Wu, Yangjun and Wang, Han and Zhang, Dongxiang and Chen, Gang and Zhang, Hao},
  booktitle = {Proceedings of the 29th International Conference on Computational Linguistics},
  title     = {Incorporating Instructional Prompts into a Unified Generative Framework for Joint Multiple Intent Detection and Slot Filling},
  year      = {2022},
  address   = {Gyeongju, Republic of Korea},
  editor    = {Calzolari, Nicoletta and Huang, Chu-Ren and Kim, Hansaem and Pustejovsky, James and Wanner, Leo and Choi, Key-Sun and Ryu, Pum-Mo and Chen, Hsin-Hsi and Donatelli, Lucia and Ji, Heng and Kurohashi, Sadao and Paggio, Patrizia and Xue, Nianwen and Kim, Seokhwan and Hahm, Younggyun and He, Zhong and Lee, Tony Kyungil and Santus, Enrico and Bond, Francis and Na, Seung-Hoon},
  month     = oct,
  pages     = {7203--7208},
  publisher = {International Committee on Computational Linguistics},
  abstract  = {The joint multiple Intent Detection (ID) and Slot Filling (SF) is a significant challenge in spoken language understanding. Because the slots in an utterance may relate to multi-intents, most existing approaches focus on utilizing task-specific components to capture the relations between intents and slots. The customized networks restrict models from modeling commonalities between tasks and generalization for broader applications. To address the above issue, we propose a Unified Generative framework (UGEN) based on a prompt-based paradigm, and formulate the task as a question-answering problem. Specifically, we design 5-type templates as instructional prompts, and each template includes a question that acts as the driver to teach UGEN to grasp the paradigm, options that list the candidate intents or slots to reduce the answer search space, and the context denotes original utterance. Through the instructional prompts, UGEN is guided to understand intents, slots, and their implicit correlations. On two popular multi-intent benchmark datasets, experimental results demonstrate that UGEN achieves new SOTA performances on full-data and surpasses the baselines by a large margin on 5-shot (28.1{\%}) and 10-shot (23{\%}) scenarios, which verify that UGEN is robust and effective.},
  groups    = {AttempsSLU},
  url       = {https://aclanthology.org/2022.coling-1.631}
}

@inproceedings{Wang2023_fewshotclassif,
  author    = {Wang, Yufan and Mei, Jie and Zou, Bowei and Fan, Rui and He, Tingting and Aw, Ai Ti},
  booktitle = {Findings of the Association for Computational Linguistics: ACL 2023},
  title     = {Making Pre-trained Language Models Better Learn Few-Shot Spoken Language Understanding in More Practical Scenarios},
  year      = {2023},
  address   = {Toronto, Canada},
  editor    = {Rogers, Anna and Boyd-Graber, Jordan and Okazaki, Naoaki},
  month     = jul,
  pages     = {13508--13523},
  publisher = {Association for Computational Linguistics},
  abstract  = {Most previous few-shot Spoken Language Understanding (SLU) models typically need to be trained on a set of data-rich source domains and adapt to the target domain with a few examples. In this paper, we explore a more practical scenario for few-shot SLU, in which we only assume access to a pre-trained language model and a few labeled examples without any other source domain data. We concentrate on understanding how far the few-shot SLU could be pushed in this setting. To this end, we develop a prompt-based intent detection model in few-shot settings, which leverages the BERT original pre-training next sentence prediction task and the prompt template to detect the user{'}s intent. For slot filling, we propose an approach of reconstructing slot labels, which reduces the training complexity by reducing the number of slot labels in few-shot settings. To evaluate the few-shot SLU for a more practical scenario, we present two benchmarks, FewShotATIS and FewShotSNIPS. And a dynamic sampling strategy is designed to construct the two datasets according to the learning difficulty of each intent and slot. Experiments on FewShotATIS and FewShotSNIPS demonstrate that our proposed model achieves state-of-the-art performance.},
  doi       = {10.18653/v1/2023.findings-acl.853},
  groups    = {AttempsSLU},
  url       = {https://aclanthology.org/2023.findings-acl.853}
}

'
@techreport{Wang2023a,
  author     = {Wang, Shuhe and Sun, Xiaofei and Li, Xiaoya and Ouyang, Rongbin and Wu, Fei and Zhang, Tianwei and Li, Jiwei and Wang, Guoyin},
  title      = {{GPT}-{NER}: {Named} {Entity} {Recognition} via {Large} {Language} {Models}},
  year       = {2023},
  month      = oct,
  note       = {arXiv:2304.10428 [cs] type: article},
  abstract   = {Despite the fact that large-scale Language Models (LLM) have achieved SOTA performances on a variety of NLP tasks, its performance on NER is still significantly below supervised baselines. This is due to the gap between the two tasks the NER and LLMs: the former is a sequence labeling task in nature while the latter is a text-generation model. In this paper, we propose GPT-NER to resolve this issue. GPT-NER bridges the gap by transforming the sequence labeling task to a generation task that can be easily adapted by LLMs e.g., the task of finding location entities in the input text "Columbus is a city" is transformed to generate the text sequence "@@Columbus\#\# is a city", where special tokens @@\#\# marks the entity to extract. To efficiently address the "hallucination" issue of LLMs, where LLMs have a strong inclination to over-confidently label NULL inputs as entities, we propose a self-verification strategy by prompting LLMs to ask itself whether the extracted entities belong to a labeled entity tag. We conduct experiments on five widely adopted NER datasets, and GPT-NER achieves comparable performances to fully supervised baselines, which is the first time as far as we are concerned. More importantly, we find that GPT-NER exhibits a greater ability in the low-resource and few-shot setups, when the amount of training data is extremely scarce, GPT-NER performs significantly better than supervised models. This demonstrates the capabilities of GPT-NER in real-world NER applications where the number of labeled examples is limited.},
  file       = {:Wang2023a - GPT NER_ Named Entity Recognition Via Large Language Models.pdf:PDF},
  groups     = {Tagging Prompting},
  keywords   = {Computer Science - Computation and Language},
  school     = {arXiv},
  shorttitle = {{GPT}-{NER}},
  url        = {http://arxiv.org/abs/2304.10428},
  urldate    = {2024-04-10}
}

@Comment{jabref-meta: databaseType:bibtex;}

@Comment{jabref-meta: grouping:
0 AllEntriesGroup:;
1 StaticGroup:AttempsSLU\;0\;0\;0x99cc99ff\;\;\;;
1 StaticGroup:Constrained Prompting\;0\;1\;0xff0000ff\;\;\;;
1 StaticGroup:General LLM-Prompting\;0\;1\;0x8a8a8aff\;\;\;;
1 StaticGroup:Tagging Prompting\;0\;1\;0x99b3ffff\;\;\;;
}

@inproceedings{zhu-etal-2024-zero-shot,
  title     = {Zero-Shot Spoken Language Understanding via Large Language Models: A Preliminary Study},
  author    = {Zhu, Zhihong  and
               Cheng, Xuxin  and
               An, Hao  and
               Wang, Zhichang  and
               Chen, Dongsheng  and
               Huang, Zhiqi},
  editor    = {Calzolari, Nicoletta  and
               Kan, Min-Yen  and
               Hoste, Veronique  and
               Lenci, Alessandro  and
               Sakti, Sakriani  and
               Xue, Nianwen},
  booktitle = {Proceedings of the 2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation (LREC-COLING 2024)},
  month     = may,
  year      = {2024},
  address   = {Torino, Italia},
  publisher = {ELRA and ICCL},
  url       = {https://aclanthology.org/2024.lrec-main.1554},
  pages     = {17877--17883},
  abstract  = {Zero-shot Spoken Language Understanding (SLU) aims to enable task-oriented dialogue systems to understand user needs without training data. Challenging but worthwhile, zero-shot SLU reduces the time and effort that data labeling takes. Recent advancements in large language models (LLMs), such as GPT3.5 and ChatGPT, have shown promising results in zero-shot settings, which motivates us to explore prompt-based methods. In this study, we investigate whether strong SLU models can be constructed by directly prompting LLMs. Specifically, we propose a simple yet effective two-stage framework dubbed GPT-SLU, which transforms the SLU task into a question-answering problem. Powered by multi-stage mutual guided prompts, GPT-SLU can leverage the correlations between two subtasks in SLU to achieve better predictions, which is greatly explored in the traditional fine-tuning paradigm. Experimental results on three SLU benchmark datasets demonstrate the significant potential of LLMs for zero-shot SLU. Comprehensive analyses validate the effectiveness of our proposed framework and also indicate that there is still room for further improvement of LLMs in SLU scenarios.}
}

@misc{allal2024SmolLM,
  title  = {SmolLM - blazingly fast and remarkably powerful},
  author = {Loubna Ben Allal and Anton Lozhkov and Elie Bakouch and Leandro von Werra and Thomas Wolf},
  year   = {2024},
  url    = {https://huggingface.co/blog/smollm}
}
@misc{abdin2024phi3technicalreporthighly,
  title         = {Phi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone},
  author        = {Marah Abdin and Sam Ade Jacobs and Ammar Ahmad Awan and Jyoti Aneja and Ahmed Awadallah and Hany Awadalla and Nguyen Bach and Amit Bahree and Arash Bakhtiari and Jianmin Bao and Harkirat Behl and Alon Benhaim and Misha Bilenko and Johan Bjorck and Sébastien Bubeck and Qin Cai and Martin Cai and Caio César Teodoro Mendes and Weizhu Chen and Vishrav Chaudhary and Dong Chen and Dongdong Chen and Yen-Chun Chen and Yi-Ling Chen and Parul Chopra and Xiyang Dai and Allie Del Giorno and Gustavo de Rosa and Matthew Dixon and Ronen Eldan and Victor Fragoso and Dan Iter and Mei Gao and Min Gao and Jianfeng Gao and Amit Garg and Abhishek Goswami and Suriya Gunasekar and Emman Haider and Junheng Hao and Russell J. Hewett and Jamie Huynh and Mojan Javaheripi and Xin Jin and Piero Kauffmann and Nikos Karampatziakis and Dongwoo Kim and Mahoud Khademi and Lev Kurilenko and James R. Lee and Yin Tat Lee and Yuanzhi Li and Yunsheng Li and Chen Liang and Lars Liden and Ce Liu and Mengchen Liu and Weishung Liu and Eric Lin and Zeqi Lin and Chong Luo and Piyush Madan and Matt Mazzola and Arindam Mitra and Hardik Modi and Anh Nguyen and Brandon Norick and Barun Patra and Daniel Perez-Becker and Thomas Portet and Reid Pryzant and Heyang Qin and Marko Radmilac and Corby Rosset and Sambudha Roy and Olatunji Ruwase and Olli Saarikivi and Amin Saied and Adil Salim and Michael Santacroce and Shital Shah and Ning Shang and Hiteshi Sharma and Swadheen Shukla and Xia Song and Masahiro Tanaka and Andrea Tupini and Xin Wang and Lijuan Wang and Chunyu Wang and Yu Wang and Rachel Ward and Guanhua Wang and Philipp Witte and Haiping Wu and Michael Wyatt and Bin Xiao and Can Xu and Jiahang Xu and Weijian Xu and Sonali Yadav and Fan Yang and Jianwei Yang and Ziyi Yang and Yifan Yang and Donghan Yu and Lu Yuan and Chengruidong Zhang and Cyril Zhang and Jianwen Zhang and Li Lyna Zhang and Yi Zhang and Yue Zhang and Yunan Zhang and Xiren Zhou},
  year          = {2024},
  eprint        = {2404.14219},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL},
  url           = {https://arxiv.org/abs/2404.14219}
}

@misc{teknium2024hermes3technicalreport,
  title         = {Hermes 3 Technical Report},
  author        = {Ryan Teknium and Jeffrey Quesnelle and Chen Guang},
  year          = {2024},
  eprint        = {2408.11857},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL},
  url           = {https://arxiv.org/abs/2408.11857}
}

@misc{dubey2024llama3herdmodels,
  title         = {The Llama 3 Herd of Models},
  author        = {Abhimanyu Dubey and Abhinav Jauhri and Abhinav Pandey and Abhishek Kadian and Ahmad Al-Dahle and Aiesha Letman and Akhil Mathur and Alan Schelten and Amy Yang and Angela Fan and Anirudh Goyal and Anthony Hartshorn and Aobo Yang and Archi Mitra and Archie Sravankumar and Artem Korenev and Arthur Hinsvark and Arun Rao and Aston Zhang and Aurelien Rodriguez and Austen Gregerson and Ava Spataru and Baptiste Roziere and Bethany Biron and Binh Tang and Bobbie Chern and Charlotte Caucheteux and Chaya Nayak and Chloe Bi and Chris Marra and Chris McConnell and Christian Keller and Christophe Touret and Chunyang Wu and Corinne Wong and Cristian Canton Ferrer and Cyrus Nikolaidis and Damien Allonsius and Daniel Song and Danielle Pintz and Danny Livshits and David Esiobu and Dhruv Choudhary and Dhruv Mahajan and Diego Garcia-Olano and Diego Perino and Dieuwke Hupkes and Egor Lakomkin and Ehab AlBadawy and Elina Lobanova and Emily Dinan and Eric Michael Smith and Filip Radenovic and Frank Zhang and Gabriel Synnaeve and Gabrielle Lee and Georgia Lewis Anderson and Graeme Nail and Gregoire Mialon and Guan Pang and Guillem Cucurell and Hailey Nguyen and Hannah Korevaar and Hu Xu and Hugo Touvron and Iliyan Zarov and Imanol Arrieta Ibarra and Isabel Kloumann and Ishan Misra and Ivan Evtimov and Jade Copet and Jaewon Lee and Jan Geffert and Jana Vranes and Jason Park and Jay Mahadeokar and Jeet Shah and Jelmer van der Linde and Jennifer Billock and Jenny Hong and Jenya Lee and Jeremy Fu and Jianfeng Chi and Jianyu Huang and Jiawen Liu and Jie Wang and Jiecao Yu and Joanna Bitton and Joe Spisak and Jongsoo Park and Joseph Rocca and Joshua Johnstun and Joshua Saxe and Junteng Jia and Kalyan Vasuden Alwala and Kartikeya Upasani and Kate Plawiak and Ke Li and Kenneth Heafield and Kevin Stone and Khalid El-Arini and Krithika Iyer and Kshitiz Malik and Kuenley Chiu and Kunal Bhalla and Lauren Rantala-Yeary and Laurens van der Maaten and Lawrence Chen and Liang Tan and Liz Jenkins and Louis Martin and Lovish Madaan and Lubo Malo and Lukas Blecher and Lukas Landzaat and Luke de Oliveira and Madeline Muzzi and Mahesh Pasupuleti and Mannat Singh and Manohar Paluri and Marcin Kardas and Mathew Oldham and Mathieu Rita and Maya Pavlova and Melanie Kambadur and Mike Lewis and Min Si and Mitesh Kumar Singh and Mona Hassan and Naman Goyal and Narjes Torabi and Nikolay Bashlykov and Nikolay Bogoychev and Niladri Chatterji and Olivier Duchenne and Onur Çelebi and Patrick Alrassy and Pengchuan Zhang and Pengwei Li and Petar Vasic and Peter Weng and Prajjwal Bhargava and Pratik Dubal and Praveen Krishnan and Punit Singh Koura and Puxin Xu and Qing He and Qingxiao Dong and Ragavan Srinivasan and Raj Ganapathy and Ramon Calderer and Ricardo Silveira Cabral and Robert Stojnic and Roberta Raileanu and Rohit Girdhar and Rohit Patel and Romain Sauvestre and Ronnie Polidoro and Roshan Sumbaly and Ross Taylor and Ruan Silva and Rui Hou and Rui Wang and Saghar Hosseini and Sahana Chennabasappa and Sanjay Singh and Sean Bell and Seohyun Sonia Kim and Sergey Edunov and Shaoliang Nie and Sharan Narang and Sharath Raparthy and Sheng Shen and Shengye Wan and Shruti Bhosale and Shun Zhang and Simon Vandenhende and Soumya Batra and Spencer Whitman and Sten Sootla and Stephane Collot and Suchin Gururangan and Sydney Borodinsky and Tamar Herman and Tara Fowler and Tarek Sheasha and Thomas Georgiou and Thomas Scialom and Tobias Speckbacher and Todor Mihaylov and Tong Xiao and Ujjwal Karn and Vedanuj Goswami and Vibhor Gupta and Vignesh Ramanathan and Viktor Kerkez and Vincent Gonguet and Virginie Do and Vish Vogeti and Vladan Petrovic and Weiwei Chu and Wenhan Xiong and Wenyin Fu and Whitney Meers and Xavier Martinet and Xiaodong Wang and Xiaoqing Ellen Tan and Xinfeng Xie and Xuchao Jia and Xuewei Wang and Yaelle Goldschlag and Yashesh Gaur and Yasmine Babaei and Yi Wen and Yiwen Song and Yuchen Zhang and Yue Li and Yuning Mao and Zacharie Delpierre Coudert and Zheng Yan and Zhengxing Chen and Zoe Papakipos and Aaditya Singh and Aaron Grattafiori and Abha Jain and Adam Kelsey and Adam Shajnfeld and Adithya Gangidi and Adolfo Victoria and Ahuva Goldstand and Ajay Menon and Ajay Sharma and Alex Boesenberg and Alex Vaughan and Alexei Baevski and Allie Feinstein and Amanda Kallet and Amit Sangani and Anam Yunus and Andrei Lupu and Andres Alvarado and Andrew Caples and Andrew Gu and Andrew Ho and Andrew Poulton and Andrew Ryan and Ankit Ramchandani and Annie Franco and Aparajita Saraf and Arkabandhu Chowdhury and Ashley Gabriel and Ashwin Bharambe and Assaf Eisenman and Azadeh Yazdan and Beau James and Ben Maurer and Benjamin Leonhardi and Bernie Huang and Beth Loyd and Beto De Paola and Bhargavi Paranjape and Bing Liu and Bo Wu and Boyu Ni and Braden Hancock and Bram Wasti and Brandon Spence and Brani Stojkovic and Brian Gamido and Britt Montalvo and Carl Parker and Carly Burton and Catalina Mejia and Changhan Wang and Changkyu Kim and Chao Zhou and Chester Hu and Ching-Hsiang Chu and Chris Cai and Chris Tindal and Christoph Feichtenhofer and Damon Civin and Dana Beaty and Daniel Kreymer and Daniel Li and Danny Wyatt and David Adkins and David Xu and Davide Testuggine and Delia David and Devi Parikh and Diana Liskovich and Didem Foss and Dingkang Wang and Duc Le and Dustin Holland and Edward Dowling and Eissa Jamil and Elaine Montgomery and Eleonora Presani and Emily Hahn and Emily Wood and Erik Brinkman and Esteban Arcaute and Evan Dunbar and Evan Smothers and Fei Sun and Felix Kreuk and Feng Tian and Firat Ozgenel and Francesco Caggioni and Francisco Guzmán and Frank Kanayet and Frank Seide and Gabriela Medina Florez and Gabriella Schwarz and Gada Badeer and Georgia Swee and Gil Halpern and Govind Thattai and Grant Herman and Grigory Sizov and Guangyi and Zhang and Guna Lakshminarayanan and Hamid Shojanazeri and Han Zou and Hannah Wang and Hanwen Zha and Haroun Habeeb and Harrison Rudolph and Helen Suk and Henry Aspegren and Hunter Goldman and Ibrahim Damlaj and Igor Molybog and Igor Tufanov and Irina-Elena Veliche and Itai Gat and Jake Weissman and James Geboski and James Kohli and Japhet Asher and Jean-Baptiste Gaya and Jeff Marcus and Jeff Tang and Jennifer Chan and Jenny Zhen and Jeremy Reizenstein and Jeremy Teboul and Jessica Zhong and Jian Jin and Jingyi Yang and Joe Cummings and Jon Carvill and Jon Shepard and Jonathan McPhie and Jonathan Torres and Josh Ginsburg and Junjie Wang and Kai Wu and Kam Hou U and Karan Saxena and Karthik Prasad and Kartikay Khandelwal and Katayoun Zand and Kathy Matosich and Kaushik Veeraraghavan and Kelly Michelena and Keqian Li and Kun Huang and Kunal Chawla and Kushal Lakhotia and Kyle Huang and Lailin Chen and Lakshya Garg and Lavender A and Leandro Silva and Lee Bell and Lei Zhang and Liangpeng Guo and Licheng Yu and Liron Moshkovich and Luca Wehrstedt and Madian Khabsa and Manav Avalani and Manish Bhatt and Maria Tsimpoukelli and Martynas Mankus and Matan Hasson and Matthew Lennie and Matthias Reso and Maxim Groshev and Maxim Naumov and Maya Lathi and Meghan Keneally and Michael L. Seltzer and Michal Valko and Michelle Restrepo and Mihir Patel and Mik Vyatskov and Mikayel Samvelyan and Mike Clark and Mike Macey and Mike Wang and Miquel Jubert Hermoso and Mo Metanat and Mohammad Rastegari and Munish Bansal and Nandhini Santhanam and Natascha Parks and Natasha White and Navyata Bawa and Nayan Singhal and Nick Egebo and Nicolas Usunier and Nikolay Pavlovich Laptev and Ning Dong and Ning Zhang and Norman Cheng and Oleg Chernoguz and Olivia Hart and Omkar Salpekar and Ozlem Kalinli and Parkin Kent and Parth Parekh and Paul Saab and Pavan Balaji and Pedro Rittner and Philip Bontrager and Pierre Roux and Piotr Dollar and Polina Zvyagina and Prashant Ratanchandani and Pritish Yuvraj and Qian Liang and Rachad Alao and Rachel Rodriguez and Rafi Ayub and Raghotham Murthy and Raghu Nayani and Rahul Mitra and Raymond Li and Rebekkah Hogan and Robin Battey and Rocky Wang and Rohan Maheswari and Russ Howes and Ruty Rinott and Sai Jayesh Bondu and Samyak Datta and Sara Chugh and Sara Hunt and Sargun Dhillon and Sasha Sidorov and Satadru Pan and Saurabh Verma and Seiji Yamamoto and Sharadh Ramaswamy and Shaun Lindsay and Shaun Lindsay and Sheng Feng and Shenghao Lin and Shengxin Cindy Zha and Shiva Shankar and Shuqiang Zhang and Shuqiang Zhang and Sinong Wang and Sneha Agarwal and Soji Sajuyigbe and Soumith Chintala and Stephanie Max and Stephen Chen and Steve Kehoe and Steve Satterfield and Sudarshan Govindaprasad and Sumit Gupta and Sungmin Cho and Sunny Virk and Suraj Subramanian and Sy Choudhury and Sydney Goldman and Tal Remez and Tamar Glaser and Tamara Best and Thilo Kohler and Thomas Robinson and Tianhe Li and Tianjun Zhang and Tim Matthews and Timothy Chou and Tzook Shaked and Varun Vontimitta and Victoria Ajayi and Victoria Montanez and Vijai Mohan and Vinay Satish Kumar and Vishal Mangla and Vítor Albiero and Vlad Ionescu and Vlad Poenaru and Vlad Tiberiu Mihailescu and Vladimir Ivanov and Wei Li and Wenchen Wang and Wenwen Jiang and Wes Bouaziz and Will Constable and Xiaocheng Tang and Xiaofang Wang and Xiaojian Wu and Xiaolan Wang and Xide Xia and Xilun Wu and Xinbo Gao and Yanjun Chen and Ye Hu and Ye Jia and Ye Qi and Yenda Li and Yilin Zhang and Ying Zhang and Yossi Adi and Youngjin Nam and Yu and Wang and Yuchen Hao and Yundi Qian and Yuzi He and Zach Rait and Zachary DeVito and Zef Rosnbrick and Zhaoduo Wen and Zhenyu Yang and Zhiwei Zhao},
  year          = {2024},
  eprint        = {2407.21783},
  archiveprefix = {arXiv},
  primaryclass  = {cs.AI},
  url           = {https://arxiv.org/abs/2407.21783}
}
@misc{coucke2018snips,
  title         = {Snips Voice Platform: an embedded Spoken Language Understanding system for private-by-design voice interfaces},
  author        = {Alice Coucke and Alaa Saade and Adrien Ball and Théodore Bluche and Alexandre Caulier and David Leroy and Clément Doumouro and Thibault Gisselbrecht and Francesco Caltagirone and Thibaut Lavril and Maël Primet and Joseph Dureau},
  year          = {2018},
  eprint        = {1805.10190},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL}
}

@inproceedings{bonneaumaynard05_interspeech,
  author    = {H. Bonneau-Maynard and Sophie Rosset and C. Ayache and A. Kuhn and Djamel Mostefa},
  title     = {{Semantic annotation of the French media dialog corpus}},
  year      = 2005,
  booktitle = {Proc. Interspeech 2005},
  pages     = {3457--3460},
  doi       = {10.21437/Interspeech.2005-312},
  issn      = {2308-457X}
}

@inproceedings{alavoine:hal-04523286,
  title       = {{New Semantic Task for the French Spoken Language Understanding MEDIA Benchmark}},
  author      = {Alavoine, Nad{\`e}ge and Laperriere, Ga{\"e}lle and Servan, Christophe and Ghannay, Sahar and Rosset, Sophie},
  url         = {https://hal.science/hal-04523286},
  booktitle   = {{The 2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation (LREC-COLING 2024)}},
  address     = {Torino, Italy},
  series      = {The 2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation (LREC-COLING 2024)},
  year        = {2024},
  month       = May,
  keywords    = {Benchmark Dataset ; Spoken Language Understanding ; Joint Intent Detection And Slot-filling ; Tri-training},
  pdf         = {https://hal.science/hal-04523286/file/main.pdf},
  hal_id      = {hal-04523286},
  hal_version = {v1}
}
@misc{pan2023preliminary,
  title         = {A Preliminary Evaluation of ChatGPT for Zero-shot Dialogue Understanding},
  author        = {Wenbo Pan and Qiguang Chen and Xiao Xu and Wanxiang Che and Libo Qin},
  year          = {2023},
  eprint        = {2304.04256},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL}
}

@misc{jiao2023chatgpt,
  title         = {Is ChatGPT A Good Translator? Yes With GPT-4 As The Engine},
  author        = {Wenxiang Jiao and Wenxuan Wang and Jen-tse Huang and Xing Wang and Shuming Shi and Zhaopeng Tu},
  year          = {2023},
  eprint        = {2301.08745},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL}
}

@misc{he2023chatgpt,
  title         = {Can ChatGPT Detect Intent? Evaluating Large Language Models for Spoken Language Understanding},
  author        = {Mutian He and Philip N. Garner},
  year          = {2023},
  eprint        = {2305.13512},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL}
}

@inproceedings{wu-etal-2021-label,
  title     = {A Label-Aware {BERT} Attention Network for Zero-Shot Multi-Intent Detection in Spoken Language Understanding},
  author    = {Wu, Ting-Wei  and
               Su, Ruolin  and
               Juang, Biing},
  editor    = {Moens, Marie-Francine  and
               Huang, Xuanjing  and
               Specia, Lucia  and
               Yih, Scott Wen-tau},
  booktitle = {Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing},
  month     = nov,
  year      = {2021},
  address   = {Online and Punta Cana, Dominican Republic},
  publisher = {Association for Computational Linguistics},
  url       = {https://aclanthology.org/2021.emnlp-main.399},
  doi       = {10.18653/v1/2021.emnlp-main.399},
  pages     = {4884--4896},
  abstract  = {With the early success of query-answer assistants such as Alexa and Siri, research attempts to expand system capabilities of handling service automation are now abundant. However, preliminary systems have quickly found the inadequacy in relying on simple classification techniques to effectively accomplish the automation task. The main challenge is that the dialogue often involves complexity in user{'}s intents (or purposes) which are multiproned, subject to spontaneous change, and difficult to track. Furthermore, public datasets have not considered these complications and the general semantic annotations are lacking which may result in zero-shot problem. Motivated by the above, we propose a Label-Aware BERT Attention Network (LABAN) for zero-shot multi-intent detection. We first encode input utterances with BERT and construct a label embedded space by considering embedded semantics in intent labels. An input utterance is then classified based on its projection weights on each intent embedding in this embedded space. We show that it successfully extends to few/zero-shot setting where part of intent labels are unseen in training data, by also taking account of semantics in these unseen intent labels. Experimental results show that our approach is capable of detecting many unseen intent labels correctly. It also achieves the state-of-the-art performance on five multi-intent datasets in normal cases.}
}

@inproceedings{hemphill-etal-1990-atis,
  title     = {The {ATIS} Spoken Language Systems Pilot Corpus},
  author    = {Hemphill, Charles T.  and
               Godfrey, John J.  and
               Doddington, George R.},
  booktitle = {Speech and Natural Language: Proceedings of a Workshop Held at Hidden Valley, {P}ennsylvania, June 24-27,1990},
  year      = {1990},
  url       = {https://aclanthology.org/H90-1021}
}

@misc{jeblick2022chatgpt,
  title         = {ChatGPT Makes Medicine Easy to Swallow: An Exploratory Case Study on Simplified Radiology Reports},
  author        = {Katharina Jeblick and Balthasar Schachtner and Jakob Dexl and Andreas Mittermeier and Anna Theresa Stüber and Johanna Topalis and Tobias Weber and Philipp Wesp and Bastian Sabel and Jens Ricke and Michael Ingrisch},
  year          = {2022},
  eprint        = {2212.14882},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL}
}

@misc{jiao2023chatgpt,
  title         = {Is ChatGPT A Good Translator? Yes With GPT-4 As The Engine},
  author        = {Wenxiang Jiao and Wenxuan Wang and Jen-tse Huang and Xing Wang and Shuming Shi and Zhaopeng Tu},
  year          = {2023},
  eprint        = {2301.08745},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL}
}

@inproceedings{li23s_interspeech,
  author    = {Guangpeng Li and Lu Chen and Kai Yu},
  title     = {{How ChatGPT is Robust for Spoken Language Understanding?}},
  year      = 2023,
  booktitle = {Proc. INTERSPEECH 2023},
  pages     = {2163--2167},
  doi       = {10.21437/Interspeech.2023-1466},
  issn      = {2308-457X}
}

@misc{wang2023gptner,
  title         = {GPT-NER: Named Entity Recognition via Large Language Models},
  author        = {Shuhe Wang and Xiaofei Sun and Xiaoya Li and Rongbin Ouyang and Fei Wu and Tianwei Zhang and Jiwei Li and Guoyin Wang},
  year          = {2023},
  eprint        = {2304.10428},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL}
}

@misc{zhang2023stance,
  title         = {How would Stance Detection Techniques Evolve after the Launch of ChatGPT?},
  author        = {Bowen Zhang and Daijun Ding and Liwen Jing},
  year          = {2023},
  eprint        = {2212.14548},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL}
}

@inproceedings{zhu-etal-2023-towards,
  title     = {Towards Unified Spoken Language Understanding Decoding via Label-aware Compact Linguistics Representations},
  author    = {Zhu, Zhihong  and
               Cheng, Xuxin  and
               Huang, Zhiqi  and
               Chen, Dongsheng  and
               Zou, Yuexian},
  editor    = {Rogers, Anna  and
               Boyd-Graber, Jordan  and
               Okazaki, Naoaki},
  booktitle = {Findings of the Association for Computational Linguistics: ACL 2023},
  month     = jul,
  year      = {2023},
  address   = {Toronto, Canada},
  publisher = {Association for Computational Linguistics},
  url       = {https://aclanthology.org/2023.findings-acl.793},
  doi       = {10.18653/v1/2023.findings-acl.793},
  pages     = {12523--12531},
  abstract  = {Joint intent detection and slot filling models have shown promising success in recent years due to the high correlations between the two tasks. However, previous works independently decode the two tasks, which could result in misaligned predictions for both tasks. To address this shortcoming, we propose a novel method named Label-aware Compact Linguistics Representation (LCLR), which leverages label embeddings to jointly guide the decoding process. Concretely, LCLR projects both task-specific hidden states into a joint label latent space, where both task-specific hidden states could be concisely represented as linear combinations of label embeddings. Such feature decomposition of task-specific hidden states increases the representing power for the linguistics of utterance. Extensive experiments on two single- and multi-intent SLU benchmarks prove that LCLR can learn more discriminative label information than previous separate decoders, and consistently outperform previous state-of-the-art methods across all metrics. More encouragingly, LCLR can be applied to boost the performance of existing approaches, making it easy to be incorporated into any existing SLU models.}
}

@inproceedings{hou-etal-2022-inverse,
  title     = {Inverse is Better! Fast and Accurate Prompt for Few-shot Slot Tagging},
  author    = {Hou, Yutai  and
               Chen, Cheng  and
               Luo, Xianzhen  and
               Li, Bohan  and
               Che, Wanxiang},
  editor    = {Muresan, Smaranda  and
               Nakov, Preslav  and
               Villavicencio, Aline},
  booktitle = {Findings of the Association for Computational Linguistics: ACL 2022},
  month     = may,
  year      = {2022},
  address   = {Dublin, Ireland},
  publisher = {Association for Computational Linguistics},
  url       = {https://aclanthology.org/2022.findings-acl.53},
  doi       = {10.18653/v1/2022.findings-acl.53},
  pages     = {637--647},
  abstract  = {Prompting methods recently achieve impressive success in few-shot learning. These methods modify input samples with prompt sentence pieces, and decode label tokens to map samples to corresponding labels. However, such a paradigm is very inefficient for the task of slot tagging. Since slot tagging samples are multiple consecutive words in a sentence, the prompting methods have to enumerate all n-grams token spans to find all the possible slots, which greatly slows down the prediction. To tackle this, we introduce an inverse paradigm for prompting. Different from the classic prompts mapping tokens to labels, we reversely predict slot values given slot types. Such inverse prompting only requires a one-turn prediction for each slot type and greatly speeds up the prediction. Besides, we propose a novel Iterative Prediction Strategy, from which the model learns to refine predictions by considering the relations between different slot types. We find, somewhat surprisingly, the proposed method not only predicts faster but also significantly improves the effect (improve over 6.1 F1-scores on 10-shot setting) and achieves new state-of-the-art performance.}
}

@inproceedings{cheng-etal-2023-mrrl,
  title     = {{MRRL}: Modifying the Reference via Reinforcement Learning for Non-Autoregressive Joint Multiple Intent Detection and Slot Filling},
  author    = {Cheng, Xuxin  and
               Zhu, Zhihong  and
               Cao, Bowen  and
               Ye, Qichen  and
               Zou, Yuexian},
  editor    = {Bouamor, Houda  and
               Pino, Juan  and
               Bali, Kalika},
  booktitle = {Findings of the Association for Computational Linguistics: EMNLP 2023},
  month     = dec,
  year      = {2023},
  address   = {Singapore},
  publisher = {Association for Computational Linguistics},
  url       = {https://aclanthology.org/2023.findings-emnlp.704},
  doi       = {10.18653/v1/2023.findings-emnlp.704},
  pages     = {10495--10505},
  abstract  = {With the rise of non-autoregressive approach, some non-autoregressive models for joint multiple intent detection and slot filling have obtained the promising inference speed. However, most existing SLU models (1) suffer from the multi-modality problem that leads to reference intents and slots may not be suitable for training; (2) lack of alignment between the correct predictions of the two tasks, which extremely limits the overall accuracy. Therefore, in this paper, we propose $\textbf{M}$odifying the $\textbf{R}$eference via $\textbf{R}$einforcement $\textbf{L}$earning (MRRL), a novel method for multiple intent detection and slot filling, which introduces a modifier and employs reinforcement learning. Specifically, we try to provide the better training target for the non-autoregressive SLU model via modifying the reference based on the output of the non-autoregressive SLU model, and propose a suitability reward to ensure that the output of the modifier module could fit well with the output of the non-autoregressive SLU model and does not deviate too far from the reference. In addition, we also propose a compromise reward to realize a flexible trade-off between the two subtasks. Experiments on two multi-intent datasets and non-autoregressive baselines demonstrate that our MRRL could consistently improve the performance of baselines. More encouragingly, our best variant achieves new state-of-the-art results, outperforming the previous best approach by 3.6 overall accuracy on MixATIS dataset.}
}

@inproceedings{ijcai2021p622,
  title     = {A Survey on Spoken Language Understanding: Recent Advances and New Frontiers},
  author    = {Qin, Libo and Xie, Tianbao and Che, Wanxiang and Liu, Ting},
  booktitle = {Proceedings of the Thirtieth International Joint Conference on
               Artificial Intelligence, {IJCAI-21}},
  publisher = {International Joint Conferences on Artificial Intelligence Organization},
  editor    = {Zhi-Hua Zhou},
  pages     = {4577--4584},
  year      = {2021},
  month     = {8},
  note      = {Survey Track},
  doi       = {10.24963/ijcai.2021/622},
  url       = {https://doi.org/10.24963/ijcai.2021/622}
}

@inproceedings{xing-tsang-2022-co,
  title     = {Co-guiding Net: Achieving Mutual Guidances between Multiple Intent Detection and Slot Filling via Heterogeneous Semantics-Label Graphs},
  author    = {Xing, Bowen  and
               Tsang, Ivor},
  editor    = {Goldberg, Yoav  and
               Kozareva, Zornitsa  and
               Zhang, Yue},
  booktitle = {Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing},
  month     = dec,
  year      = {2022},
  address   = {Abu Dhabi, United Arab Emirates},
  publisher = {Association for Computational Linguistics},
  url       = {https://aclanthology.org/2022.emnlp-main.12},
  doi       = {10.18653/v1/2022.emnlp-main.12},
  pages     = {159--169},
  abstract  = {Recent graph-based models for joint multiple intent detection and slot filling have obtained promising results through modeling the guidance from the prediction of intents to the decoding of slot filling.However, existing methods (1) only model the \textit{unidirectional guidance} from intent to slot; (2) adopt \textit{homogeneous graphs} to model the interactions between the slot semantics nodes and intent label nodes, which limit the performance.In this paper, we propose a novel model termed Co-guiding Net, which implements a two-stage framework achieving the \textit{mutual guidances} between the two tasks.In the first stage, the initial estimated labels of both tasks are produced, and then they are leveraged in the second stage to model the mutual guidances.Specifically, we propose two \textit{heterogeneous graph attention networks} working on the proposed two \textit{heterogeneous semantics-label graphs}, which effectively represent the relations among the semantics nodes and label nodes.Experiment results show that our model outperforms existing models by a large margin, obtaining a relative improvement of 19.3{\%} over the previous best model on MixATIS dataset in overall accuracy.}
}

@inproceedings{bastianelli-etal-2020-slurp,
  title     = {{SLURP}: A Spoken Language Understanding Resource Package},
  author    = {Bastianelli, Emanuele  and
               Vanzo, Andrea  and
               Swietojanski, Pawel  and
               Rieser, Verena},
  editor    = {Webber, Bonnie  and
               Cohn, Trevor  and
               He, Yulan  and
               Liu, Yang},
  booktitle = {Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)},
  month     = nov,
  year      = {2020},
  address   = {Online},
  publisher = {Association for Computational Linguistics},
  url       = {https://aclanthology.org/2020.emnlp-main.588},
  doi       = {10.18653/v1/2020.emnlp-main.588},
  pages     = {7252--7262},
  abstract  = {Spoken Language Understanding infers semantic meaning directly from audio data, and thus promises to reduce error propagation and misunderstandings in end-user applications. However, publicly available SLU resources are limited. In this paper, we release SLURP, a new SLU package containing the following: (1) A new challenging dataset in English spanning 18 domains, which is substantially bigger and linguistically more diverse than existing datasets; (2) Competitive baselines based on state-of-the-art NLU and ASR systems; (3) A new transparent metric for entity labelling which enables a detailed error analysis for identifying potential areas of improvement. SLURP is available at \url{https://github.com/pswietojanski/slurp}.}
}

@inproceedings{cheng23b_interspeech,
  author    = {Xuxin Cheng and Wanshi Xu and Ziyu Yao and Zhihong Zhu and Yaowei Li and Hongxiang Li and Yuexian Zou},
  title     = {{FC-MTLF: A Fine- and Coarse-grained Multi-Task Learning Framework for Cross-Lingual Spoken Language Understanding}},
  year      = 2023,
  booktitle = {Proc. INTERSPEECH 2023},
  pages     = {690--694},
  doi       = {10.21437/Interspeech.2023-46},
  issn      = {2308-457X}
}

@misc{guo2023close,
  title         = {How Close is ChatGPT to Human Experts? Comparison Corpus, Evaluation, and Detection},
  author        = {Biyang Guo and Xin Zhang and Ziyuan Wang and Minqi Jiang and Jinran Nie and Yuxuan Ding and Jianwei Yue and Yupeng Wu},
  year          = {2023},
  eprint        = {2301.07597},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL}
}

@inproceedings{qin-etal-2022-gl,
  title     = {{GL}-{CL}e{F}: A Global{--}Local Contrastive Learning Framework for Cross-lingual Spoken Language Understanding},
  author    = {Qin, Libo  and
               Chen, Qiguang  and
               Xie, Tianbao  and
               Li, Qixin  and
               Lou, Jian-Guang  and
               Che, Wanxiang  and
               Kan, Min-Yen},
  editor    = {Muresan, Smaranda  and
               Nakov, Preslav  and
               Villavicencio, Aline},
  booktitle = {Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
  month     = may,
  year      = {2022},
  address   = {Dublin, Ireland},
  publisher = {Association for Computational Linguistics},
  url       = {https://aclanthology.org/2022.acl-long.191},
  doi       = {10.18653/v1/2022.acl-long.191},
  pages     = {2677--2686},
  abstract  = {Due to high data demands of current methods, attention to zero-shot cross-lingual spoken language understanding (SLU) has grown, as such approaches greatly reduce human annotation effort. However, existing models solely rely on shared parameters, which can only perform implicit alignment across languages. We present Global-Local Contrastive Learning Framework (GL-CLeF) to address this shortcoming. Specifically, we employ contrastive learning, leveraging bilingual dictionaries to construct multilingual views of the same utterance, then encourage their representations to be more similar than negative example pairs, which achieves to explicitly align representations of similar sentences across languages. In addition, a key step in GL-CLeF is a proposed Local and Global component, which achieves a fine-grained cross-lingual transfer (i.e., sentence-level Local intent transfer, token-level Local slot transfer, and semantic-level Global transfer across intent and slot). Experiments on MultiATIS++ show that GL-CLeF achieves the best performance and successfully pulls representations of similar sentences across languages closer.}
}

@inproceedings{ijcai2022p565,
  title     = {Towards Joint Intent Detection and Slot Filling via Higher-order Attention},
  author    = {Chen, Dongsheng and Huang, Zhiqi and Wu, Xian and Ge, Shen and Zou, Yuexian},
  booktitle = {Proceedings of the Thirty-First International Joint Conference on
               Artificial Intelligence, {IJCAI-22}},
  publisher = {International Joint Conferences on Artificial Intelligence Organization},
  editor    = {Lud De Raedt},
  pages     = {4072--4078},
  year      = {2022},
  month     = {7},
  note      = {Main Track},
  doi       = {10.24963/ijcai.2022/565},
  url       = {https://doi.org/10.24963/ijcai.2022/565}
}

@inproceedings{cheng-etal-2023-ml,
  title     = {{ML}-{LMCL}: Mutual Learning and Large-Margin Contrastive Learning for Improving {ASR} Robustness in Spoken Language Understanding},
  author    = {Cheng, Xuxin  and
               Cao, Bowen  and
               Ye, Qichen  and
               Zhu, Zhihong  and
               Li, Hongxiang  and
               Zou, Yuexian},
  editor    = {Rogers, Anna  and
               Boyd-Graber, Jordan  and
               Okazaki, Naoaki},
  booktitle = {Findings of the Association for Computational Linguistics: ACL 2023},
  month     = jul,
  year      = {2023},
  address   = {Toronto, Canada},
  publisher = {Association for Computational Linguistics},
  url       = {https://aclanthology.org/2023.findings-acl.406},
  doi       = {10.18653/v1/2023.findings-acl.406},
  pages     = {6492--6505},
  abstract  = {Spoken language understanding (SLU) is a fundamental task in the task-oriented dialogue systems. However, the inevitable errors from automatic speech recognition (ASR) usually impair the understanding performance and lead to error propagation. Although there are some attempts to address this problem through contrastive learning, they (1) treat clean manual transcripts and ASR transcripts equally without discrimination in fine-tuning; (2) neglect the fact that the semantically similar pairs are still pushed away when applying contrastive learning; (3) suffer from the problem of Kullback{--}Leibler (KL) vanishing. In this paper, we propose Mutual Learning and Large-Margin Contrastive Learning (ML-LMCL), a novel framework for improving ASR robustness in SLU. Specifically, in fine-tuning, we apply mutual learning and train two SLU models on the manual transcripts and the ASR transcripts, respectively, aiming to iteratively share knowledge between these two models. We also introduce a distance polarization regularizer to avoid pushing away the intra-cluster pairs as much as possible. Moreover, we use a cyclical annealing schedule to mitigate KL vanishing issue. Experiments on three datasets show that ML-LMCL outperforms existing models and achieves new state-of-the-art performance.}
}

@inproceedings{zhu-etal-2023-enhancing,
  title     = {Enhancing Code-Switching for Cross-lingual {SLU}: A Unified View of Semantic and Grammatical Coherence},
  author    = {Zhu, Zhihong  and
               Cheng, Xuxin  and
               Huang, Zhiqi  and
               Chen, Dongsheng  and
               Zou, Yuexian},
  editor    = {Bouamor, Houda  and
               Pino, Juan  and
               Bali, Kalika},
  booktitle = {Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing},
  month     = dec,
  year      = {2023},
  address   = {Singapore},
  publisher = {Association for Computational Linguistics},
  url       = {https://aclanthology.org/2023.emnlp-main.486},
  doi       = {10.18653/v1/2023.emnlp-main.486},
  pages     = {7849--7856},
  abstract  = {Despite the success of spoken language understanding (SLU) in high-resource languages, achieving similar performance in low-resource settings, such as zero-shot scenarios, remains challenging due to limited labeled training data. To improve zero-shot cross-lingual SLU, recent studies have explored code-switched sentences containing tokens from multiple languages. However, vanilla code-switched sentences often lack semantic and grammatical coherence. We ascribe this lack to two issues: (1) randomly replacing code-switched tokens with equal probability and (2) disregarding token-level dependency within each language. To tackle these issues, in this paper, we propose a novel method termed SoGo, for zero-shot cross-lingual SLU. First, we use a saliency-based substitution approach to extract keywords as substitution options. Then, we introduce a novel token-level alignment strategy that considers the similarity between the context and the code-switched tokens, ensuring grammatical coherence in code-switched sentences. Extensive experiments and analyses demonstrate the superior performance of SoGo across nine languages on MultiATIS++.}
}

@inproceedings{chang22c_interspeech,
  author    = {Ya-Hsin Chang and Yun-Nung Chen},
  title     = {{Contrastive Learning for Improving ASR Robustness in Spoken Language Understanding}},
  year      = 2022,
  booktitle = {Proc. Interspeech 2022},
  pages     = {3458--3462},
  doi       = {10.21437/Interspeech.2022-781},
  issn      = {2308-457X}
}

@misc{susnjak2022chatgpt,
  title         = {ChatGPT: The End of Online Exam Integrity?},
  author        = {Teo Susnjak},
  year          = {2022},
  eprint        = {2212.09292},
  archiveprefix = {arXiv},
  primaryclass  = {cs.AI}
}

@inproceedings{Fu2023,
  author    = {Fu, Yao and Peng, Hao and Ou, Litu and Sabharwal, Ashish and Khot, Tushar},
  booktitle = {Proceedings of the 40 th International Conference on Machine Learning},
  title     = {Specializing {Smaller} {Language} {Models} towards {Multi}-{Step} {Reasoning}},
  year      = {2023},
  month     = jul,
  pages     = {10421--10430},
  publisher = {PMLR},
  abstract  = {The surprising ability of Large Language Models (LLMs) to perform well on complex reasoning with only few-shot chain-of-thought prompts is believed to emerge only in very large-scale models. We show that such abilities can, in fact, be distilled down from GPT-3.5 (≥ 175B) to T5 variants (≤ 11B). We propose model specialization, to specialize the model’s ability towards a target task. The hypothesis is that large models (commonly viewed as larger than 100B) have strong modeling power such that they can perform a large spectrum of tasks. Small models (commonly viewed as smaller than 10B) have limited model capacity, but if we specialize their capacity towards a target task, the model can achieve decent performance improvements. We use multi-step math reasoning as our testbed because it is a very typical emergent ability. We show two important aspects of model abilities: (1) balancing language model’s performance on multiple tasks is a delicate matter, as improvements on one task may compromise other tasks; (2) yet by intentionally paying the price of decreased generic ability, we can clearly improve across different model scales smaller than 10B towards a specialized multi-step math reasoning ability. We further give comprehensive discussions about important design choices for better generalization, including the data format mixture and the start model checkpoint. We hope our practice and discoveries can serve as an important attempt towards specialized smaller models in the new research paradigm set by LLMs.},
  file      = {:fu_specializing_2023 - Specializing Smaller Language Models Towards Multi Step Reasoning.pdf:PDF},
  groups    = {ICML 2023, Reasoning},
  issn      = {2640-3498},
  language  = {en},
  url       = {https://proceedings.mlr.press/v202/fu23d.html},
  urldate   = {2023-10-22}
}

'
@inproceedings{Phang2023,
  author     = {Phang, Jason and Mao, Yi and He, Pengcheng and Chen, Weizhu},
  booktitle  = {Proceedings of the 40 th International Conference on Machine Learning},
  title      = {{HyperTuning}: {Toward} {Adapting} {Large} {Language} {Models} without {Back}-propagation},
  year       = {2023},
  month      = jul,
  pages      = {27854--27875},
  publisher  = {PMLR},
  abstract   = {Fine-tuning large language models for different tasks can be costly and inefficient, and even methods that reduce the number of tuned parameters still require full gradient-based optimization. We propose HyperTuning, a novel approach to model adaptation that uses a hypermodel to generate task-specific parameters for a fixed downstream model. We demonstrate a simple setup for hypertuning with HyperT5, a T5-based hypermodel that produces soft prefixes or LoRA parameters for a frozen T5 model from few-shot examples. We train HyperT5 in two stages: first, hyperpretraining with a modified conditional language modeling objective that trains a hypermodel to generate parameters; second, multi-task fine-tuning (MTF) on a large number of diverse language tasks. We evaluate HyperT5 on P3, MetaICL and Super-NaturalInstructions datasets, and show that it can effectively generate parameters for unseen tasks. Moreover, we show that using hypermodel-generated parameters as initializations for further parameter-efficient fine-tuning improves performance. HyperTuning can thus be a flexible and efficient way to leverage large language models for diverse downstream applications.},
  comment    = {Lien du poster :},
  file       = {:phang_hypertuning__2023 - HyperTuning_ toward Adapting Large Language Models without Back Propagation.pdf:PDF},
  groups     = {ICML 2023, PEFT},
  issn       = {2640-3498},
  language   = {en},
  shorttitle = {{HyperTuning}},
  url        = {https://proceedings.mlr.press/v202/phang23a.html},
  urldate    = {2023-10-22}
}

'
@inproceedings{Meng2023,
  author    = {Meng, Yu and Michalski, Martin and Huang, Jiaxin and Zhang, Yu and Abdelzaher, Tarek and Han, Jiawei},
  booktitle = {Proceedings of the 40 th International Conference on Machine Learning},
  title     = {Tuning {Language} {Models} as {Training} {Data} {Generators} for {Augmentation}-{Enhanced} {Few}-{Shot} {Learning}},
  year      = {2023},
  month     = jul,
  pages     = {24457--24477},
  publisher = {PMLR},
  abstract  = {Recent studies have revealed the intriguing few-shot learning ability of pretrained language models (PLMs): They can quickly adapt to a new task when fine-tuned on a small amount of labeled data formulated as prompts, without requiring abundant task-specific annotations. Despite their promising performance, most existing few-shot approaches that only learn from the small training set still underperform fully supervised training by nontrivial margins. In this work, we study few-shot learning with PLMs from a different perspective: We first tune an autoregressive PLM on the few-shot samples and then use it as a generator to synthesize a large amount of novel training samples which augment the original training set. To encourage the generator to produce label-discriminative samples, we train it via weighted maximum likelihood where the weight of each token is automatically adjusted based on a discriminative meta-learning objective. A classification PLM can then be fine-tuned on both the few-shot and the synthetic samples with regularization for better generalization and stability. Our approach FewGen achieves an overall better result across seven classification tasks of the GLUE benchmark than existing few-shot learning methods, improving no-augmentation methods by 5+ average points, and outperforming augmentation methods by 3+ average points.},
  comment   = {Lien du poster : 
               https://icml.cc/media/PosterPDFs/ICML%202023/24783.png?t=1687728273.0565941},
  file      = {:Meng2023 - Tuning Language Models As Training Data Generators for Augmentation Enhanced Few Shot Learning.pdf:PDF},
  groups    = {ICML 2023, Data Augmentation},
  issn      = {2640-3498},
  language  = {en},
  url       = {https://proceedings.mlr.press/v202/meng23b.html},
  urldate   = {2023-10-22}
}

'
@inproceedings{Jang2023,
  author    = {Jang, Joel and Kim, Seungone and Ye, Seonghyeon and Kim, Doyoung and Logeswaran, Lajanugen and Lee, Moontae and Lee, Kyungjae and Seo, Minjoon},
  booktitle = {Proceedings of the 40 th International Conference on Machine Learning},
  title     = {Exploring the {Benefits} of {Training} {Expert} {Language} {Models} over {Instruction} {Tuning}},
  year      = {2023},
  month     = jul,
  pages     = {14702--14729},
  publisher = {PMLR},
  abstract  = {Recently, Language Models (LMs) instruction-tuned on multiple tasks, also known as multitask-prompted fine-tuning (MT), have shown capabilities to generalize to unseen tasks. Previous work has shown that scaling the number of finetuning datasets and instructions is the key component in making stronger MT LMs. In this work, we report surprising findings that show an expert LM trained on just a single task can outperform an MT LM trained with 300+ different tasks on 11 different unseen datasets and on 13 datasets of the BIG-bench benchmark by an average of 3.20\% and 1.29\%, respectively. This finding casts doubt on the previously held belief that simply scaling the number of tasks makes stronger MT LMs. Leveraging this finding, we further show that this distributed approach of training multiple expert LMs instead of a single MT LM for zero-shot inference possesses many benefits including (1) avoiding negative task transfer that often occurs during instruction tuning, (2) being able to continually learn new tasks without having to re-train on previous tasks to avoid catastrophic forgetting, and (3) showing compositional capabilities when merging individual experts together.},
  file      = {:jang_exploring_2023 - Exploring the Benefits of Training Expert Language Models Over Instruction Tuning.pdf:PDF},
  groups    = {ICML 2023},
  issn      = {2640-3498},
  language  = {en},
  url       = {https://proceedings.mlr.press/v202/jang23a.html},
  urldate   = {2023-10-22}
}

'
@inproceedings{Shao2023,
  author     = {Shao, Zhihong and Gong, Yeyun and Shen, Yelong and Huang, Minlie and Duan, Nan and Chen, Weizhu},
  booktitle  = {Proceedings of the 40 th International Conference on Machine Learning},
  title      = {Synthetic {Prompting}: {Generating} {Chain}-of-{Thought} {Demonstrations} for {Large} {Language} {Models}},
  year       = {2023},
  month      = jul,
  pages      = {30706--30775},
  publisher  = {PMLR},
  abstract   = {Large language models can perform various reasoning tasks by using chain-of-thought prompting, which guides them to find answers through step-by-step demonstrations. However, the quality of the prompts depends on the demonstrations given to the models, and creating many of them by hand is costly. We introduce Synthetic prompting, a method that leverages a few handcrafted examples to prompt the model to generate more examples by itself, and selects effective demonstrations to elicit better reasoning. Our method alternates between a backward and forward process to generate new examples. The backward process generates a question that match a sampled reasoning chain, so that the question is solvable and clear. The forward process produces a more detailed reasoning chain for the question, improving the quality of the example. We evaluate our method on numerical, symbolic, and algorithmic reasoning tasks, and show that it outperforms existing prompting techniques.},
  file       = {:shao_synthetic_2023 - Synthetic Prompting_ Generating Chain of Thought Demonstrations for Large Language Models.pdf:PDF},
  groups     = {ICML 2023, Prompting},
  issn       = {2640-3498},
  language   = {en},
  shorttitle = {Synthetic {Prompting}},
  url        = {https://proceedings.mlr.press/v202/shao23a.html},
  urldate    = {2023-10-22}
}

'
@inproceedings{Hou2023,
  author     = {Hou, Bairu and O’Connor, Joe and Andreas, Jacob and Chang, Shiyu and Zhang, Yang},
  booktitle  = {Proceedings of the 40 th International Conference on Machine Learning},
  title      = {{PromptBoosting}: {Black}-{Box} {Text} {Classification} with {Ten} {Forward} {Passes}},
  year       = {2023},
  month      = jul,
  pages      = {13309--13324},
  publisher  = {PMLR},
  abstract   = {We describe PromptBoosting, a query-efficient procedure for building a text classifier from a neural language model (LM) without access to the LM’s parameters, gradients, or hidden representations. This form of "black-box" classifier training has become increasingly important as the cost of training and inference in large-scale LMs has grown. But existing black-box LM classifier learning approaches are themselves computationally inefficient, typically specializing LMs to the target task by searching in a large space of (discrete or continuous) prompts using zeroth-order optimization methods. Instead of directly optimizing in prompt space, PromptBoosting obtains a small pool of prompts via a gradient-free approach and then constructs a large pool of weak learners by pairing these prompts with different elements of the LM’s output distribution. These weak learners are then ensembled using the AdaBoost algorithm. The entire learning process requires only a small number of forward passes and no backward pass. Experiments show that PromptBoosting achieves state-of-the-art performance in multiple black-box few-shot classification tasks, and matches or outperforms full fine-tuning in both few-shot and standard learning paradigms, while training 10x faster than existing black-box methods.},
  file       = {Full Text PDF:https\://proceedings.mlr.press/v202/hou23b/hou23b.pdf:application/pdf},
  groups     = {ICML 2023, Prompting},
  issn       = {2640-3498},
  language   = {en},
  shorttitle = {{PromptBoosting}},
  url        = {https://proceedings.mlr.press/v202/hou23b.html},
  urldate    = {2023-10-23}
}

'
@techreport{Suzgun2022,
  author   = {Suzgun, Mirac and Scales, Nathan and Schärli, Nathanael and Gehrmann, Sebastian and Tay, Yi and Chung, Hyung Won and Chowdhery, Aakanksha and Le, Quoc V. and Chi, Ed H. and Zhou, Denny and Wei, Jason},
  title    = {Challenging {BIG}-{Bench} {Tasks} and {Whether} {Chain}-of-{Thought} {Can} {Solve} {Them}},
  year     = {2022},
  month    = oct,
  note     = {arXiv:2210.09261 [cs] type: article},
  abstract = {BIG-Bench (Srivastava et al., 2022) is a diverse evaluation suite that focuses on tasks believed to be beyond the capabilities of current language models. Language models have already made good progress on this benchmark, with the best model in the BIG-Bench paper outperforming average reported human-rater results on 65\% of the BIG-Bench tasks via few-shot prompting. But on what tasks do language models fall short of average human-rater performance, and are those tasks actually unsolvable by current language models? In this work, we focus on a suite of 23 challenging BIG-Bench tasks which we call BIG-Bench Hard (BBH). These are the task for which prior language model evaluations did not outperform the average human-rater. We find that applying chain-of-thought (CoT) prompting to BBH tasks enables PaLM to surpass the average human-rater performance on 10 of the 23 tasks, and Codex (code-davinci-002) to surpass the average human-rater performance on 17 of the 23 tasks. Since many tasks in BBH require multi-step reasoning, few-shot prompting without CoT, as done in the BIG-Bench evaluations (Srivastava et al., 2022), substantially underestimates the best performance and capabilities of language models, which is better captured via CoT prompting. As further analysis, we explore the interaction between CoT and model scale on BBH, finding that CoT enables emergent task performance on several BBH tasks with otherwise flat scaling curves.},
  annote   = {Comment: GitHub repository: https://github.com/suzgunmirac/BIG-Bench-Hard},
  doi      = {10.48550/arXiv.2210.09261},
  file     = {:Suzgun2022 - Challenging BIG Bench Tasks and Whether Chain of Thought Can Solve Them.pdf:PDF},
  groups   = {Datasets 2023},
  keywords = {Computer Science - Computation and Language, Computer Science - Artificial Intelligence},
  school   = {arXiv},
  url      = {http://arxiv.org/abs/2210.09261},
  urldate  = {2023-10-23}
}

'
@techreport{Srivastava2023,
  author     = {Srivastava, Aarohi and Rastogi, Abhinav and Rao, Abhishek and Shoeb, Abu Awal Md and Abid, Abubakar and Fisch, Adam and Brown, Adam R. and Santoro, Adam and Gupta, Aditya and Garriga-Alonso, Adrià and Kluska, Agnieszka and Lewkowycz, Aitor and Agarwal, Akshat and Power, Alethea and Ray, Alex and Warstadt, Alex and Kocurek, Alexander W. and Safaya, Ali and Tazarv, Ali and Xiang, Alice and Parrish, Alicia and Nie, Allen and Hussain, Aman and Askell, Amanda and Dsouza, Amanda and Slone, Ambrose and Rahane, Ameet and Iyer, Anantharaman S. and Andreassen, Anders and Madotto, Andrea and Santilli, Andrea and Stuhlmüller, Andreas and Dai, Andrew and La, Andrew and Lampinen, Andrew and Zou, Andy and Jiang, Angela and Chen, Angelica and Vuong, Anh and Gupta, Animesh and Gottardi, Anna and Norelli, Antonio and Venkatesh, Anu and Gholamidavoodi, Arash and Tabassum, Arfa and Menezes, Arul and Kirubarajan, Arun and Mullokandov, Asher and Sabharwal, Ashish and Herrick, Austin and Efrat, Avia and Erdem, Aykut and Karakaş, Ayla and Roberts, B. Ryan and Loe, Bao Sheng and Zoph, Barret and Bojanowski, Bartłomiej and Özyurt, Batuhan and Hedayatnia, Behnam and Neyshabur, Behnam and Inden, Benjamin and Stein, Benno and Ekmekci, Berk and Lin, Bill Yuchen and Howald, Blake and Orinion, Bryan and Diao, Cameron and Dour, Cameron and Stinson, Catherine and Argueta, Cedrick and Ramírez, César Ferri and Singh, Chandan and Rathkopf, Charles and Meng, Chenlin and Baral, Chitta and Wu, Chiyu and Callison-Burch, Chris and Waites, Chris and Voigt, Christian and Manning, Christopher D. and Potts, Christopher and Ramirez, Cindy and Rivera, Clara E. and Siro, Clemencia and Raffel, Colin and Ashcraft, Courtney and Garbacea, Cristina and Sileo, Damien and Garrette, Dan and Hendrycks, Dan and Kilman, Dan and Roth, Dan and Freeman, Daniel and Khashabi, Daniel and Levy, Daniel and González, Daniel Moseguí and Perszyk, Danielle and Hernandez, Danny and Chen, Danqi and Ippolito, Daphne and Gilboa, Dar and Dohan, David and Drakard, David and Jurgens, David and Datta, Debajyoti and Ganguli, Deep and Emelin, Denis and Kleyko, Denis and Yuret, Deniz and Chen, Derek and Tam, Derek and Hupkes, Dieuwke and Misra, Diganta and Buzan, Dilyar and Mollo, Dimitri Coelho and Yang, Diyi and Lee, Dong-Ho and Schrader, Dylan and Shutova, Ekaterina and Cubuk, Ekin Dogus and Segal, Elad and Hagerman, Eleanor and Barnes, Elizabeth and Donoway, Elizabeth and Pavlick, Ellie and Rodola, Emanuele and Lam, Emma and Chu, Eric and Tang, Eric and Erdem, Erkut and Chang, Ernie and Chi, Ethan A. and Dyer, Ethan and Jerzak, Ethan and Kim, Ethan and Manyasi, Eunice Engefu and Zheltonozhskii, Evgenii and Xia, Fanyue and Siar, Fatemeh and Martínez-Plumed, Fernando and Happé, Francesca and Chollet, Francois and Rong, Frieda and Mishra, Gaurav and Winata, Genta Indra and de Melo, Gerard and Kruszewski, Germán and Parascandolo, Giambattista and Mariani, Giorgio and Wang, Gloria and Jaimovitch-López, Gonzalo and Betz, Gregor and Gur-Ari, Guy and Galijasevic, Hana and Kim, Hannah and Rashkin, Hannah and Hajishirzi, Hannaneh and Mehta, Harsh and Bogar, Hayden and Shevlin, Henry and Schütze, Hinrich and Yakura, Hiromu and Zhang, Hongming and Wong, Hugh Mee and Ng, Ian and Noble, Isaac and Jumelet, Jaap and Geissinger, Jack and Kernion, Jackson and Hilton, Jacob and Lee, Jaehoon and Fisac, Jaime Fernández and Simon, James B. and Koppel, James and Zheng, James and Zou, James and Kocoń, Jan and Thompson, Jana and Wingfield, Janelle and Kaplan, Jared and Radom, Jarema and Sohl-Dickstein, Jascha and Phang, Jason and Wei, Jason and Yosinski, Jason and Novikova, Jekaterina and Bosscher, Jelle and Marsh, Jennifer and Kim, Jeremy and Taal, Jeroen and Engel, Jesse and Alabi, Jesujoba and Xu, Jiacheng and Song, Jiaming and Tang, Jillian and Waweru, Joan and Burden, John and Miller, John and Balis, John U. and Batchelder, Jonathan and Berant, Jonathan and Frohberg, Jörg and Rozen, Jos and Hernandez-Orallo, Jose and Boudeman, Joseph and Guerr, Joseph and Jones, Joseph and Tenenbaum, Joshua B. and Rule, Joshua S. and Chua, Joyce and Kanclerz, Kamil and Livescu, Karen and Krauth, Karl and Gopalakrishnan, Karthik and Ignatyeva, Katerina and Markert, Katja and Dhole, Kaustubh D. and Gimpel, Kevin and Omondi, Kevin and Mathewson, Kory and Chiafullo, Kristen and Shkaruta, Ksenia and Shridhar, Kumar and McDonell, Kyle and Richardson, Kyle and Reynolds, Laria and Gao, Leo and Zhang, Li and Dugan, Liam and Qin, Lianhui and Contreras-Ochando, Lidia and Morency, Louis-Philippe and Moschella, Luca and Lam, Lucas and Noble, Lucy and Schmidt, Ludwig and He, Luheng and Colón, Luis Oliveros and Metz, Luke and Şenel, Lütfi Kerem and Bosma, Maarten and Sap, Maarten and ter Hoeve, Maartje and Farooqi, Maheen and Faruqui, Manaal and Mazeika, Mantas and Baturan, Marco and Marelli, Marco and Maru, Marco and Quintana, Maria Jose Ramírez and Tolkiehn, Marie and Giulianelli, Mario and Lewis, Martha and Potthast, Martin and Leavitt, Matthew L. and Hagen, Matthias and Schubert, Mátyás and Baitemirova, Medina Orduna and Arnaud, Melody and McElrath, Melvin and Yee, Michael A. and Cohen, Michael and Gu, Michael and Ivanitskiy, Michael and Starritt, Michael and Strube, Michael and Swędrowski, Michał and Bevilacqua, Michele and Yasunaga, Michihiro and Kale, Mihir and Cain, Mike and Xu, Mimee and Suzgun, Mirac and Walker, Mitch and Tiwari, Mo and Bansal, Mohit and Aminnaseri, Moin and Geva, Mor and Gheini, Mozhdeh and T, Mukund Varma and Peng, Nanyun and Chi, Nathan A. and Lee, Nayeon and Krakover, Neta Gur-Ari and Cameron, Nicholas and Roberts, Nicholas and Doiron, Nick and Martinez, Nicole and Nangia, Nikita and Deckers, Niklas and Muennighoff, Niklas and Keskar, Nitish Shirish and Iyer, Niveditha S. and Constant, Noah and Fiedel, Noah and Wen, Nuan and Zhang, Oliver and Agha, Omar and Elbaghdadi, Omar and Levy, Omer and Evans, Owain and Casares, Pablo Antonio Moreno and Doshi, Parth and Fung, Pascale and Liang, Paul Pu and Vicol, Paul and Alipoormolabashi, Pegah and Liao, Peiyuan and Liang, Percy and Chang, Peter and Eckersley, Peter and Htut, Phu Mon and Hwang, Pinyu and Miłkowski, Piotr and Patil, Piyush and Pezeshkpour, Pouya and Oli, Priti and Mei, Qiaozhu and Lyu, Qing and Chen, Qinlang and Banjade, Rabin and Rudolph, Rachel Etta and Gabriel, Raefer and Habacker, Rahel and Risco, Ramon and Millière, Raphaël and Garg, Rhythm and Barnes, Richard and Saurous, Rif A. and Arakawa, Riku and Raymaekers, Robbe and Frank, Robert and Sikand, Rohan and Novak, Roman and Sitelew, Roman and LeBras, Ronan and Liu, Rosanne and Jacobs, Rowan and Zhang, Rui and Salakhutdinov, Ruslan and Chi, Ryan and Lee, Ryan and Stovall, Ryan and Teehan, Ryan and Yang, Rylan and Singh, Sahib and Mohammad, Saif M. and Anand, Sajant and Dillavou, Sam and Shleifer, Sam and Wiseman, Sam and Gruetter, Samuel and Bowman, Samuel R. and Schoenholz, Samuel S. and Han, Sanghyun and Kwatra, Sanjeev and Rous, Sarah A. and Ghazarian, Sarik and Ghosh, Sayan and Casey, Sean and Bischoff, Sebastian and Gehrmann, Sebastian and Schuster, Sebastian and Sadeghi, Sepideh and Hamdan, Shadi and Zhou, Sharon and Srivastava, Shashank and Shi, Sherry and Singh, Shikhar and Asaadi, Shima and Gu, Shixiang Shane and Pachchigar, Shubh and Toshniwal, Shubham and Upadhyay, Shyam and Shyamolima and Debnath and Shakeri, Siamak and Thormeyer, Simon and Melzi, Simone and Reddy, Siva and Makini, Sneha Priscilla and Lee, Soo-Hwan and Torene, Spencer and Hatwar, Sriharsha and Dehaene, Stanislas and Divic, Stefan and Ermon, Stefano and Biderman, Stella and Lin, Stephanie and Prasad, Stephen and Piantadosi, Steven T. and Shieber, Stuart M. and Misherghi, Summer and Kiritchenko, Svetlana and Mishra, Swaroop and Linzen, Tal and Schuster, Tal and Li, Tao and Yu, Tao and Ali, Tariq and Hashimoto, Tatsu and Wu, Te-Lin and Desbordes, Théo and Rothschild, Theodore and Phan, Thomas and Wang, Tianle and Nkinyili, Tiberius and Schick, Timo and Kornev, Timofei and Tunduny, Titus and Gerstenberg, Tobias and Chang, Trenton and Neeraj, Trishala and Khot, Tushar and Shultz, Tyler and Shaham, Uri and Misra, Vedant and Demberg, Vera and Nyamai, Victoria and Raunak, Vikas and Ramasesh, Vinay and Prabhu, Vinay Uday and Padmakumar, Vishakh and Srikumar, Vivek and Fedus, William and Saunders, William and Zhang, William and Vossen, Wout and Ren, Xiang and Tong, Xiaoyu and Zhao, Xinran and Wu, Xinyi and Shen, Xudong and Yaghoobzadeh, Yadollah and Lakretz, Yair and Song, Yangqiu and Bahri, Yasaman and Choi, Yejin and Yang, Yichi and Hao, Yiding and Chen, Yifu and Belinkov, Yonatan and Hou, Yu and Hou, Yufang and Bai, Yuntao and Seid, Zachary and Zhao, Zhuoye and Wang, Zijian and Wang, Zijie J. and Wang, Zirui and Wu, Ziyi},
  title      = {Beyond the {Imitation} {Game}: {Quantifying} and extrapolating the capabilities of language models},
  year       = {2023},
  month      = jun,
  note       = {arXiv:2206.04615 [cs, stat] type: article},
  abstract   = {Language models demonstrate both quantitative improvement and new qualitative capabilities with increasing scale. Despite their potentially transformative impact, these new capabilities are as yet poorly characterized. In order to inform future research, prepare for disruptive new model capabilities, and ameliorate socially harmful effects, it is vital that we understand the present and near-future capabilities and limitations of language models. To address this challenge, we introduce the Beyond the Imitation Game benchmark (BIG-bench). BIG-bench currently consists of 204 tasks, contributed by 450 authors across 132 institutions. Task topics are diverse, drawing problems from linguistics, childhood development, math, common-sense reasoning, biology, physics, social bias, software development, and beyond. BIG-bench focuses on tasks that are believed to be beyond the capabilities of current language models. We evaluate the behavior of OpenAI's GPT models, Google-internal dense transformer architectures, and Switch-style sparse transformers on BIG-bench, across model sizes spanning millions to hundreds of billions of parameters. In addition, a team of human expert raters performed all tasks in order to provide a strong baseline. Findings include: model performance and calibration both improve with scale, but are poor in absolute terms (and when compared with rater performance); performance is remarkably similar across model classes, though with benefits from sparsity; tasks that improve gradually and predictably commonly involve a large knowledge or memorization component, whereas tasks that exhibit "breakthrough" behavior at a critical scale often involve multiple steps or components, or brittle metrics; social bias typically increases with scale in settings with ambiguous context, but this can be improved with prompting.},
  annote     = {Comment: 27 pages, 17 figures + references and appendices, repo: https://github.com/google/BIG-bench},
  comment    = {BigBench},
  file       = {:Srivastava2023 - Beyond the Imitation Game_ Quantifying and Extrapolating the Capabilities of Language Models.pdf:PDF},
  groups     = {Datasets 2023},
  keywords   = {Computer Science - Computation and Language, Computer Science - Artificial Intelligence, Computer Science - Computers and Society, Computer Science - Machine Learning, Statistics - Machine Learning},
  school     = {arXiv},
  shorttitle = {Beyond the {Imitation} {Game}},
  url        = {http://arxiv.org/abs/2206.04615},
  urldate    = {2023-10-23}
}

'
@techreport{Rajpurkar2018,
  author     = {Rajpurkar, Pranav and Jia, Robin and Liang, Percy},
  title      = {Know {What} {You} {Don}'t {Know}: {Unanswerable} {Questions} for {SQuAD}},
  year       = {2018},
  month      = jun,
  note       = {arXiv:1806.03822 [cs] type: article},
  abstract   = {Extractive reading comprehension systems can often locate the correct answer to a question in a context document, but they also tend to make unreliable guesses on questions for which the correct answer is not stated in the context. Existing datasets either focus exclusively on answerable questions, or use automatically generated unanswerable questions that are easy to identify. To address these weaknesses, we present SQuAD 2.0, the latest version of the Stanford Question Answering Dataset (SQuAD). SQuAD 2.0 combines existing SQuAD data with over 50,000 unanswerable questions written adversarially by crowdworkers to look similar to answerable ones. To do well on SQuAD 2.0, systems must not only answer questions when possible, but also determine when no answer is supported by the paragraph and abstain from answering. SQuAD 2.0 is a challenging natural language understanding task for existing models: a strong neural system that gets 86\% F1 on SQuAD 1.1 achieves only 66\% F1 on SQuAD 2.0.},
  annote     = {Comment: ACL 2018},
  comment    = {Squad2.0},
  doi        = {10.48550/arXiv.1806.03822},
  file       = {:Rajpurkar2018 - Know What You Don't Know_ Unanswerable Questions for SQuAD.pdf:PDF},
  groups     = {Datasets 2023},
  keywords   = {Computer Science - Computation and Language},
  school     = {arXiv},
  shorttitle = {Know {What} {You} {Don}'t {Know}},
  url        = {http://arxiv.org/abs/1806.03822},
  urldate    = {2023-10-23}
}

'
@techreport{Cobbe2021,
  author   = {Cobbe, Karl and Kosaraju, Vineet and Bavarian, Mohammad and Chen, Mark and Jun, Heewoo and Kaiser, Lukasz and Plappert, Matthias and Tworek, Jerry and Hilton, Jacob and Nakano, Reiichiro and Hesse, Christopher and Schulman, John},
  title    = {Training {Verifiers} to {Solve} {Math} {Word} {Problems}},
  year     = {2021},
  month    = nov,
  note     = {arXiv:2110.14168 [cs] type: article},
  abstract = {State-of-the-art language models can match human performance on many tasks, but they still struggle to robustly perform multi-step mathematical reasoning. To diagnose the failures of current models and support research, we introduce GSM8K, a dataset of 8.5K high quality linguistically diverse grade school math word problems. We find that even the largest transformer models fail to achieve high test performance, despite the conceptual simplicity of this problem distribution. To increase performance, we propose training verifiers to judge the correctness of model completions. At test time, we generate many candidate solutions and select the one ranked highest by the verifier. We demonstrate that verification significantly improves performance on GSM8K, and we provide strong empirical evidence that verification scales more effectively with increased data than a finetuning baseline.},
  file     = {:Cobbe2021 - Training Verifiers to Solve Math Word Problems.pdf:PDF},
  groups   = {Datasets 2023},
  keywords = {Computer Science - Machine Learning, Computer Science - Computation and Language},
  school   = {arXiv},
  url      = {http://arxiv.org/abs/2110.14168},
  urldate  = {2023-10-23}
}

'
@techreport{Zellers2019,
  author     = {Zellers, Rowan and Holtzman, Ari and Bisk, Yonatan and Farhadi, Ali and Choi, Yejin},
  title      = {{HellaSwag}: {Can} a {Machine} {Really} {Finish} {Your} {Sentence}?},
  year       = {2019},
  month      = may,
  note       = {arXiv:1905.07830 [cs] type: article},
  abstract   = {Recent work by Zellers et al. (2018) introduced a new task of commonsense natural language inference: given an event description such as "A woman sits at a piano," a machine must select the most likely followup: "She sets her fingers on the keys." With the introduction of BERT, near human-level performance was reached. Does this mean that machines can perform human level commonsense inference? In this paper, we show that commonsense inference still proves difficult for even state-of-the-art models, by presenting HellaSwag, a new challenge dataset. Though its questions are trivial for humans ({\textgreater}95\% accuracy), state-of-the-art models struggle ({\textless}48\%). We achieve this via Adversarial Filtering (AF), a data collection paradigm wherein a series of discriminators iteratively select an adversarial set of machine-generated wrong answers. AF proves to be surprisingly robust. The key insight is to scale up the length and complexity of the dataset examples towards a critical 'Goldilocks' zone wherein generated text is ridiculous to humans, yet often misclassified by state-of-the-art models. Our construction of HellaSwag, and its resulting difficulty, sheds light on the inner workings of deep pretrained models. More broadly, it suggests a new path forward for NLP research, in which benchmarks co-evolve with the evolving state-of-the-art in an adversarial way, so as to present ever-harder challenges.},
  annote     = {Comment: ACL 2019. Project page at https://rowanzellers.com/hellaswag},
  file       = {:Zellers2019 - HellaSwag_ Can a Machine Really Finish Your Sentence_.pdf:PDF},
  groups     = {Datasets 2023},
  keywords   = {Computer Science - Computation and Language},
  school     = {arXiv},
  shorttitle = {{HellaSwag}},
  url        = {http://arxiv.org/abs/1905.07830},
  urldate    = {2023-10-23}
}

'
@techreport{Zhao2023,
  author   = {Zhao, Wayne Xin and Zhou, Kun and Li, Junyi and Tang, Tianyi and Wang, Xiaolei and Hou, Yupeng and Min, Yingqian and Zhang, Beichen and Zhang, Junjie and Dong, Zican and Du, Yifan and Yang, Chen and Chen, Yushuo and Chen, Zhipeng and Jiang, Jinhao and Ren, Ruiyang and Li, Yifan and Tang, Xinyu and Liu, Zikang and Liu, Peiyu and Nie, Jian-Yun and Wen, Ji-Rong},
  title    = {A {Survey} of {Large} {Language} {Models}},
  year     = {2023},
  month    = sep,
  note     = {arXiv:2303.18223 [cs] type: article},
  abstract = {Language is essentially a complex, intricate system of human expressions governed by grammatical rules. It poses a significant challenge to develop capable AI algorithms for comprehending and grasping a language. As a major approach, language modeling has been widely studied for language understanding and generation in the past two decades, evolving from statistical language models to neural language models. Recently, pre-trained language models (PLMs) have been proposed by pre-training Transformer models over large-scale corpora, showing strong capabilities in solving various NLP tasks. Since researchers have found that model scaling can lead to performance improvement, they further study the scaling effect by increasing the model size to an even larger size. Interestingly, when the parameter scale exceeds a certain level, these enlarged language models not only achieve a significant performance improvement but also show some special abilities that are not present in small-scale language models. To discriminate the difference in parameter scale, the research community has coined the term large language models (LLM) for the PLMs of significant size. Recently, the research on LLMs has been largely advanced by both academia and industry, and a remarkable progress is the launch of ChatGPT, which has attracted widespread attention from society. The technical evolution of LLMs has been making an important impact on the entire AI community, which would revolutionize the way how we develop and use AI algorithms. In this survey, we review the recent advances of LLMs by introducing the background, key findings, and mainstream techniques. In particular, we focus on four major aspects of LLMs, namely pre-training, adaptation tuning, utilization, and capacity evaluation. Besides, we also summarize the available resources for developing LLMs and discuss the remaining issues for future directions.},
  annote   = {Comment: ongoing work; 97 pages, 683 citations},
  file     = {:Zhao2023a - A Survey of Large Language Models.pdf:PDF},
  groups   = {promting, dede, ArXiv},
  keywords = {Computer Science - Computation and Language, Computer Science - Artificial Intelligence},
  school   = {arXiv},
  url      = {http://arxiv.org/abs/2303.18223},
  urldate  = {2023-10-17}
}

'
@techreport{Zhang2023,
  author     = {Zhang, Shengyu and Dong, Linfeng and Li, Xiaoya and Zhang, Sen and Sun, Xiaofei and Wang, Shuhe and Li, Jiwei and Hu, Runyi and Zhang, Tianwei and Wu, Fei and Wang, Guoyin},
  title      = {Instruction {Tuning} for {Large} {Language} {Models}: {A} {Survey}},
  year       = {2023},
  month      = oct,
  note       = {arXiv:2308.10792 [cs] type: article},
  abstract   = {This paper surveys research works in the quickly advancing field of instruction tuning (IT), a crucial technique to enhance the capabilities and controllability of large language models (LLMs). Instruction tuning refers to the process of further training LLMs on a dataset consisting of {\textbackslash}textsc\{(instruction, output)\} pairs in a supervised fashion, which bridges the gap between the next-word prediction objective of LLMs and the users' objective of having LLMs adhere to human instructions. In this work, we make a systematic review of the literature, including the general methodology of IT, the construction of IT datasets, the training of IT models, and applications to different modalities, domains and applications, along with an analysis on aspects that influence the outcome of IT (e.g., generation of instruction outputs, size of the instruction dataset, etc). We also review the potential pitfalls of IT along with criticism against it, along with efforts pointing out current deficiencies of existing strategies and suggest some avenues for fruitful research. Project page: github.com/xiaoya-li/Instruction-Tuning-Survey},
  annote     = {Comment: A Survey paper, Pre-print},
  file       = {:Zhang2023 - Instruction Tuning for Large Language Models_ a Survey.pdf:PDF},
  groups     = {ArXiv},
  keywords   = {Computer Science - Computation and Language, Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
  school     = {arXiv},
  shorttitle = {Instruction {Tuning} for {Large} {Language} {Models}},
  url        = {http://arxiv.org/abs/2308.10792},
  urldate    = {2023-10-12}
}

'
@techreport{Smith2022,
  author     = {Smith, Ryan and Fries, Jason A. and Hancock, Braden and Bach, Stephen H.},
  title      = {Language {Models} in the {Loop}: {Incorporating} {Prompting} into {Weak} {Supervision}},
  year       = {2022},
  month      = may,
  note       = {arXiv:2205.02318 [cs] type: article},
  abstract   = {We propose a new strategy for applying large pre-trained language models to novel tasks when labeled training data is limited. Rather than apply the model in a typical zero-shot or few-shot fashion, we treat the model as the basis for labeling functions in a weak supervision framework. To create a classifier, we first prompt the model to answer multiple distinct queries about an example and define how the possible responses should be mapped to votes for labels and abstentions. We then denoise these noisy label sources using the Snorkel system and train an end classifier with the resulting training data. Our experimental evaluation shows that prompting large language models within a weak supervision framework can provide significant gains in accuracy. On the WRENCH weak supervision benchmark, this approach can significantly improve over zero-shot performance, an average 19.5\% reduction in errors. We also find that this approach produces classifiers with comparable or superior accuracy to those trained from hand-engineered rules.},
  file       = {:Smith2022 - Language Models in the Loop_ Incorporating Prompting into Weak Supervision.pdf:PDF},
  groups     = {ArXiv},
  keywords   = {Computer Science - Machine Learning, Computer Science - Computation and Language},
  school     = {arXiv},
  shorttitle = {Language {Models} in the {Loop}},
  url        = {http://arxiv.org/abs/2205.02318},
  urldate    = {2023-06-05}
}

'
@inproceedings{Xu2023,
  author    = {Xu, Frank F. and Alon, Uri and Neubig, Graham},
  title     = {Why do {Nearest} {Neighbor} {Language} {Models} {Work}?},
  year      = {2023},
  month     = jul,
  pages     = {38325--38341},
  publisher = {PMLR},
  abstract  = {Language models (LMs) compute the probability of a text by sequentially computing a representation of an already-seen context and using this representation to predict the next word. Currently, most LMs calculate these representations through a neural network consuming the immediate previous context. However recently, retrieval-augmented LMs have shown to improve over standard neural LMs, by accessing information retrieved from a large datastore, in addition to their standard, parametric, next-word prediction. In this paper, we set out to understand why retrieval-augmented language models, and specifically why k-nearest neighbor language models (kNN-LMs) perform better than standard parametric LMs, even when the k-nearest neighbor component retrieves examples from the same training set that the LM was originally trained on. To this end, we perform analysis of various dimensions over which kNN-LM diverges from standard LMs, and investigate these dimensions one by one. Empirically, we identify three main reasons why kNN-LM performs better than standard LMs: using a different input representation for predicting the next tokens, approximate kNN search, and the importance of softmax temperature for the kNN distribution. Further, we incorporate some insights into the standard parametric LM, improving performance without the need for an explicit retrieval component. The code is available at https://github.com/frankxu2004/knnlm-why.},
  file      = {:xu_why_2023 - Why Do Nearest Neighbor Language Models Work_.pdf:PDF},
  groups    = {ICML 2023},
  issn      = {2640-3498},
  language  = {en},
  url       = {https://proceedings.mlr.press/v202/xu23a.html},
  urldate   = {2023-10-23}
}

'
@inproceedings{Kim2023,
  author     = {Kim, Jaehyung and Shin, Jinwoo and Kang, Dongyeop},
  title      = {Prefer to {Classify}: {Improving} {Text} {Classifiers} via {Auxiliary} {Preference} {Learning}},
  year       = {2023},
  month      = jul,
  pages      = {16807--16828},
  publisher  = {PMLR},
  abstract   = {The development of largely human-annotated benchmarks has driven the success of deep neural networks in various NLP tasks. To enhance the effectiveness of existing benchmarks, collecting new additional input-output pairs is often too costly and challenging, particularly considering their marginal impact on improving the current model accuracy. Instead, additional or complementary annotations on the existing input texts in the benchmarks can be preferable as an efficient way to pay the additional human cost. In this paper, we investigate task-specific preferences between pairs of input texts as a new alternative way for such auxiliary data annotation. From pair-wise comparisons with respect to the task, the auxiliary preference learning enables the model to learn an additional informative training signal that cannot be captured with instance-wise task labels. To this end, we propose a novel multi-task learning framework, called prefer-to-classify (P2C), which can enjoy the cooperative effect of learning both the given classification task and the auxiliary preferences. Here, we provide three different ways to collect preference signals in practice: (a) implicitly extracting from annotation records (for free, but often unavailable), (b) collecting explicitly from crowd workers (high paid), or (c) pre-trained large language models such as GPT-3 (low paid). Given existing classification NLP benchmarks, we demonstrate that the proposed auxiliary preference learning via P2C on them is effective in improving text classifiers. Our codes are publicly available.},
  file       = {:Kim2023 - Prefer to Classify_ Improving Text Classifiers Via Auxiliary Preference Learning.pdf:PDF},
  groups     = {ICML 2023, Multitask Learning},
  issn       = {2640-3498},
  language   = {en},
  shorttitle = {Prefer to {Classify}},
  url        = {https://proceedings.mlr.press/v202/kim23u.html},
  urldate    = {2023-10-23}
}

'
@inproceedings{Shi2023,
  author    = {Shi, Freda and Chen, Xinyun and Misra, Kanishka and Scales, Nathan and Dohan, David and Chi, Ed H. and Schärli, Nathanael and Zhou, Denny},
  title     = {Large {Language} {Models} {Can} {Be} {Easily} {Distracted} by {Irrelevant} {Context}},
  year      = {2023},
  month     = jul,
  pages     = {31210--31227},
  publisher = {PMLR},
  abstract  = {Large language models have achieved impressive performance on various natural language processing tasks. However, so far they have been evaluated primarily on benchmarks where all information in the input context is relevant for solving the task. In this work, we investigate the distractibility of large language models, i.e., how the model prediction can be distracted by irrelevant context. In particular, we introduce Grade-School Math with Irrelevant Context (GSM-IC), an arithmetic reasoning dataset with irrelevant information in the problem description. We use this benchmark to measure the distractibility of different prompting techniques for large language models, and find that the model is easily distracted by irrelevant information. We also identify several approaches for mitigating this deficiency, such as decoding with self-consistency and adding to the prompt an instruction that tells the language model to ignore the irrelevant information.},
  file      = {:Shi2023 - Large Language Models Can Be Easily Distracted by Irrelevant Context.pdf:PDF},
  groups    = {ICML 2023},
  issn      = {2640-3498},
  language  = {en},
  url       = {https://proceedings.mlr.press/v202/shi23a.html},
  urldate   = {2023-10-23}
}

'
@inproceedings{Bender2020,
  author     = {Bender, Emily M. and Koller, Alexander},
  booktitle  = {Proceedings of the 58th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics}},
  title      = {Climbing towards {NLU}: {On} {Meaning}, {Form}, and {Understanding} in the {Age} of {Data}},
  year       = {2020},
  address    = {Online},
  month      = jul,
  pages      = {5185--5198},
  publisher  = {Association for Computational Linguistics},
  abstract   = {The success of the large neural language models on many NLP tasks is exciting. However, we find that these successes sometimes lead to hype in which these models are being described as “understanding” language or capturing “meaning”. In this position paper, we argue that a system trained only on form has a priori no way to learn meaning. In keeping with the ACL 2020 theme of “Taking Stock of Where We've Been and Where We're Going”, we argue that a clear understanding of the distinction between form and meaning will help guide the field towards better science around natural language understanding.},
  doi        = {10.18653/v1/2020.acl-main.463},
  file       = {:Bender2020 - Climbing Towards NLU_ on Meaning, Form, and Understanding in the Age of Data.pdf:PDF},
  shorttitle = {Climbing towards {NLU}},
  url        = {https://aclanthology.org/2020.acl-main.463},
  urldate    = {2023-10-23}
}

'
@techreport{Ashok2023,
  author     = {Ashok, Dhananjay and Lipton, Zachary C.},
  title      = {{PromptNER}: {Prompting} {For} {Named} {Entity} {Recognition}},
  year       = {2023},
  month      = jun,
  note       = {arXiv:2305.15444 [cs] type: article},
  abstract   = {In a surprising turn, Large Language Models (LLMs) together with a growing arsenal of prompt-based heuristics now offer powerful off-the-shelf approaches providing few-shot solutions to myriad classic NLP problems. However, despite promising early results, these LLM-based few-shot methods remain far from the state of the art in Named Entity Recognition (NER), where prevailing methods include learning representations via end-to-end structural understanding and fine-tuning on standard labeled corpora. In this paper, we introduce PromptNER, a new state-of-the-art algorithm for few-Shot and cross-domain NER. To adapt to any new NER task PromptNER requires a set of entity definitions in addition to the standard few-shot examples. Given a sentence, PromptNER prompts an LLM to produce a list of potential entities along with corresponding explanations justifying their compatibility with the provided entity type definitions. Remarkably, PromptNER achieves state-of-the-art performance on few-shot NER, achieving a 4\% (absolute) improvement in F1 score on the ConLL dataset, a 9\% (absolute) improvement on the GENIA dataset, and a 4\% (absolute) improvement on the FewNERD dataset. PromptNER also moves the state of the art on Cross Domain NER, outperforming prior methods (including those not limited to the few-shot setting), setting a new mark on 3/5 CrossNER target domains, with an average F1 gain of 3\%, despite using less than 2\% of the available data.},
  file       = {:Ashok2023 - PromptNER_ Prompting for Named Entity Recognition.pdf:PDF},
  groups     = {ArXiv, NER},
  keywords   = {Computer Science - Computation and Language, Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
  school     = {arXiv},
  shorttitle = {{PromptNER}},
  url        = {http://arxiv.org/abs/2305.15444},
  urldate    = {2023-10-24}
}

'
@techreport{Zhang2023a,
  author     = {Zhang, Mozhi and Yan, Hang and Zhou, Yaqian and Qiu, Xipeng},
  title      = {{PromptNER}: {A} {Prompting} {Method} for {Few}-shot {Named} {Entity} {Recognition} via k {Nearest} {Neighbor} {Search}},
  year       = {2023},
  month      = may,
  note       = {arXiv:2305.12217 [cs] type: article},
  abstract   = {Few-shot Named Entity Recognition (NER) is a task aiming to identify named entities via limited annotated samples. Recently, prototypical networks have shown promising performance in few-shot NER. Most of prototypical networks will utilize the entities from the support set to construct label prototypes and use the query set to compute span-level similarities and optimize these label prototype representations. However, these methods are usually unsuitable for fine-tuning in the target domain, where only the support set is available. In this paper, we propose PromptNER: a novel prompting method for few-shot NER via k nearest neighbor search. We use prompts that contains entity category information to construct label prototypes, which enables our model to fine-tune with only the support set. Our approach achieves excellent transfer learning ability, and extensive experiments on the Few-NERD and CrossNER datasets demonstrate that our model achieves superior performance over state-of-the-art methods.},
  annote     = {Comment: work in progress},
  file       = {:Zhang2023a - PromptNER_ a Prompting Method for Few Shot Named Entity Recognition Via K Nearest Neighbor Search.pdf:PDF},
  groups     = {ArXiv, NER},
  keywords   = {Computer Science - Computation and Language},
  school     = {arXiv},
  shorttitle = {{PromptNER}},
  url        = {http://arxiv.org/abs/2305.12217},
  urldate    = {2023-10-24}
}

'
@techreport{Wang2023,
  author     = {Wang, Shuhe and Sun, Xiaofei and Li, Xiaoya and Ouyang, Rongbin and Wu, Fei and Zhang, Tianwei and Li, Jiwei and Wang, Guoyin},
  title      = {{GPT}-{NER}: {Named} {Entity} {Recognition} via {Large} {Language} {Models}},
  year       = {2023},
  month      = oct,
  note       = {arXiv:2304.10428 [cs] type: article},
  abstract   = {Despite the fact that large-scale Language Models (LLM) have achieved SOTA performances on a variety of NLP tasks, its performance on NER is still significantly below supervised baselines. This is due to the gap between the two tasks the NER and LLMs: the former is a sequence labeling task in nature while the latter is a text-generation model. In this paper, we propose GPT-NER to resolve this issue. GPT-NER bridges the gap by transforming the sequence labeling task to a generation task that can be easily adapted by LLMs e.g., the task of finding location entities in the input text "Columbus is a city" is transformed to generate the text sequence "@@Columbus\#\# is a city", where special tokens @@\#\# marks the entity to extract. To efficiently address the "hallucination" issue of LLMs, where LLMs have a strong inclination to over-confidently label NULL inputs as entities, we propose a self-verification strategy by prompting LLMs to ask itself whether the extracted entities belong to a labeled entity tag. We conduct experiments on five widely adopted NER datasets, and GPT-NER achieves comparable performances to fully supervised baselines, which is the first time as far as we are concerned. More importantly, we find that GPT-NER exhibits a greater ability in the low-resource and few-shot setups, when the amount of training data is extremely scarce, GPT-NER performs significantly better than supervised models. This demonstrates the capabilities of GPT-NER in real-world NER applications where the number of labeled examples is limited.},
  file       = {:Wang2023 - GPT NER_ Named Entity Recognition Via Large Language Models.pdf:PDF},
  groups     = {ArXiv, NER},
  keywords   = {Computer Science - Computation and Language},
  school     = {arXiv},
  shorttitle = {{GPT}-{NER}},
  url        = {http://arxiv.org/abs/2304.10428},
  urldate    = {2023-10-24}
}

'
@techreport{Gao2023,
  author     = {Gao, Luyu and Madaan, Aman and Zhou, Shuyan and Alon, Uri and Liu, Pengfei and Yang, Yiming and Callan, Jamie and Neubig, Graham},
  title      = {{PAL}: {Program}-aided {Language} {Models}},
  year       = {2023},
  month      = jan,
  note       = {arXiv:2211.10435 [cs] type: article},
  abstract   = {Large language models (LLMs) have recently demonstrated an impressive ability to perform arithmetic and symbolic reasoning tasks, when provided with a few examples at test time ("few-shot prompting"). Much of this success can be attributed to prompting methods such as "chain-of-thought'', which employ LLMs for both understanding the problem description by decomposing it into steps, as well as solving each step of the problem. While LLMs seem to be adept at this sort of step-by-step decomposition, LLMs often make logical and arithmetic mistakes in the solution part, even when the problem is decomposed correctly. In this paper, we present Program-Aided Language models (PAL): a novel approach that uses the LLM to read natural language problems and generate programs as the intermediate reasoning steps, but offloads the solution step to a runtime such as a Python interpreter. With PAL, decomposing the natural language problem into runnable steps remains the only learning task for the LLM, while solving is delegated to the interpreter. We demonstrate this synergy between a neural LLM and a symbolic interpreter across 13 mathematical, symbolic, and algorithmic reasoning tasks from BIG-Bench Hard and other benchmarks. In all these natural language reasoning tasks, generating code using an LLM and reasoning using a Python interpreter leads to more accurate results than much larger models. For example, PAL using Codex achieves state-of-the-art few-shot accuracy on the GSM8K benchmark of math word problems, surpassing PaLM-540B which uses chain-of-thought by absolute 15\% top-1. Our code and data are publicly available at http://reasonwithpal.com/ .},
  annote     = {Comment: The first three authors contributed equally. Our code and data are publicly available at http://reasonwithpal.com/},
  file       = {:Gao2023 - PAL_ Program Aided Language Models.pdf:PDF},
  groups     = {Datasets 2023, ArXiv},
  keywords   = {Computer Science - Computation and Language, Computer Science - Artificial Intelligence},
  school     = {arXiv},
  shorttitle = {{PAL}},
  url        = {http://arxiv.org/abs/2211.10435},
  urldate    = {2023-10-25}
}

'
@techreport{Patel2021,
  author   = {Patel, Arkil and Bhattamishra, Satwik and Goyal, Navin},
  title    = {Are {NLP} {Models} really able to {Solve} {Simple} {Math} {Word} {Problems}?},
  year     = {2021},
  month    = apr,
  note     = {arXiv:2103.07191 [cs] type: article},
  abstract = {The problem of designing NLP solvers for math word problems (MWP) has seen sustained research activity and steady gains in the test accuracy. Since existing solvers achieve high performance on the benchmark datasets for elementary level MWPs containing one-unknown arithmetic word problems, such problems are often considered "solved" with the bulk of research attention moving to more complex MWPs. In this paper, we restrict our attention to English MWPs taught in grades four and lower. We provide strong evidence that the existing MWP solvers rely on shallow heuristics to achieve high performance on the benchmark datasets. To this end, we show that MWP solvers that do not have access to the question asked in the MWP can still solve a large fraction of MWPs. Similarly, models that treat MWPs as bag-of-words can also achieve surprisingly high accuracy. Further, we introduce a challenge dataset, SVAMP, created by applying carefully chosen variations over examples sampled from existing datasets. The best accuracy achieved by state-of-the-art models is substantially lower on SVAMP, thus showing that much remains to be done even for the simplest of the MWPs.},
  annote   = {Comment: NAACL 2021},
  doi      = {10.48550/arXiv.2103.07191},
  file     = {:Patel2021 - Are NLP Models Really Able to Solve Simple Math Word Problems_.pdf:PDF},
  groups   = {Datasets 2023, ArXiv},
  keywords = {Computer Science - Computation and Language},
  school   = {arXiv},
  url      = {http://arxiv.org/abs/2103.07191},
  urldate  = {2023-10-25}
}

'
@inproceedings{Miao2020,
  author    = {Miao, Shen-yun and Liang, Chao-Chun and Su, Keh-Yih},
  booktitle = {Proceedings of the 58th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics}},
  title     = {A {Diverse} {Corpus} for {Evaluating} and {Developing} {English} {Math} {Word} {Problem} {Solvers}},
  year      = {2020},
  address   = {Online},
  month     = jul,
  pages     = {975--984},
  publisher = {Association for Computational Linguistics},
  abstract  = {We present ASDiv (Academia Sinica Diverse MWP Dataset), a diverse (in terms of both language patterns and problem types) English math word problem (MWP) corpus for evaluating the capability of various MWP solvers. Existing MWP corpora for studying AI progress remain limited either in language usage patterns or in problem types. We thus present a new English MWP corpus with 2,305 MWPs that cover more text patterns and most problem types taught in elementary school. Each MWP is annotated with its problem type and grade level (for indicating the level of difficulty). Furthermore, we propose a metric to measure the lexicon usage diversity of a given MWP corpus, and demonstrate that ASDiv is more diverse than existing corpora. Experiments show that our proposed corpus reflects the true capability of MWP solvers more faithfully.},
  doi       = {10.18653/v1/2020.acl-main.92},
  file      = {:Miao2020 - A Diverse Corpus for Evaluating and Developing English Math Word Problem Solvers.pdf:PDF},
  groups    = {Datasets 2023, 2020, ACL},
  url       = {https://aclanthology.org/2020.acl-main.92},
  urldate   = {2023-10-25}
}

'
@inproceedings{Chen2023,
  author    = {Chen, Yulin and Ding, Ning and Wang, Xiaobin and Hu, Shengding and Zheng, Haitao and Liu, Zhiyuan and Xie, Pengjun},
  booktitle = {Proceedings of the 61st {Annual} {Meeting} of the {Association} for {Computational} {Linguistics} ({Volume} 1: {Long} {Papers})},
  title     = {Exploring {Lottery} {Prompts} for {Pre}-trained {Language} {Models}},
  year      = {2023},
  address   = {Toronto, Canada},
  month     = jul,
  pages     = {15428--15444},
  publisher = {Association for Computational Linguistics},
  abstract  = {Consistently scaling pre-trained language models (PLMs) imposes substantial burdens on model adaptation, necessitating more efficient alternatives to conventional fine-tuning. Given the advantage of prompting in the zero-shot setting and the observed performance fluctuation among different prompts, we explore the instance-level prompt and their generalizability.By searching through the prompt space, we first validate the assumption that for every instance, there is almost always a lottery prompt that induces the correct prediction from the PLM, and such prompt can be obtained at a low cost thanks to the inherent ability of PLMs.Meanwhile, it is shown that some strong lottery prompts have high performance over the whole training set, and they are equipped with distinguishable linguistic features. Lastly, we attempt to generalize the searched strong lottery prompts to unseen data with prompt ensembling method. Experiments are conducted on various types of NLP classification tasks and demonstrate that the proposed method can achieve comparable results with other gradient-free and optimization-free baselines.},
  doi       = {10.18653/v1/2023.acl-long.860},
  file      = {:Chen2023 - Exploring Lottery Prompts for Pre Trained Language Models.pdf:PDF},
  groups    = {Prompting, 2023, ACL},
  url       = {https://aclanthology.org/2023.acl-long.860},
  urldate   = {2023-10-23}
}

'
@inproceedings{Shridhar2023,
  author    = {Shridhar, Kumar and Stolfo, Alessandro and Sachan, Mrinmaya},
  booktitle = {Findings of the {Association} for {Computational} {Linguistics}: {ACL} 2023},
  title     = {Distilling {Reasoning} {Capabilities} into {Smaller} {Language} {Models}},
  year      = {2023},
  address   = {Toronto, Canada},
  month     = jul,
  pages     = {7059--7073},
  publisher = {Association for Computational Linguistics},
  abstract  = {Step-by-step reasoning approaches like chain of thought (CoT) have proved to be very effective in inducing reasoning capabilities in large language models. However, the success of the CoT approach is fundamentally tied to the model size, and billion parameter-scale models are often needed to get CoT to work. In this paper, we propose a knowledge distillation approach that leverages the step-by-step CoT reasoning capabilities of larger models and distills these abilities into smaller models. In this work, we propose an alternative reasoning scheme, Socratic CoT that learns a decomposition of the original problem into a sequence of subproblems and uses it to guide the intermediate reasoning steps. We use Socratic CoT to train a combination of two small distilled models: a problem decomposer and a subproblem solver. In practice, given a new problem, the two distilled models work in sync to decompose and solve complex problems. On multiple reasoning datasets (GSM8K, StrategyQA, and SVAMP), our proposed distillation strategies boosts the performance of smaller models over 70\% compared to the baselines. Finally, we investigate when Socratic CoT is an effective alternative to CoT, demonstrating cases where a much smaller model (GPT-2 large) can outperform a 10X larger model (GPT-3 6B). Our code is available: https://github.com/kumar-shridhar/Distiiling-LM.},
  doi       = {10.18653/v1/2023.findings-acl.441},
  file      = {:Shridhar2023 - Distilling Reasoning Capabilities into Smaller Language Models.pdf:PDF},
  groups    = {Reasoning, 2023, ACL},
  url       = {https://aclanthology.org/2023.findings-acl.441},
  urldate   = {2023-10-23}
}

'
@inproceedings{Hsieh2023,
  author    = {Hsieh, Cheng-Yu and Li, Chun-Liang and Yeh, Chih-kuan and Nakhost, Hootan and Fujii, Yasuhisa and Ratner, Alex and Krishna, Ranjay and Lee, Chen-Yu and Pfister, Tomas},
  booktitle = {Findings of the {Association} for {Computational} {Linguistics}: {ACL} 2023},
  title     = {Distilling {Step}-by-{Step}! {Outperforming} {Larger} {Language} {Models} with {Less} {Training} {Data} and {Smaller} {Model} {Sizes}},
  year      = {2023},
  address   = {Toronto, Canada},
  month     = jul,
  pages     = {8003--8017},
  publisher = {Association for Computational Linguistics},
  abstract  = {Deploying large language models (LLMs) is challenging because they are memory inefficient and compute-intensive for practical applications. In reaction, researchers train smaller task-specific models by either finetuning with human labels or distilling using LLM-generated labels. However, finetuning and distillation require large amounts of training data to achieve comparable performance to LLMs. We introduce Distilling step-by-step, a new mechanism that (a) trains smaller models that outperform LLMs, and (b) achieves so by leveraging less training data needed by finetuning or distillation. Our method extracts LLM rationales as additional supervision for training small models within a multi-task framework. We present three findings across 4 NLP benchmarks: First, compared to both finetuning and distillation, our mechanism achieves better performance with much fewer labeled/unlabeled training examples. Second, compared to few-shot prompted LLMs, we achieve better performance using substantially smaller model sizes. Third, we reduce both the model size and the amount of data required to outperform LLMs; our finetuned 770M T5 model outperforms the few-shot prompted 540B PaLM model using only 80\% of available data on a benchmark, whereas standard finetuning the same T5 model struggles to match even by using 100\% of the dataset.},
  doi       = {10.18653/v1/2023.findings-acl.507},
  file      = {:Hsieh2023a - Distilling Step by Step! Outperforming Larger Language Models with Less Training Data and Smaller Model Sizes.pdf:PDF},
  groups    = {Reasoning, Multitask Learning, 2023, ACL},
  url       = {https://aclanthology.org/2023.findings-acl.507},
  urldate   = {2023-10-23}
}

'
@inproceedings{Ho2023,
  author    = {Ho, Namgyu and Schmid, Laura and Yun, Se-Young},
  booktitle = {Proceedings of the 61st {Annual} {Meeting} of the {Association} for {Computational} {Linguistics} ({Volume} 1: {Long} {Papers})},
  title     = {Large {Language} {Models} {Are} {Reasoning} {Teachers}},
  year      = {2023},
  address   = {Toronto, Canada},
  month     = jul,
  pages     = {14852--14882},
  publisher = {Association for Computational Linguistics},
  abstract  = {Recent works have shown that chain-of-thought (CoT) prompting can elicit language models to solve complex reasoning tasks, step-by-step. However, prompt-based CoT methods are dependent on very large models such as GPT-3 175B which are prohibitive to deploy at scale. In this paper, we use these large models as reasoning teachers to enable complex reasoning in smaller models and reduce model size requirements by several orders of magnitude. We propose Fine-tune-CoT, a method that generates reasoning samples from very large teacher models to fine-tune smaller models. We evaluate our method on a wide range of public models and complex tasks. We find that Fine-tune-CoT enables substantial reasoning capability in small models, far outperforming prompt-based baselines and even the teacher model in many tasks. Additionally, we extend our method by leveraging the teacher model's ability to generate multiple distinct rationales for each original sample. Enriching the fine-tuning data with such diverse reasoning results in a substantial performance boost across datasets, even for very small models. We conduct ablations and sample studies to understand the emergence of reasoning capabilities of student models. Our code implementation and data are available at https://github.com/itsnamgyu/reasoning-teacher.},
  doi       = {10.18653/v1/2023.acl-long.830},
  file      = {:Ho2023 - Large Language Models Are Reasoning Teachers (1).pdf:PDF},
  groups    = {Reasoning, 2023, ACL},
  url       = {https://aclanthology.org/2023.acl-long.830},
  urldate   = {2023-10-23}
}

'
@inproceedings{Magister2023,
  author    = {Magister, Lucie Charlotte and Mallinson, Jonathan and Adamek, Jakub and Malmi, Eric and Severyn, Aliaksei},
  booktitle = {Proceedings of the 61st {Annual} {Meeting} of the {Association} for {Computational} {Linguistics} ({Volume} 2: {Short} {Papers})},
  title     = {Teaching {Small} {Language} {Models} to {Reason}},
  year      = {2023},
  address   = {Toronto, Canada},
  month     = jul,
  pages     = {1773--1781},
  publisher = {Association for Computational Linguistics},
  abstract  = {Chain of thought prompting successfully improves the reasoning capabilities of large language models, achieving state of the art results on a range of datasets. However, these reasoning capabilities only appear to emerge in models with at least tens of billions of parameters. In this paper, we explore the transfer of such reasoning capabilities to smaller models via knowledge distillation, also investigating model and dataset size trade-off. Specifically, we finetune a student model on the chain of thought outputs generated by a larger teacher model. Our experiments show that the proposed method improves task performance across arithmetic, commonsense and symbolic reasoning datasets. For example, the accuracy of T5 XXL on GSM8K improves from 8.11\% to 21.99\% and 18.42\% when finetuned on PaLM 540B and GPT-3 175B generated chains of thought, respectively.},
  doi       = {10.18653/v1/2023.acl-short.151},
  file      = {:Magister2023 - Teaching Small Language Models to Reason.pdf:PDF},
  groups    = {2023, ACL},
  url       = {https://aclanthology.org/2023.acl-short.151},
  urldate   = {2023-10-23}
}

'
@inproceedings{Zhao2023a,
  author    = {Zhao, Xuandong and Ouyang, Siqi and Yu, Zhiguo and Wu, Ming and Li, Lei},
  booktitle = {Proceedings of the 61st {Annual} {Meeting} of the {Association} for {Computational} {Linguistics} ({Volume} 1: {Long} {Papers})},
  title     = {Pre-trained {Language} {Models} {Can} be {Fully} {Zero}-{Shot} {Learners}},
  year      = {2023},
  address   = {Toronto, Canada},
  month     = jul,
  pages     = {15590--15606},
  publisher = {Association for Computational Linguistics},
  abstract  = {How can we extend a pre-trained model to many language understanding tasks, without labeled or additional unlabeled data? Pre-trained language models (PLMs) have been effective for a wide range of NLP tasks. However, existing approaches either require fine-tuning on downstream labeled datasets or manually constructing proper prompts. In this paper, we propose nonparametric prompting PLM (NPPrompt) for fully zero-shot language understanding. Unlike previous methods, NPPrompt uses only pre-trained language models and does not require any labeled data or additional raw corpus for further fine-tuning, nor does it rely on humans to construct a comprehensive set of prompt label words. We evaluate NPPrompt against previous major few-shot and zero-shot learning methods on diverse NLP tasks: including text classification, text entailment, similar text retrieval, paraphrasing, and multiple-choice question answering. Experimental results demonstrate that our NPPrompt outperforms the previous best fully zero-shot method by big margins, with absolute gains of 12.8\% in accuracy on text classification and 15.6\% on the GLUE benchmark. Our source code is available at https://anonymous.4open.science/r/NPPrompt.},
  doi       = {10.18653/v1/2023.acl-long.869},
  file      = {:Zhao2023a - Pre Trained Language Models Can Be Fully Zero Shot Learners.pdf:PDF},
  groups    = {Prompting, 2023, ACL},
  url       = {https://aclanthology.org/2023.acl-long.869},
  urldate   = {2023-10-24}
}

'
@inproceedings{Lu2023,
  author    = {Lu, Jinghui and Zhu, Dongsheng and Han, Weidong and Zhao, Rui and Mac Namee, Brian and Tan, Fei},
  booktitle = {Proceedings of the 61st {Annual} {Meeting} of the {Association} for {Computational} {Linguistics} ({Volume} 1: {Long} {Papers})},
  title     = {What {Makes} {Pre}-trained {Language} {Models} {Better} {Zero}-shot {Learners}?},
  year      = {2023},
  address   = {Toronto, Canada},
  month     = jul,
  pages     = {2288--2303},
  publisher = {Association for Computational Linguistics},
  abstract  = {Current methods for prompt learning in zero-shot scenarios widely rely on a development set with sufficient human-annotated data to select the best-performing prompt template a posteriori. This is not ideal because in a real-world zero-shot scenario of practical relevance, no labelled data is available. Thus, we propose a simple yet effective method for screening reasonable prompt templates in zero-shot text classification: Perplexity Selection (Perplection). We hypothesize that language discrepancy can be used to measure the efficacy of prompt templates, and thereby develop a substantiated perplexity-based scheme allowing for forecasting the performance of prompt templates in advance. Experiments show that our method leads to improved prediction performance in a realistic zero-shot setting, eliminating the need for any labelled examples.},
  doi       = {10.18653/v1/2023.acl-long.128},
  file      = {:Lu2023 - What Makes Pre Trained Language Models Better Zero Shot Learners_.pdf:PDF},
  groups    = {Prompting, 2023, ACL},
  url       = {https://aclanthology.org/2023.acl-long.128},
  urldate   = {2023-10-24}
}

'
@inproceedings{Chen2023a,
  author    = {Chen, Jiawei and Lu, Yaojie and Lin, Hongyu and Lou, Jie and Jia, Wei and Dai, Dai and Wu, Hua and Cao, Boxi and Han, Xianpei and Sun, Le},
  booktitle = {Proceedings of the 61st {Annual} {Meeting} of the {Association} for {Computational} {Linguistics} ({Volume} 1: {Long} {Papers})},
  title     = {Learning {In}-context {Learning} for {Named} {Entity} {Recognition}},
  year      = {2023},
  address   = {Toronto, Canada},
  month     = jul,
  pages     = {13661--13675},
  publisher = {Association for Computational Linguistics},
  abstract  = {Named entity recognition in real-world applications suffers from the diversity of entity types, the emergence of new entity types, and the lack of high-quality annotations. To address the above problems, this paper proposes an in-context learning-based NER approach, which can effectively inject in-context NER ability into PLMs and recognize entities of novel types on-the-fly using only a few demonstrative instances. Specifically, we model PLMs as a meta-function Lambda\_instruction, demonstrations, text.M, and a new entity extractor can be implicitly constructed by applying new instruction and demonstrations to PLMs, i.e., (Lambda . M) (instruction, demonstrations) -{\textbackslash}textgreaterF where F will be a new entity extractor F: text -{\textbackslash}textgreater entities. To inject the above in-context NER ability into PLMs, we propose a meta-function pre-training algorithm, which pre-trains PLMs by comparing the (instruction, demonstration)-initialized extractor with a surrogate golden extractor. Experimental results on 4 few-shot NER datasets show that our method can effectively inject in-context NER ability into PLMs and significantly outperforms the PLMs+fine-tuning counterparts.},
  doi       = {10.18653/v1/2023.acl-long.764},
  file      = {:Chen2023a - Learning in Context Learning for Named Entity Recognition.pdf:PDF},
  groups    = {NER, 2023, ACL},
  url       = {https://aclanthology.org/2023.acl-long.764},
  urldate   = {2023-10-24}
}

'
@inproceedings{Shen2023,
  author     = {Shen, Yongliang and Tan, Zeqi and Wu, Shuhui and Zhang, Wenqi and Zhang, Rongsheng and Xi, Yadong and Lu, Weiming and Zhuang, Yueting},
  booktitle  = {Proceedings of the 61st {Annual} {Meeting} of the {Association} for {Computational} {Linguistics} ({Volume} 1: {Long} {Papers})},
  title      = {{PromptNER}: {Prompt} {Locating} and {Typing} for {Named} {Entity} {Recognition}},
  year       = {2023},
  address    = {Toronto, Canada},
  month      = jul,
  pages      = {12492--12507},
  publisher  = {Association for Computational Linguistics},
  abstract   = {Prompt learning is a new paradigm for utilizing pre-trained language models and has achieved great success in many tasks. To adopt prompt learning in the NER task, two kinds of methods have been explored from a pair of symmetric perspectives, populating the template by enumerating spans to predict their entity types or constructing type-specific prompts to locate entities. However, these methods not only require a multi-round prompting manner with a high time overhead and computational cost, but also require elaborate prompt templates, that are difficult to apply in practical scenarios. In this paper, we unify entity locating and entity typing into prompt learning, and design a dual-slot multi-prompt template with the position slot and type slot to prompt locating and typing respectively. Multiple prompts can be input to the model simultaneously, and then the model extracts all entities by parallel predictions on the slots. To assign labels for the slots during training, we design a dynamic template filling mechanism that uses the extended bipartite graph matching between prompts and the ground-truth entities. We conduct experiments in various settings, including resource-rich flat and nested NER datasets and low-resource in-domain and cross-domain datasets. Experimental results show that the proposed model achieves a significant performance improvement, especially in the cross-domain few-shot setting, which outperforms the state-of-the-art model by +7.7\% on average.},
  doi        = {10.18653/v1/2023.acl-long.698},
  file       = {:Shen2023 - PromptNER_ Prompt Locating and Typing for Named Entity Recognition.pdf:PDF},
  groups     = {NER, 2023, ACL},
  shorttitle = {{PromptNER}},
  url        = {https://aclanthology.org/2023.acl-long.698},
  urldate    = {2023-10-24}
}

'
@inproceedings{Gao2021,
  author    = {Gao, Tianyu and Fisch, Adam and Chen, Danqi},
  booktitle = {Proceedings of the 59th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics} and the 11th {International} {Joint} {Conference} on {Natural} {Language} {Processing} ({Volume} 1: {Long} {Papers})},
  title     = {Making {Pre}-trained {Language} {Models} {Better} {Few}-shot {Learners}},
  year      = {2021},
  address   = {Online},
  month     = aug,
  pages     = {3816--3830},
  publisher = {Association for Computational Linguistics},
  abstract  = {The recent GPT-3 model (Brown et al., 2020) achieves remarkable few-shot performance solely by leveraging a natural-language prompt and a few task demonstrations as input context. Inspired by their findings, we study few-shot learning in a more practical scenario, where we use smaller language models for which fine-tuning is computationally efficient. We present LM-BFF—better few-shot fine-tuning of language models—a suite of simple and complementary techniques for fine-tuning language models on a small number of annotated examples. Our approach includes (1) prompt-based fine-tuning together with a novel pipeline for automating prompt generation; and (2) a refined strategy for dynamically and selectively incorporating demonstrations into each context. Finally, we present a systematic evaluation for analyzing few-shot performance on a range of NLP tasks, including classification and regression. Our experiments demonstrate that our methods combine to dramatically outperform standard fine-tuning procedures in this low resource setting, achieving up to 30\% absolute improvement, and 11\% on average across all tasks. Our approach makes minimal assumptions on task resources and domain expertise, and hence constitutes a strong task-agnostic method for few-shot learning.},
  doi       = {10.18653/v1/2021.acl-long.295},
  file      = {:Gao2021 - Making Pre Trained Language Models Better Few Shot Learners.pdf:PDF},
  groups    = {2021, ACL},
  url       = {https://aclanthology.org/2021.acl-long.295},
  urldate   = {2023-10-26}
}

'
@techreport{Liu2021,
  author     = {Liu, Pengfei and Yuan, Weizhe and Fu, Jinlan and Jiang, Zhengbao and Hayashi, Hiroaki and Neubig, Graham},
  title      = {Pre-train, {Prompt}, and {Predict}: {A} {Systematic} {Survey} of {Prompting} {Methods} in {Natural} {Language} {Processing}},
  year       = {2021},
  month      = jul,
  note       = {arXiv:2107.13586 [cs] type: article},
  abstract   = {This paper surveys and organizes research works in a new paradigm in natural language processing, which we dub "prompt-based learning". Unlike traditional supervised learning, which trains a model to take in an input x and predict an output y as P(y{\textbar}x), prompt-based learning is based on language models that model the probability of text directly. To use these models to perform prediction tasks, the original input x is modified using a template into a textual string prompt x' that has some unfilled slots, and then the language model is used to probabilistically fill the unfilled information to obtain a final string x, from which the final output y can be derived. This framework is powerful and attractive for a number of reasons: it allows the language model to be pre-trained on massive amounts of raw text, and by defining a new prompting function the model is able to perform few-shot or even zero-shot learning, adapting to new scenarios with few or no labeled data. In this paper we introduce the basics of this promising paradigm, describe a unified set of mathematical notations that can cover a wide variety of existing work, and organize existing work along several dimensions, e.g.the choice of pre-trained models, prompts, and tuning strategies. To make the field more accessible to interested beginners, we not only make a systematic review of existing works and a highly structured typology of prompt-based concepts, but also release other resources, e.g., a website http://pretrain.nlpedia.ai/ including constantly-updated survey, and paperlist.},
  annote     = {Comment: Website: http://pretrain.nlpedia.ai/},
  file       = {:Liu2021 - Pre Train, Prompt, and Predict_ a Systematic Survey of Prompting Methods in Natural Language Processing.pdf:PDF},
  groups     = {ArXiv, Prompting},
  keywords   = {Computer Science - Computation and Language, Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
  school     = {arXiv},
  shorttitle = {Pre-train, {Prompt}, and {Predict}},
  url        = {http://arxiv.org/abs/2107.13586},
  urldate    = {2023-10-26}
}

'
@techreport{Zhu2018,
  author     = {Zhu, Yaoming and Lu, Sidi and Zheng, Lei and Guo, Jiaxian and Zhang, Weinan and Wang, Jun and Yu, Yong},
  title      = {Texygen: {A} {Benchmarking} {Platform} for {Text} {Generation} {Models}},
  year       = {2018},
  month      = feb,
  note       = {arXiv:1802.01886 [cs] type: article},
  abstract   = {We introduce Texygen, a benchmarking platform to support research on open-domain text generation models. Texygen has not only implemented a majority of text generation models, but also covered a set of metrics that evaluate the diversity, the quality and the consistency of the generated texts. The Texygen platform could help standardize the research on text generation and facilitate the sharing of fine-tuned open-source implementations among researchers for their work. As a consequence, this would help in improving the reproductivity and reliability of future research work in text generation.},
  annote     = {Comment: 4 pages},
  file       = {:Zhu2018 - Texygen_ a Benchmarking Platform for Text Generation Models.pdf:PDF},
  groups     = {Metrics},
  keywords   = {Computer Science - Computation and Language, Computer Science - Information Retrieval, Computer Science - Machine Learning},
  school     = {arXiv},
  shorttitle = {Texygen},
  url        = {http://arxiv.org/abs/1802.01886},
  urldate    = {2023-10-27}
}

@inproceedings{Jiang2023,
  author    = {Jiang, Zhiying and Yang, Matthew and Tsirlin, Mikhail and Tang, Raphael and Dai, Yiqin and Lin, Jimmy},
  booktitle = {Findings of the Association for Computational Linguistics: ACL 2023},
  title     = {{``}Low-Resource{''} Text Classification: A Parameter-Free Classification Method with Compressors},
  year      = {2023},
  address   = {Toronto, Canada},
  month     = {jul},
  pages     = {6810--6828},
  publisher = {Association for Computational Linguistics},
  doi       = {10.18653/v1/2023.findings-acl.426},
  groups    = {ACL, 2023},
  url       = {https://aclanthology.org/2023.findings-acl.426}
}

@inproceedings{Shah2023,
  author    = {Shah, Aditya and Thapa, Surendrabikram and Jain, Aneesh and Huang, Lifu},
  booktitle = {Proceedings of The Fourth Workshop on Simple and Efficient Natural Language Processing (SustaiNLP)},
  title     = {{ADEPT}: Adapter-based Efficient Prompt Tuning Approach for Language Models},
  year      = {2023},
  address   = {Toronto, Canada (Hybrid)},
  month     = {jul},
  pages     = {121--128},
  publisher = {Association for Computational Linguistics},
  doi       = {10.18653/v1/2023.sustainlp-1.8},
  groups    = {ACL, 2023, Soft-Prompt},
  url       = {https://aclanthology.org/2023.sustainlp-1.8}
}

@inproceedings{Yu2023,
  author    = {Yu, Peilin and Bach, Stephen},
  booktitle = {Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 3: System Demonstrations)},
  title     = {Alfred: A System for Prompted Weak Supervision},
  year      = {2023},
  address   = {Toronto, Canada},
  month     = {jul},
  pages     = {479--488},
  publisher = {Association for Computational Linguistics},
  doi       = {10.18653/v1/2023.acl-demo.46},
  groups    = {ACL, 2023, Weak Supervision},
  url       = {https://aclanthology.org/2023.acl-demo.46}
}

'
@inproceedings{Fei2022,
  author     = {Fei, Yu and Meng, Zhao and Nie, Ping and Wattenhofer, Roger and Sachan, Mrinmaya},
  booktitle  = {Proceedings of the 2022 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing}},
  title      = {Beyond prompting: {Making} {Pre}-trained {Language} {Models} {Better} {Zero}-shot {Learners} by {Clustering} {Representations}},
  year       = {2022},
  address    = {Abu Dhabi, United Arab Emirates},
  editor     = {Goldberg, Yoav and Kozareva, Zornitsa and Zhang, Yue},
  month      = dec,
  pages      = {8560--8579},
  publisher  = {Association for Computational Linguistics},
  abstract   = {Recent work has demonstrated that pre-trained language models (PLMs) are zero-shot learners. However, most existing zero-shot methods involve heavy human engineering or complicated self-training pipelines, hindering their application to new situations. In this work, we show that zero-shot text classification can be improved simply by clustering texts in the embedding spaces of PLMs. Specifically, we fit the unlabeled texts with a Bayesian Gaussian Mixture Model after initializing cluster positions and shapes using class names. Despite its simplicity, this approach achieves superior or comparable performance on both topic and sentiment classification datasets and outperforms prior works significantly on unbalanced datasets. We further explore the applicability of our clustering approach by evaluating it on 14 datasets with more diverse topics, text lengths, and numbers of classes. Our approach achieves an average of 20\% absolute improvement over prompt-based zero-shot learning. Finally, we compare different PLM embedding spaces and find that texts are well-clustered by topics even if the PLM is not explicitly pre-trained to generate meaningful sentence embeddings. This work indicates that PLM embeddings can categorize texts without task-specific fine-tuning, thus providing a new way to analyze and utilize their knowledge and zero-shot learning ability.},
  doi        = {10.18653/v1/2022.emnlp-main.587},
  file       = {:Fei2022 - Beyond Prompting_ Making Pre Trained Language Models Better Zero Shot Learners by Clustering Representations.pdf:PDF},
  groups     = {Prompting, EMNLP},
  shorttitle = {Beyond prompting},
  url        = {https://aclanthology.org/2022.emnlp-main.587},
  urldate    = {2023-10-31}
}

@inproceedings{Fei2023,
  author    = {Fei, Hao and Li, Bobo and Liu, Qian and Bing, Lidong and Li, Fei and Chua, Tat-Seng},
  booktitle = {Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)},
  title     = {Reasoning Implicit Sentiment with Chain-of-Thought Prompting},
  year      = {2023},
  address   = {Toronto, Canada},
  editor    = {Rogers, Anna and Boyd-Graber, Jordan and Okazaki, Naoaki},
  month     = {jul},
  pages     = {1171--1182},
  publisher = {Association for Computational Linguistics},
  abstract  = {While sentiment analysis systems try to determine the sentiment polarities of given targets based on the key opinion expressions in input texts, in implicit sentiment analysis (ISA) the opinion cues come in an implicit and obscure manner. Thus detecting implicit sentiment requires the common-sense and multi-hop reasoning ability to infer the latent intent of opinion. Inspired by the recent chain-of-thought (CoT) idea, in this work we introduce a Three-hop Reasoning (THOR) CoT framework to mimic the human-like reasoning process for ISA. We design a three-step prompting principle for THOR to step-by-step induce the implicit aspect, opinion, and finally the sentiment polarity. Our THOR+Flan-T5 (11B) pushes the state-of-the-art (SoTA) by over 6{\%} F1 on supervised setup. More strikingly, THOR+GPT3 (175B) boosts the SoTA by over 50{\%} F1 on zero-shot setting.},
  doi       = {10.18653/v1/2023.acl-short.101},
  group     = {2_sentiment_emotion_classification_analysis},
  groups    = {2_sentiment_emotion_classification_analysis, ACL, 2023, Reasoning, Prompting},
  url       = {https://aclanthology.org/2023.acl-short.101}
}

@inproceedings{Chen2023b,
  author    = {Chen, Chih Yao and Hung, Tun Min and Hsu, Yi-Li and Ku, Lun-Wei},
  booktitle = {Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
  title     = {Label-Aware Hyperbolic Embeddings for Fine-grained Emotion Classification},
  year      = {2023},
  address   = {Toronto, Canada},
  editor    = {Rogers, Anna and Boyd-Graber, Jordan and Okazaki, Naoaki},
  month     = {jul},
  pages     = {10947--10958},
  publisher = {Association for Computational Linguistics},
  abstract  = {Fine-grained emotion classification (FEC) is a challenging task. Specifically, FEC needs to handle subtle nuance between labels, which can be complex and confusing. Most existing models only address text classification problem in the euclidean space, which we believe may not be the optimal solution as labels of close semantic (e.g., afraid and terrified) may not be differentiated in such space, which harms the performance. In this paper, we propose HypEmo, a novel framework that can integrate hyperbolic embeddings to improve the FEC task. First, we learn label embeddings in the hyperbolic space to better capture their hierarchical structure, and then our model projects contextualized representations to the hyperbolic space to compute the distance between samples and labels. Experimental results show that incorporating such distance to weight cross entropy loss substantially improve the performance on two benchmark datasets, with around 3{\%} improvement compared to previous state-of-the-art, and could even improve up to 8.6{\%} when the labels are hard to distinguish. Code is available at \url{https://github.com/dinobby/HypEmo}.},
  doi       = {10.18653/v1/2023.acl-long.613},
  group     = {2_sentiment_emotion_classification_analysis},
  groups    = {2_sentiment_emotion_classification_analysis, ACL, 2023},
  url       = {https://aclanthology.org/2023.acl-long.613}
}

@inproceedings{Chakraborty2023,
  author    = {Chakraborty, Mohna and Kulkarni, Adithya and Li, Qi},
  booktitle = {Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
  title     = {Zero-shot Approach to Overcome Perturbation Sensitivity of Prompts},
  year      = {2023},
  address   = {Toronto, Canada},
  editor    = {Rogers, Anna and Boyd-Graber, Jordan and Okazaki, Naoaki},
  month     = {jul},
  pages     = {5698--5711},
  publisher = {Association for Computational Linguistics},
  abstract  = {Recent studies have demonstrated that natural-language prompts can help to leverage the knowledge learned by pre-trained language models for the binary sentence-level sentiment classification task. Specifically, these methods utilize few-shot learning settings to fine-tune the sentiment classification model using manual or automatically generated prompts. However, the performance of these methods is sensitive to the perturbations of the utilized prompts. Furthermore, these methods depend on a few labeled instances for automatic prompt generation and prompt ranking. This study aims to find high-quality prompts for the given task in a zero-shot setting. Given a base prompt, our proposed approach automatically generates multiple prompts similar to the base prompt employing positional, reasoning, and paraphrasing techniques and then ranks the prompts using a novel metric. We empirically demonstrate that the top-ranked prompts are high-quality and significantly outperform the base prompt and the prompts generated using few-shot learning for the binary sentence-level sentiment classification task.},
  doi       = {10.18653/v1/2023.acl-long.313},
  group     = {2_sentiment_emotion_classification_analysis},
  groups    = {2_sentiment_emotion_classification_analysis, ACL, 2023, Prompting},
  url       = {https://aclanthology.org/2023.acl-long.313}
}

@inproceedings{Barnes2023,
  author    = {Barnes, Jeremy},
  booktitle = {Proceedings of the 13th Workshop on Computational Approaches to Subjectivity, Sentiment, {\&} Social Media Analysis},
  title     = {Sentiment and Emotion Classification in Low-resource Settings},
  year      = {2023},
  address   = {Toronto, Canada},
  editor    = {Barnes, Jeremy and De Clercq, Orph{\'e}e and Klinger, Roman},
  month     = {jul},
  pages     = {290--304},
  publisher = {Association for Computational Linguistics},
  abstract  = {The popularity of sentiment and emotion analysis has lead to an explosion of datasets, approaches, and papers. However, these are often tested in optimal settings, where plentiful training and development data are available, and compared mainly with recent state-of-the-art models that have been similarly evaluated. In this paper, we instead present a systematic comparison of sentiment and emotion classification methods, ranging from rule- and dictionary-based methods to recently proposed few-shot and prompting methods with large language models. We test these methods in-domain, out-of-domain, and in cross-lingual settings and find that in low-resource settings, rule- and dictionary-based methods perform as well or better than few-shot and prompting methods, especially for emotion classification. Zero-shot cross-lingual approaches, however, still outperform in-language dictionary induction.},
  doi       = {10.18653/v1/2023.wassa-1.26},
  group     = {2_sentiment_emotion_classification_analysis},
  groups    = {2_sentiment_emotion_classification_analysis, ACL, 2023, Prompting},
  ranking   = {rank5},
  url       = {https://aclanthology.org/2023.wassa-1.26}
}

@inproceedings{Zhang2023,
  author    = {Zhang, Jinghui and Yang, Dongming and Bao, Siyu and Cao, Lina and Fan, Shunguo},
  booktitle = {Proceedings of the 13th Workshop on Computational Approaches to Subjectivity, Sentiment, {\&} Social Media Analysis},
  title     = {Emotion classification on code-mixed text messages via soft prompt tuning},
  year      = {2023},
  address   = {Toronto, Canada},
  editor    = {Barnes, Jeremy and De Clercq, Orph{\'e}e and Klinger, Roman},
  month     = jul,
  pages     = {596--600},
  publisher = {Association for Computational Linguistics},
  abstract  = {Emotion classification on code-mixed text messages is challenging due to the multilingual languages and non-literal cues (i.e., emoticons). To solve these problems, we propose an innovative soft prompt tuning method, which is lightweight and effective to release potential abilities of the pre-trained language models and improve the classification results. Firstly, we transform emoticons into textual information to utilize their rich emotional information. Then, variety of innovative templates and verbalizers are applied to promote emotion classification. Extensive experiments show that transforming emoticons and employing prompt tuning both benefit the performance. Finally, as a part of WASSA 2023, we obtain the accuracy of 0.972 in track MLEC and 0.892 in track MCEC, yielding the second place in both two tracks.},
  doi       = {10.18653/v1/2023.wassa-1.57},
  group     = {2_sentiment_emotion_classification_analysis},
  groups    = {2_sentiment_emotion_classification_analysis, ACL, 2023, Soft-Prompt},
  url       = {https://aclanthology.org/2023.wassa-1.57}
}

@inproceedings{Varia2023,
  author    = {Varia, Siddharth and Wang, Shuai and Halder, Kishaloy and Vacareanu, Robert and Ballesteros, Miguel and Benajiba, Yassine and Anna John, Neha and Anubhai, Rishita and Muresan, Smaranda and Roth, Dan},
  booktitle = {Proceedings of the 13th Workshop on Computational Approaches to Subjectivity, Sentiment, {\&} Social Media Analysis},
  title     = {Instruction Tuning for Few-Shot Aspect-Based Sentiment Analysis},
  year      = {2023},
  address   = {Toronto, Canada},
  editor    = {Barnes, Jeremy and De Clercq, Orph{\'e}e and Klinger, Roman},
  month     = {jul},
  pages     = {19--27},
  publisher = {Association for Computational Linguistics},
  abstract  = {Aspect-based Sentiment Analysis (ABSA) is a fine-grained sentiment analysis task which involves four elements from user-generated texts:aspect term, aspect category, opinion term, and sentiment polarity. Most computational approaches focus on some of the ABSA sub-taskssuch as tuple (aspect term, sentiment polarity) or triplet (aspect term, opinion term, sentiment polarity) extraction using either pipeline or joint modeling approaches. Recently, generative approaches have been proposed to extract all four elements as (one or more) quadrupletsfrom text as a single task. In this work, we take a step further and propose a unified framework for solving ABSA, and the associated sub-tasksto improve the performance in few-shot scenarios. To this end, we fine-tune a T5 model with instructional prompts in a multi-task learning fashion covering all the sub-tasks, as well as the entire quadruple prediction task. In experiments with multiple benchmark datasets, we show that the proposed multi-task prompting approach brings performance boost (by absolute 8.29 F1) in the few-shot learning setting.},
  doi       = {10.18653/v1/2023.wassa-1.3},
  group     = {2_sentiment_emotion_classification_analysis},
  groups    = {2_sentiment_emotion_classification_analysis, ACL, 2023, Tunning/Learning, Instructions},
  url       = {https://aclanthology.org/2023.wassa-1.3}
}

@inproceedings{Zhao2023b,
  author    = {Zhao, Yang and Nasukawa, Tetsuya and Muraoka, Masayasu and Bhattacharjee, Bishwaranjan},
  booktitle = {Findings of the Association for Computational Linguistics: ACL 2023},
  title     = {A Simple Yet Strong Domain-Agnostic De-bias Method for Zero-Shot Sentiment Classification},
  year      = {2023},
  address   = {Toronto, Canada},
  editor    = {Rogers, Anna and Boyd-Graber, Jordan and Okazaki, Naoaki},
  month     = {jul},
  pages     = {3923--3931},
  publisher = {Association for Computational Linguistics},
  abstract  = {Zero-shot prompt-based learning has made much progress in sentiment analysis, and considerable effort has been dedicated to designing high-performing prompt templates. However, two problems exist; First, large language models are often biased to their pre-training data, leading to poor performance in prompt templates that models have rarely seen. Second, in order to adapt to different domains, re-designing prompt templates is usually required, which is time-consuming and inefficient. To remedy both shortcomings, we propose a simple yet strong data construction method to de-bias a given prompt template, yielding a large performance improvement in sentiment analysis tasks across different domains, pre-trained language models, and prompt templates. Also, we demonstrate the advantage of using domain-agnostic generic responses over the in-domain ground-truth data.},
  doi       = {10.18653/v1/2023.findings-acl.242},
  group     = {2_sentiment_emotion_classification_analysis},
  groups    = {2_sentiment_emotion_classification_analysis, ACL, 2023, Prompting},
  url       = {https://aclanthology.org/2023.findings-acl.242}
}

@inproceedings{Zhuang2023,
  author    = {Zhuang, Yuan and Riloff, Ellen},
  booktitle = {Findings of the Association for Computational Linguistics: ACL 2023},
  title     = {Eliciting Affective Events from Language Models by Multiple View Co-prompting},
  year      = {2023},
  address   = {Toronto, Canada},
  editor    = {Rogers, Anna and Boyd-Graber, Jordan and Okazaki, Naoaki},
  month     = {jul},
  pages     = {3189--3201},
  publisher = {Association for Computational Linguistics},
  abstract  = {Prior research on affective event classification showed that exploiting weakly labeled data for training can improve model performance. In this work, we propose a simpler and more effective approach for generating training data by automatically acquiring and labeling affective events with Multiple View Co-prompting, which leverages two language model prompts that provide independent views of an event. The approach starts with a modest amount of gold data and prompts pre-trained language models to generate new events. Next, information about the probable affective polarity of each event is collected from two complementary language model prompts and jointly used to assign polarity labels. Experimental results on two datasets show that the newly acquired events improve a state-of-the-art affective event classifier. We also present analyses which show that using multiple views produces polarity labels of higher quality than either view on its own.},
  doi       = {10.18653/v1/2023.findings-acl.199},
  group     = {2_sentiment_emotion_classification_analysis},
  groups    = {2_sentiment_emotion_classification_analysis, ACL, 2023, Prompting, Weak Supervision},
  url       = {https://aclanthology.org/2023.findings-acl.199}
}

@inproceedings{Liu2023,
  author    = {Liu, Chaoqun and Zhang, Wenxuan and Chen, Guizhen and Wu, Xiaobao and Luu, Anh Tuan and Chang, Chip Hong and Bing, Lidong},
  booktitle = {Findings of the Association for Computational Linguistics: ACL 2023},
  title     = {Zero-Shot Text Classification via Self-Supervised Tuning},
  year      = {2023},
  address   = {Toronto, Canada},
  editor    = {Rogers, Anna and Boyd-Graber, Jordan and Okazaki, Naoaki},
  month     = {jul},
  pages     = {1743--1761},
  publisher = {Association for Computational Linguistics},
  abstract  = {Existing solutions to zero-shot text classification either conduct prompting with pre-trained language models, which is sensitive to the choices of templates, or rely on large-scale annotated data of relevant tasks for meta-tuning. In this work, we propose a new paradigm based on self-supervised learning to solve zero-shot text classification tasks by tuning the language models with unlabeled data, called self-supervised tuning. By exploring the inherent structure of free texts, we propose a new learning objective called first sentence prediction to bridge the gap between unlabeled data and text classification tasks. After tuning the model to learn to predict the first sentence in a paragraph based on the rest, the model is able to conduct zero-shot inference on unseen tasks such as topic classification and sentiment analysis. Experimental results show that our model outperforms the state-of-the-art baselines on 7 out of 10 tasks. Moreover, the analysis reveals that our model is less sensitive to the prompt design. Our code and pre-trained models are publicly available at \url{https://github.com/DAMO-NLP-SG/SSTuning}.},
  doi       = {10.18653/v1/2023.findings-acl.110},
  group     = {2_sentiment_emotion_classification_analysis},
  groups    = {2_sentiment_emotion_classification_analysis, ACL, 2023, Tunning/Learning},
  url       = {https://aclanthology.org/2023.findings-acl.110}
}

@inproceedings{Cho2023,
  author    = {Cho, Hyunsoo and Kim, Youna and Lee, Sang-goo},
  booktitle = {Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
  title     = {{CELDA}: Leveraging Black-box Language Model as Enhanced Classifier without Labels},
  year      = {2023},
  address   = {Toronto, Canada},
  editor    = {Rogers, Anna and Boyd-Graber, Jordan and Okazaki, Naoaki},
  month     = {jul},
  pages     = {4364--4379},
  publisher = {Association for Computational Linguistics},
  abstract  = {Utilizing language models (LMs) without internal access is becoming an attractive paradigm in the field of NLP as many cutting-edge LMs are released through APIs and boast a massive scale. The de-facto method in this type of black-box scenario is known as prompting, which has shown progressive performance enhancements in situations where data labels are scarce or unavailable. Despite their efficacy, they still fall short in comparison to fully supervised counterparts and are generally brittle to slight modifications. In this paper, we propose Clustering-enhanced Linear Discriminative Analysis (CELDA), a novel approach that improves the text classification accuracy with a very weak-supervision signal (i.e., name of the labels).Our framework draws a precise decision boundary without accessing weights or gradients of the LM model or data labels. The core ideas of CELDA are twofold:(1) extracting a refined pseudo-labeled dataset from an unlabeled dataset, and (2) training a lightweight and robust model on the top of LM, which learns an accurate decision boundary from an extracted noisy dataset. Throughout in-depth investigations on various datasets, we demonstrated that CELDA reaches new state-of-the-art in weakly-supervised text classification and narrows the gap with a fully-supervised model. Additionally, our proposed methodology can be applied universally to any LM and has the potential to scale to larger models, making it a more viable option for utilizing large LMs.},
  doi       = {10.18653/v1/2023.acl-long.239},
  group     = {5_classification_data_methods_label},
  groups    = {5_classification_data_methods_label, ACL, 2023, Prompting},
  url       = {https://aclanthology.org/2023.acl-long.239}
}

@inproceedings{Honovich2023,
  author    = {Honovich, Or and Shaham, Uri and Bowman, Samuel R. and Levy, Omer},
  booktitle = {Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
  title     = {Instruction Induction: From Few Examples to Natural Language Task Descriptions},
  year      = {2023},
  address   = {Toronto, Canada},
  editor    = {Rogers, Anna and Boyd-Graber, Jordan and Okazaki, Naoaki},
  month     = {jul},
  pages     = {1935--1952},
  publisher = {Association for Computational Linguistics},
  abstract  = {Large language models are able to perform a task by conditioning on a few input-output demonstrations - a paradigm known as in-context learning. We show that language models can explicitly infer an underlying task from a few demonstrations by prompting them to generate a natural language instruction that fits the examples. To explore this ability, we introduce the instruction induction challenge, compile a dataset consisting of 24 tasks, and define a novel evaluation metric based on executing the generated instruction. We discover that, to a large extent, the ability to generate instructions does indeed emerge when using a model that is both large enough and aligned to follow instructions; InstructGPT achieves 65.7{\%} of human performance in our execution-based metric, while the original GPT-3 model reaches only 9.8{\%} of human performance. This surprising result suggests that instruction induction might be a viable learning paradigm in and of itself, where instead of fitting a set of latent continuous parameters to the data, one searches for the best description in the natural language hypothesis space.},
  doi       = {10.18653/v1/2023.acl-long.108},
  group     = {-1_language_tasks_learning_task},
  groups    = {-1_training data_hierarchical classification_knowledge graph_named entity, ACL, 2023, Prompting, Generation},
  url       = {https://aclanthology.org/2023.acl-long.108}
}

@inproceedings{Xiang2023,
  author    = {Xiang, Wei and Liang, Chao and Wang, Bang},
  booktitle = {Findings of the Association for Computational Linguistics: ACL 2023},
  title     = {{TEP}rompt: Task Enlightenment Prompt Learning for Implicit Discourse Relation Recognition},
  year      = {2023},
  address   = {Toronto, Canada},
  editor    = {Rogers, Anna and Boyd-Graber, Jordan and Okazaki, Naoaki},
  month     = {jul},
  pages     = {12403--12414},
  publisher = {Association for Computational Linguistics},
  abstract  = {Implicit Discourse Relation Recognition (IDRR) aims at classifying the relation sense between two arguments without an explicit connective. Recently, the ConnPrompt (Xiang et al., 2022) has leveraged the powerful prompt learning for IDRR based on the fusion of multi-prompt decisions from three different yet much similar connective prediction templates. Instead of multi-prompt ensembling, we propose to design auxiliary tasks with enlightened prompt learning for the IDRR task. Although an auxiliary task is not used to directly output final prediction, we argue that during the joint training some of its learned features can be useful to boost the main task. In light of such motivations, we propose a task enlightenment prompt learning model, called TEPrompt, to fuse learned features from three related tasks for IDRR. In particular, the TEPrompt contains three tasks, viz., Discourse Relation Recognition (DRR), Sense Semantics Classification (SSC) and Annotated Connective Prediction (ACP), each with a unique prompt template and an answer space. In the training phase, we jointly train three prompt learning tasks with shared argument representation. In the testing phase, we only take the DRR output with fused features as the final IDRR decision. Experiments with the same conditions have shown that the proposed TEPrompt outperforms the ConnPrompt. This can be attributed to the promoted decision features and language models benefited from joint-training of auxiliary tasks.},
  doi       = {10.18653/v1/2023.findings-acl.785},
  group     = {-1_language_tasks_learning_task},
  groups    = {-1_training data_hierarchical classification_knowledge graph_named entity, 2023, ACL, Prompting},
  url       = {https://aclanthology.org/2023.findings-acl.785}
}

@inproceedings{Wang2023a,
  author    = {Wang, Yau-Shian and Chi, Ta-Chung and Zhang, Ruohong and Yang, Yiming},
  booktitle = {Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
  title     = {{PESCO}: Prompt-enhanced Self Contrastive Learning for Zero-shot Text Classification},
  year      = {2023},
  address   = {Toronto, Canada},
  editor    = {Rogers, Anna and Boyd-Graber, Jordan and Okazaki, Naoaki},
  month     = {jul},
  pages     = {14897--14911},
  publisher = {Association for Computational Linguistics},
  abstract  = {We present PESCO, a novel contrastive learning framework that substantially improves the performance of zero-shot text classification. We formulate text classification as a neural text retrieval problem where each document is treated as a query, and the system learns the mapping from each query to the relevant class labels by (1) adding prompts to enhance label retrieval, and (2) using retrieved labels to enrich the training set in a self-training loop of contrastive learning. PESCO achieves state-of-the-art performance on four benchmark text classification datasets. On DBpedia, we achieve 98.5{\%} accuracy without any labeled data, which is close to the fully-supervised result. Extensive experiments and analyses show all the components of PESCO are necessary for improving the performance of zero-shot text classification.},
  doi       = {10.18653/v1/2023.acl-long.832},
  group     = {-1_language_tasks_learning_task},
  groups    = {0_contrastive learning_active learning_prompt tuning_data augmentation, ACL, 2023},
  url       = {https://aclanthology.org/2023.acl-long.832}
}

@inproceedings{Pang2023,
  author    = {Pang, Bo and Yavuz, Semih and Xiong, Caiming and Zhou, Yingbo},
  booktitle = {Findings of the Association for Computational Linguistics: EACL 2023},
  title     = {{S}har{PT}: Shared Latent Space Prompt Tuning},
  year      = {2023},
  address   = {Dubrovnik, Croatia},
  editor    = {Vlachos, Andreas and Augenstein, Isabelle},
  month     = {may},
  pages     = {1244--1250},
  publisher = {Association for Computational Linguistics},
  abstract  = {Prompt tuning is an efficient method for adapting large language models, and Soft Prompt Transfer (SPoT) further narrows the gap between prompt tuning and full model tuning by transferring prompts learned from source tasks to target tasks. It is nevertheless difficult and expensive to identify the source task that provides optimal prompts. In this work, we propose to learn a shared latent space which captures a set of basis skills from a mixture of source tasks. Given an instance, its embedding queries the latent space, yielding a basis skill vector. This vector generates soft prompts, via a lightweight prompt generator, which modulates a frozen model. The latent space and prompt transformation are learned end-to-end by training on source tasks. Transfer learning from source tasks to a target task simply amounts to finetuning the prompt generator, accounting for roughly 0.3{\%} parameters of the frozen backbone model, while the shared latent space is also frozen in finetuning. Our approach outperforms prior soft prompt methods by a significant margin on a variety of tasks such as NLI, sentence completion, QA, conference resolution, word sense disambiguation. We also find, on various model scales, our method achieves competitive performance compared to finetuning the full model.},
  doi       = {10.18653/v1/2023.findings-eacl.92},
  group     = {-1_language_tasks_learning_task},
  groups    = {0_contrastive learning_active learning_prompt tuning_data augmentation, ACL, 2023, Tunning/Learning, Soft-Prompt},
  url       = {https://aclanthology.org/2023.findings-eacl.92}
}

@inproceedings{Rahamim2023,
  author    = {Rahamim, Adir and Uziel, Guy and Goldbraich, Esther and Anaby Tavor, Ateret},
  booktitle = {Findings of the Association for Computational Linguistics: ACL 2023},
  title     = {Text Augmentation Using Dataset Reconstruction for Low-Resource Classification},
  year      = {2023},
  address   = {Toronto, Canada},
  editor    = {Rogers, Anna and Boyd-Graber, Jordan and Okazaki, Naoaki},
  month     = {jul},
  pages     = {7389--7402},
  publisher = {Association for Computational Linguistics},
  abstract  = {In the deployment of real-world text classification models, label scarcity is a common problem and as the number of classes increases, this problem becomes even more complex. An approach to addressing this problem is by applying text augmentation methods. One of the more prominent methods involves using the text-generation capabilities of language models. In this paper, we propose Text AUgmentation by Dataset Reconstruction (TAU-DR), a novel method of data augmentation for text classification. We conduct experiments on several multi-class datasets, showing that our approach improves the current state-of-the-art techniques for data augmentation.},
  doi       = {10.18653/v1/2023.findings-acl.466},
  group     = {-1_language_tasks_learning_task},
  groups    = {0_contrastive learning_active learning_prompt tuning_data augmentation, ACL, 2023, Data Augmentation},
  url       = {https://aclanthology.org/2023.findings-acl.466}
}

@inproceedings{Wertz2023,
  author    = {Wertz, Lukas and Bogojeska, Jasmina and Mirylenka, Katsiaryna and Kuhn, Jonas},
  booktitle = {Findings of the Association for Computational Linguistics: ACL 2023},
  title     = {Reinforced Active Learning for Low-Resource, Domain-Specific, Multi-Label Text Classification},
  year      = {2023},
  address   = {Toronto, Canada},
  editor    = {Rogers, Anna and Boyd-Graber, Jordan and Okazaki, Naoaki},
  month     = {jul},
  pages     = {10959--10977},
  publisher = {Association for Computational Linguistics},
  abstract  = {Text classification datasets from specialised or technical domains are in high demand, especially in industrial applications. However, due to the high cost of annotation such datasets are usually expensive to create. While Active Learning (AL) can reduce the labeling cost, required AL strategies are often only tested on general knowledge domains and tend to use information sources that are not consistent across tasks. We propose Reinforced Active Learning (RAL) to train a Reinforcement Learning policy that utilizes many different aspects of the data and the task in order to select the most informative unlabeled subset dynamically over the course of the AL procedure. We demonstrate the superior performance of the proposed RAL framework compared to strong AL baselines across four intricate multi-class, multi-label text classification datasets taken from specialised domains. In addition, we experiment with a unique data augmentation approach to further reduce the number of samples RAL needs to annotate.},
  doi       = {10.18653/v1/2023.findings-acl.697},
  group     = {-1_language_tasks_learning_task},
  groups    = {0_contrastive learning_active learning_prompt tuning_data augmentation, ACL, 2023, Reinformencement Learning},
  url       = {https://aclanthology.org/2023.findings-acl.697}
}

'
@techreport{Tan2023,
  author     = {Tan, Bowen and Zhu, Yun and Liu, Lijuan and Xing, Eric and Hu, Zhiting and Chen, Jindong},
  title      = {Cappy: {Outperforming} and {Boosting} {Large} {Multi}-{Task} {LMs} with a {Small} {Scorer}},
  year       = {2023},
  month      = nov,
  note       = {arXiv:2311.06720 [cs] type: article},
  abstract   = {Large language models (LLMs) such as T0, FLAN, and OPT-IML, excel in multi-tasking under a unified instruction-following paradigm, where they also exhibit remarkable generalization abilities to unseen tasks. Despite their impressive performance, these LLMs, with sizes ranging from several billion to hundreds of billions of parameters, demand substantial computational resources, making their training and inference expensive and inefficient. Furthermore, adapting these models to downstream applications, particularly complex tasks, is often unfeasible due to the extensive hardware requirements for finetuning, even when utilizing parameter-efficient approaches such as prompt tuning. Additionally, the most powerful multi-task LLMs, such as OPT-IML-175B and FLAN-PaLM-540B, are not publicly accessible, severely limiting their customization potential. To address these challenges, we introduce a pretrained small scorer, Cappy, designed to enhance the performance and efficiency of multi-task LLMs. With merely 360 million parameters, Cappy functions either independently on classification tasks or serve as an auxiliary component for LLMs, boosting their performance. Moreover, Cappy enables efficiently integrating downstream supervision without requiring LLM finetuning nor the access to their parameters. Our experiments demonstrate that, when working independently on 11 language understanding tasks from PromptSource, Cappy outperforms LLMs that are several orders of magnitude larger. Besides, on 45 complex tasks from BIG-Bench, Cappy boosts the performance of the advanced multi-task LLM, FLAN-T5, by a large margin. Furthermore, Cappy is flexible to cooperate with other LLM adaptations, including finetuning and in-context learning, offering additional performance enhancement.},
  annote     = {Comment: In proceedings of NeurIPS 2023; Code and model available at https://github.com/tanyuqian/cappy and https://huggingface.co/btan2/cappy-large, respectively},
  file       = {:Tan2023 - Cappy_ Outperforming and Boosting Large Multi Task LMs with a Small Scorer.pdf:PDF},
  groups     = {Neurips},
  keywords   = {Computer Science - Machine Learning, Computer Science - Computation and Language},
  school     = {arXiv},
  shorttitle = {Cappy},
  url        = {http://arxiv.org/abs/2311.06720},
  urldate    = {2023-11-14}
}

'
@techreport{Zhang2023b,
  author     = {Zhang, Zhihan and Wang, Shuohang and Yu, Wenhao and Xu, Yichong and Iter, Dan and Zeng, Qingkai and Liu, Yang and Zhu, Chenguang and Jiang, Meng},
  title      = {Auto-{Instruct}: {Automatic} {Instruction} {Generation} and {Ranking} for {Black}-{Box} {Language} {Models}},
  year       = {2023},
  month      = oct,
  note       = {arXiv:2310.13127 [cs] type: article},
  abstract   = {Large language models (LLMs) can perform a wide range of tasks by following natural language instructions, without the necessity of task-specific fine-tuning. Unfortunately, the performance of LLMs is greatly influenced by the quality of these instructions, and manually writing effective instructions for each task is a laborious and subjective process. In this paper, we introduce Auto-Instruct, a novel method to automatically improve the quality of instructions provided to LLMs. Our method leverages the inherent generative ability of LLMs to produce diverse candidate instructions for a given task, and then ranks them using a scoring model trained on a variety of 575 existing NLP tasks. In experiments on 118 out-of-domain tasks, Auto-Instruct surpasses both human-written instructions and existing baselines of LLM-generated instructions. Furthermore, our method exhibits notable generalizability even with other LLMs that are not incorporated into its training process.},
  annote     = {Comment: Accepted to EMNLP 2023 Findings. Work was done before July 2023},
  file       = {:Zhang2023b - Auto Instruct_ Automatic Instruction Generation and Ranking for Black Box Language Models.pdf:PDF},
  groups     = {EMNLP},
  keywords   = {Computer Science - Computation and Language},
  school     = {arXiv},
  shorttitle = {Auto-{Instruct}},
  url        = {http://arxiv.org/abs/2310.13127},
  urldate    = {2023-11-14}
}

'
@techreport{Robinson2023,
  author   = {Robinson, Joshua and Rytting, Christopher Michael and Wingate, David},
  title    = {Leveraging {Large} {Language} {Models} for {Multiple} {Choice} {Question} {Answering}},
  year     = {2023},
  month    = mar,
  note     = {arXiv:2210.12353 [cs] type: article},
  abstract = {While large language models (LLMs) like GPT-3 have achieved impressive results on multiple choice question answering (MCQA) tasks in the zero, one, and few-shot settings, they generally lag behind the MCQA state of the art (SOTA). MCQA tasks have traditionally been presented to LLMs like cloze tasks. An LLM is conditioned on a question (without the associated answer options) and its chosen option is the one assigned the highest probability after normalization (for length, etc.). A more natural prompting approach is to present the question and answer options to the LLM jointly and have it output the symbol (e.g., "A") associated with its chosen answer option. This approach allows the model to explicitly compare answer options, reduces computational costs, and mitigates the effects of tokenization scheme and answer option representations on answer selection. For the natural approach to be effective, the LLM it is used with must be able to associate answer options with the symbols that represent them. The LLM needs what we term multiple choice symbol binding (MCSB) ability. This ability varies greatly by model. We show that a model with high MCSB ability performs much better with the natural approach than with the traditional approach across 20 diverse datasets and largely closes the gap with the SOTA, suggesting that the MCQA ability of LLMs has been previously underestimated.},
  annote   = {Comment: Accepted for ICLR 2023},
  file     = {:Robinson2023 - Leveraging Large Language Models for Multiple Choice Question Answering.pdf:PDF},
  groups   = {ICLR},
  keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning},
  school   = {arXiv},
  url      = {http://arxiv.org/abs/2210.12353},
  urldate  = {2023-11-15}
}

'
@inproceedings{Sorensen2022,
  author    = {Sorensen, Taylor and Robinson, Joshua and Rytting, Christopher and Shaw, Alexander and Rogers, Kyle and Delorey, Alexia and Khalil, Mahmoud and Fulda, Nancy and Wingate, David},
  booktitle = {Proceedings of the 60th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics} ({Volume} 1: {Long} {Papers})},
  title     = {An {Information}-theoretic {Approach} to {Prompt} {Engineering} {Without} {Ground} {Truth} {Labels}},
  year      = {2022},
  address   = {Dublin, Ireland},
  editor    = {Muresan, Smaranda and Nakov, Preslav and Villavicencio, Aline},
  month     = may,
  pages     = {819--862},
  publisher = {Association for Computational Linguistics},
  abstract  = {Pre-trained language models derive substantial linguistic and factual knowledge from the massive corpora on which they are trained, and prompt engineering seeks to align these models to specific tasks. Unfortunately, existing prompt engineering methods require significant amounts of labeled data, access to model parameters, or both. We introduce a new method for selecting prompt templates without labeled examples and without direct access to the model. Specifically, over a set of candidate templates, we choose the template that maximizes the mutual information between the input and the corresponding model output. Across 8 datasets representing 7 distinct NLP tasks, we show that when a template has high mutual information, it also has high accuracy on the task. On the largest model, selecting prompts with our method gets 90\% of the way from the average prompt accuracy to the best prompt accuracy and requires no ground truth labels.},
  doi       = {10.18653/v1/2022.acl-long.60},
  file      = {:Sorensen2022 - An Information Theoretic Approach to Prompt Engineering without Ground Truth Labels.pdf:PDF},
  url       = {https://aclanthology.org/2022.acl-long.60},
  urldate   = {2023-11-16}
}

'
@techreport{Mitra2023,
  author     = {Mitra, Arindam and Del Corro, Luciano and Mahajan, Shweti and Codas, Andres and Simoes, Clarisse and Agarwal, Sahaj and Chen, Xuxi and Razdaibiedina, Anastasia and Jones, Erik and Aggarwal, Kriti and Palangi, Hamid and Zheng, Guoqing and Rosset, Corby and Khanpour, Hamed and Awadallah, Ahmed},
  title      = {Orca 2: {Teaching} {Small} {Language} {Models} {How} to {Reason}},
  year       = {2023},
  month      = nov,
  note       = {arXiv:2311.11045 [cs] type: article},
  abstract   = {Orca 1 learns from rich signals, such as explanation traces, allowing it to outperform conventional instruction-tuned models on benchmarks like BigBench Hard and AGIEval. In Orca 2, we continue exploring how improved training signals can enhance smaller LMs' reasoning abilities. Research on training small LMs has often relied on imitation learning to replicate the output of more capable models. We contend that excessive emphasis on imitation may restrict the potential of smaller models. We seek to teach small LMs to employ different solution strategies for different tasks, potentially different from the one used by the larger model. For example, while larger models might provide a direct answer to a complex task, smaller models may not have the same capacity. In Orca 2, we teach the model various reasoning techniques (step-by-step, recall then generate, recall-reason-generate, direct answer, etc.). More crucially, we aim to help the model learn to determine the most effective solution strategy for each task. We evaluate Orca 2 using a comprehensive set of 15 diverse benchmarks (corresponding to approximately 100 tasks and over 36,000 unique prompts). Orca 2 significantly surpasses models of similar size and attains performance levels similar or better to those of models 5-10x larger, as assessed on complex tasks that test advanced reasoning abilities in zero-shot settings. make Orca 2 weights publicly available at aka.ms/orca-lm to support research on the development, evaluation, and alignment of smaller LMs},
  annote     = {Comment: Added url to model weights fixed typo in Author name},
  file       = {:Mitra2023 - Orca 2_ Teaching Small Language Models How to Reason.pdf:PDF},
  groups     = {ArXiv},
  keywords   = {Computer Science - Artificial Intelligence},
  school     = {arXiv},
  shorttitle = {Orca 2},
  url        = {http://arxiv.org/abs/2311.11045},
  urldate    = {2023-12-12}
}

@inproceedings{Zhang2021,
  author    = {Zhang, Jieyu and Yu, Yue and and Wang, Yujing and Yang, Yaming and Yang, Mao and Ratner, Alexander},
  booktitle = {Proceedings of the Neural Information Processing Systems Track on Datasets and Benchmarks},
  title     = {WRENCH: A Comprehensive Benchmark for Weak Supervision},
  year      = {2021},
  editor    = {J. Vanschoren and S. Yeung},
  publisher = {Curran},
  volume    = {1},
  url       = {https://datasets-benchmarks-proceedings.neurips.cc/paper_files/paper/2021/file/1c9ac0159c94d8d0cbedc973445af2da-Paper-round2.pdf}
}

@misc{Gunasekar2023,
  author   = {Gunasekar, Suriya and Zhang, Yi and Aneja, Jyoti and Cesar, Caio and Mendes, Teodoro and Giorno, Allie Del and Gopi, Sivakanth and Javaheripi, Mojan and Kauffmann, Piero and de Rosa, Gustavo and Saarikivi, Olli and Salim, Adil and Shah, Shital and Singh Behl, Harkirat and Wang, Xin and Bubeck, Sébastien and Eldan, Ronen and Kalai, Adam Tauman and Lee, Yin Tat and Li, Yuanzhi},
  month    = {June},
  title    = {Textbooks Are All You Need},
  year     = {2023},
  abstract = {We introduce phi-1, a new large language model for code, with significantly smaller size than competing models: phi-1 is a Transformer-based model with 1.3B parameters, trained for 4 days on 8 A100s, using a selection of ``textbook quality" data from the web (6B tokens) and synthetically generated textbooks and exercises with GPT-3.5 (1B tokens). Despite this small scale, phi-1 attains pass@1 accuracy 50.6% on HumanEval and 55.5% on MBPP. It also displays surprising emergent properties compared to phi-1-base, our model before our finetuning stage on a dataset of coding exercises, and phi-1-small, a smaller model with 350M parameters trained with the same pipeline as phi-1 that still achieves 45% on HumanEval.},
  url      = {https://www.microsoft.com/en-us/research/publication/textbooks-are-all-you-need/}
}

@inproceedings{Laperriere2022,
  title     = {The Spoken Language Understanding {MEDIA} Benchmark Dataset in the Era of Deep Learning: data updates, training and evaluation tools},
  author    = {Laperri{\`e}re, Ga{\"e}lle  and
               Pelloin, Valentin  and
               Caubri{\`e}re, Antoine  and
               Mdhaffar, Salima  and
               Camelin, Nathalie  and
               Ghannay, Sahar  and
               Jabaian, Bassam  and
               Est{\`e}ve, Yannick},
  editor    = {Calzolari, Nicoletta  and
               B{\'e}chet, Fr{\'e}d{\'e}ric  and
               Blache, Philippe  and
               Choukri, Khalid  and
               Cieri, Christopher  and
               Declerck, Thierry  and
               Goggi, Sara  and
               Isahara, Hitoshi  and
               Maegaard, Bente  and
               Mariani, Joseph  and
               Mazo, H{\'e}l{\`e}ne  and
               Odijk, Jan  and
               Piperidis, Stelios},
  booktitle = {Proceedings of the Thirteenth Language Resources and Evaluation Conference},
  month     = jun,
  year      = {2022},
  address   = {Marseille, France},
  publisher = {European Language Resources Association},
  url       = {https://aclanthology.org/2022.lrec-1.171},
  pages     = {1595--1602}
}


@inproceedings{Schick2021,
  author    = {Schick, Timo and Schütze, Hinrich},
  booktitle = {Proceedings of the 16th {Conference} of the {European} {Chapter} of the {Association} for {Computational} {Linguistics}: {Main} {Volume}},
  title     = {Exploiting {Cloze}-{Questions} for {Few}-{Shot} {Text} {Classification} and {Natural} {Language} {Inference}},
  year      = {2021},
  address   = {Online},
  editor    = {Merlo, Paola and Tiedemann, Jorg and Tsarfaty, Reut},
  month     = apr,
  pages     = {255--269},
  publisher = {Association for Computational Linguistics},
  abstract  = {Some NLP tasks can be solved in a fully unsupervised fashion by providing a pretrained language model with “task descriptions” in natural language (e.g., Radford et al., 2019). While this approach underperforms its supervised counterpart, we show in this work that the two ideas can be combined: We introduce Pattern-Exploiting Training (PET), a semi-supervised training procedure that reformulates input examples as cloze-style phrases to help language models understand a given task. These phrases are then used to assign soft labels to a large set of unlabeled examples. Finally, standard supervised training is performed on the resulting training set. For several tasks and languages, PET outperforms supervised training and strong semi-supervised approaches in low-resource settings by a large margin.},
  doi       = {10.18653/v1/2021.eacl-main.20},
  file      = {:Schick2021 - Exploiting Cloze Questions for Few Shot Text Classification and Natural Language Inference.pdf:PDF},
  url       = {https://aclanthology.org/2021.eacl-main.20},
  urldate   = {2024-02-23}
}

@Comment{jabref-meta: databaseType:bibtex;}

@Comment{jabref-meta: grouping:
0 AllEntriesGroup:;
1 StaticGroup:ACL\;0\;1\;0x99cc99ff\;\;\;;
2 StaticGroup:2020\;0\;1\;0x8a8a8aff\;\;\;;
2 StaticGroup:2021\;0\;1\;0x8a8a8aff\;\;\;;
2 StaticGroup:2023\;0\;1\;0x8a8a8aff\;\;\;;
1 StaticGroup:ArXiv\;0\;1\;0xffff99ff\;\;\;;
1 StaticGroup:Data Augmentation\;0\;1\;0x8a8a8aff\;\;\;;
1 StaticGroup:Datasets 2023\;0\;1\;0x99b3ffff\;\;\;;
1 StaticGroup:EMNLP\;0\;0\;0x669999ff\;\;\;;
1 StaticGroup:Generation\;0\;1\;0x8a8a8aff\;\;\;;
1 StaticGroup:ICLR\;0\;1\;0x8a8a8aff\;\;\;;
1 StaticGroup:ICML 2023\;0\;0\;0xff8080ff\;\;\;;
1 StaticGroup:Metrics\;0\;0\;0x8a8a8aff\;\;\;;
1 StaticGroup:Multitask Learning\;0\;0\;0x8a8a8aff\;\;\;;
1 StaticGroup:NER\;0\;1\;0xff00ffff\;\;\;;
1 StaticGroup:Neurips\;0\;1\;0x800000ff\;\;\;;
1 StaticGroup:PEFT\;0\;1\;0x8a8a8aff\;\;\;;
1 StaticGroup:Prompting\;0\;0\;0x8a8a8aff\;\;\;;
1 StaticGroup:Reasoning\;0\;0\;0xccffffff\;\;\;;
1 StaticGroup:Reinformencement Learning\;0\;1\;0x8a8a8aff\;\;\;;
1 StaticGroup:Representation\;0\;1\;0x8a8a8aff\;\;\;;
1 StaticGroup:Tunning/Learning\;0\;1\;0x8a8a8aff\;\;\;;
2 StaticGroup:Instructions\;0\;1\;0x8a8a8aff\;\;\;;
2 StaticGroup:Soft-Prompt\;0\;1\;0x8a8a8aff\;\;\;;
1 StaticGroup:Weak Supervision\;0\;1\;0x8a8a8aff\;\;\;;
}
