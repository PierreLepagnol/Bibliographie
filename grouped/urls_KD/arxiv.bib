@misc{Dai_2023_2302.13007v3,
 author = {Zhengliang Liu and Wenxiong Liao and Xiaoke Huang and Yihan Cao and Zihao Wu and Lin Zhao and Shaochen Xu and Wei Liu and Ninghao Liu and Sheng Li and Dajiang Zhu and Hongmin Cai and Lichao Sun and Quanzheng Li and Dinggang Shen and Tianming Liu and Xiang Li},
 eprint = {2302.13007v3},
 title = {AugGPT: Leveraging ChatGPT for Text Data Augmentation},
 url = {http://www.arxiv.org/abs/2302.13007v3},
 year = {2023}
}

@misc{Gao_2023_2205.12679v2,
 author = {Renjie Pi and Yong Lin and Hang Xu and Jiacheng Ye and Zhiyong Wu and Weizhong Zhang and Xiaodan Liang and Zhenguo Li and Lingpeng Kong},
 eprint = {2205.12679v2},
 title = {Self-Guided Noise-Free Data Generation for Efficient Zero-Shot Learning},
 url = {http://www.arxiv.org/abs/2205.12679v2},
 year = {2023}
}

@misc{Gu_2024_2306.08543v4,
 author = {Li Dong and Furu Wei and Minlie Huang},
 eprint = {2306.08543v4},
 title = {MiniLLM: Knowledge Distillation of Large Language Models},
 url = {http://www.arxiv.org/abs/2306.08543v4},
 year = {2024}
}

@misc{Hsieh_2023_2305.02301v2,
 author = {Chun-Liang Li and Chih-Kuan Yeh and Hootan Nakhost and Yasuhisa Fujii and Alexander Ratner and Ranjay Krishna and Chen-Yu Lee and Tomas Pfister},
 eprint = {2305.02301v2},
 title = {Distilling Step-by-Step! Outperforming Larger Language Models with Less Training Data and Smaller Model Sizes},
 url = {http://www.arxiv.org/abs/2305.02301v2},
 year = {2023}
}

@misc{Kang_2024_2407.13952v1,
 author = {},
 eprint = {2407.13952v1},
 title = {Knowledge Distillation Approaches for Accurate and Efficient Recommender System},
 url = {http://www.arxiv.org/abs/2407.13952v1},
 year = {2024}
}

@misc{Li_2024_2406.11285v1,
 author = {Yi Liu and Chongyang Liu and Xiaoning Ren and Ling Shi and Weisong Sun and Yinxing Xue},
 eprint = {2406.11285v1},
 title = {Self and Cross-Model Distillation for LLMs: Effective Methods for Refusal Pattern Alignment},
 url = {http://www.arxiv.org/abs/2406.11285v1},
 year = {2024}
}

@misc{Loo_2024_2409.20449v2,
 author = {Fotis Iliopoulos and Wei Hu and Erik Vee},
 eprint = {2409.20449v2},
 title = {Linear Projections of Teacher Embeddings for Few-Class Distillation},
 url = {http://www.arxiv.org/abs/2409.20449v2},
 year = {2024}
}

@misc{Mohammadshahi_2024_2410.08407v1,
 author = {Yani Ioannou},
 eprint = {2410.08407v1},
 title = {What is Left After Distillation? How Knowledge Transfer Impacts Fairness and Bias},
 url = {http://www.arxiv.org/abs/2410.08407v1},
 year = {2024}
}

@misc{Muralidharan_2024_2407.14679v1,
 author = {Sharath Turuvekere Sreenivas and Raviraj Joshi and Marcin Chochowski and Mostofa Patwary and Mohammad Shoeybi and Bryan Catanzaro and Jan Kautz and Pavlo Molchanov},
 eprint = {2407.14679v1},
 title = {Compact Language Models via Pruning and Knowledge Distillation},
 url = {http://www.arxiv.org/abs/2407.14679v1},
 year = {2024}
}

@misc{Pareek_2024_2407.04600v1,
 author = {Simon S. Du and Sewoong Oh},
 eprint = {2407.04600v1},
 title = {Understanding the Gains from Repeated Self-Distillation},
 url = {http://www.arxiv.org/abs/2407.04600v1},
 year = {2024}
}

@misc{Schmidt_2024_2406.12739v1,
 author = {Philipp Borchert and Ivan Vulić and Goran Glavaš},
 eprint = {2406.12739v1},
 title = {Self-Distillation for Model Stacking Unlocks Cross-Lingual NLU in 200+ Languages},
 url = {http://www.arxiv.org/abs/2406.12739v1},
 year = {2024}
}

@misc{Sreenivas_2024_2408.11796v2,
 author = {Saurav Muralidharan and Raviraj Joshi and Marcin Chochowski and Mostofa Patwary and Mohammad Shoeybi and Bryan Catanzaro and Jan Kautz and Pavlo Molchanov},
 eprint = {2408.11796v2},
 title = {LLM Pruning and Distillation in Practice: The Minitron Approach},
 url = {http://www.arxiv.org/abs/2408.11796v2},
 year = {2024}
}

@misc{Ubani_2023_2304.14334v1,
 author = {Suleyman Olcay Polat and Rodney Nielsen},
 eprint = {2304.14334v1},
 title = {ZeroShotDataAug: Generating and Augmenting Training Data with ChatGPT},
 url = {http://www.arxiv.org/abs/2304.14334v1},
 year = {2023}
}

@misc{Xu_2024_2402.13116v4,
 author = {Ming Li and Chongyang Tao and Tao Shen and Reynold Cheng and Jinyang Li and Can Xu and Dacheng Tao and Tianyi Zhou},
 eprint = {2402.13116v4},
 title = {A Survey on Knowledge Distillation of Large Language Models},
 url = {http://www.arxiv.org/abs/2402.13116v4},
 year = {2024}
}

@misc{Yang_2024_2407.01885v1,
 author = {Wang Lu and Yao Zhu and Yidong Wang and Qian Chen and Chenlong Gao and Bingjie Yan and Yiqiang Chen},
 eprint = {2407.01885v1},
 title = {Survey on Knowledge Distillation for Large Language Models: Methods, Evaluation, and Application},
 url = {http://www.arxiv.org/abs/2407.01885v1},
 year = {2024}
}

@misc{Zhang_2024_2407.13911v1,
 author = {Yunhui Guo and Yu Xiang},
 eprint = {2407.13911v1},
 title = {Continual Distillation Learning},
 url = {http://www.arxiv.org/abs/2407.13911v1},
 year = {2024}
}
