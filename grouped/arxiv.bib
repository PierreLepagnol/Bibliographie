@misc{Gao_2023_2205.12679v2,
 author = {Renjie Pi and Yong Lin and Hang Xu and Jiacheng Ye and Zhiyong Wu and Weizhong Zhang and Xiaodan Liang and Zhenguo Li and Lingpeng Kong},
 eprint = {2205.12679v2},
 title = {Self-Guided Noise-Free Data Generation for Efficient Zero-Shot Learning},
 url = {http://www.arxiv.org/abs/2205.12679v2},
 year = {2023}
}

@misc{Kang_2024_2407.13952v1,
 author = {},
 eprint = {2407.13952v1},
 title = {Knowledge Distillation Approaches for Accurate and Efficient Recommender System},
 url = {http://www.arxiv.org/abs/2407.13952v1},
 year = {2024}
}

@misc{Li_2024_2406.11285v1,
 author = {Yi Liu and Chongyang Liu and Xiaoning Ren and Ling Shi and Weisong Sun and Yinxing Xue},
 eprint = {2406.11285v1},
 title = {Self and Cross-Model Distillation for LLMs: Effective Methods for Refusal Pattern Alignment},
 url = {http://www.arxiv.org/abs/2406.11285v1},
 year = {2024}
}

@misc{Mohammadshahi_2024_2410.08407v1,
 author = {Yani Ioannou},
 eprint = {2410.08407v1},
 title = {What is Left After Distillation? How Knowledge Transfer Impacts Fairness and Bias},
 url = {http://www.arxiv.org/abs/2410.08407v1},
 year = {2024}
}

@misc{Muralidharan_2024_2407.14679v1,
 author = {Sharath Turuvekere Sreenivas and Raviraj Joshi and Marcin Chochowski and Mostofa Patwary and Mohammad Shoeybi and Bryan Catanzaro and Jan Kautz and Pavlo Molchanov},
 eprint = {2407.14679v1},
 title = {Compact Language Models via Pruning and Knowledge Distillation},
 url = {http://www.arxiv.org/abs/2407.14679v1},
 year = {2024}
}

@misc{Pareek_2024_2407.04600v1,
 author = {Simon S. Du and Sewoong Oh},
 eprint = {2407.04600v1},
 title = {Understanding the Gains from Repeated Self-Distillation},
 url = {http://www.arxiv.org/abs/2407.04600v1},
 year = {2024}
}

@misc{Schmidt_2024_2406.12739v1,
 author = {Philipp Borchert and Ivan Vulić and Goran Glavaš},
 eprint = {2406.12739v1},
 title = {Self-Distillation for Model Stacking Unlocks Cross-Lingual NLU in 200+ Languages},
 url = {http://www.arxiv.org/abs/2406.12739v1},
 year = {2024}
}

@misc{Xu_2024_2402.13116v4,
 author = {Ming Li and Chongyang Tao and Tao Shen and Reynold Cheng and Jinyang Li and Can Xu and Dacheng Tao and Tianyi Zhou},
 eprint = {2402.13116v4},
 title = {A Survey on Knowledge Distillation of Large Language Models},
 url = {http://www.arxiv.org/abs/2402.13116v4},
 year = {2024}
}

@misc{Yang_2024_2407.01885v1,
 author = {Wang Lu and Yao Zhu and Yidong Wang and Qian Chen and Chenlong Gao and Bingjie Yan and Yiqiang Chen},
 eprint = {2407.01885v1},
 title = {Survey on Knowledge Distillation for Large Language Models: Methods, Evaluation, and Application},
 url = {http://www.arxiv.org/abs/2407.01885v1},
 year = {2024}
}

@misc{Zhang_2024_2407.13911v1,
 author = {Yunhui Guo and Yu Xiang},
 eprint = {2407.13911v1},
 title = {Continual Distillation Learning},
 url = {http://www.arxiv.org/abs/2407.13911v1},
 year = {2024}
}
