J'aimerais travailler sur : 
- **Distillation** (Génération-Apprentissage)

Distillation efficace de Petit LLM pour une inférence rapide et efficace  

A partir d'un exemple par classe générer des données synthétiques pour distiller un Petit LLM dans un tout petit BERT.

<!-- Distilling SLM into for fast, cheap,  efficient specialized models.-->



But sélectionner les meilleures instructions pour faire de l'augmentation de données.
 1. Generation d'instruction ==> Pool d'instructions d'augmentation de données
 2. Pool ==> Selection des instructions avec modèle de reward.==> Pool* d'instruction
 3. Pool* + données labélisée ==> Augmentation de données labélisée

Motivations
On n'a pas toujours 

Hypothèses

Expériences



Est-ce possible de créer des données synthétiques avec des petits modèles de langues sans aucun entraînement ? 

----
How Small Can Language Models Be and Still Be a Good Teacher ?




----
is Reward Models a good Zero-shot instruction ranker ?

But : Produire des nouvelles données d'apprentissage
Problème : Avoir des instructions d'augmentation de bonne qualité

1. Création des instructions d'augmentations manuellement est faisalbe mais pas scalable et peu consistent (Toute les instruction d'augmentation ne marche pas pour tout les problèmes)
2. 



---
But : Produire des nouvelles données d'apprentissage
Problème : Avoir des instructions d'augmentation de bonne qualité

1. Création des instructions d'augmentations manuellement est faisalbe mais pas scalable et peu consistent (Toute les instruction d'augmentation ne marche pas pour tout les problèmes)
2. 


