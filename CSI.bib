@misc{chen2019bert,
      title={BERT for Joint Intent Classification and Slot Filling}, 
      author={Qian Chen and Zhu Zhuo and Wen Wang},
      year={2019},
      eprint={1902.10909},
      archivePrefix={arXiv},
      primaryClass={id='cs.CL' full_name='Computation and Language' is_active=True alt_name='cmp-lg' in_archive='cs' is_general=False description='Covers natural language processing. Roughly includes material in ACM Subject Class I.2.7. Note that work on artificial languages (programming languages, logics, formal systems) that does not explicitly address natural-language issues broadly construed (natural-language processing, computational linguistics, speech, text retrieval, etc.) is not appropriate for this area.'}
}

@inproceedings{lepagnol-etal-2024-small-language,
  title     = {Small Language Models Are Good Too: An Empirical Study of Zero-Shot Classification},
  author    = {Lepagnol, Pierre  and
               Gerald, Thomas  and
               Ghannay, Sahar  and
               Servan, Christophe  and
               Rosset, Sophie},
  editor    = {Calzolari, Nicoletta  and
               Kan, Min-Yen  and
               Hoste, Veronique  and
               Lenci, Alessandro  and
               Sakti, Sakriani  and
               Xue, Nianwen},
  booktitle = {Proceedings of the 2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation (LREC-COLING 2024)},
  month     = may,
  year      = {2024},
  address   = {Torino, Italia},
  publisher = {ELRA and ICCL},
  url       = {https://aclanthology.org/2024.lrec-main.1299},
  pages     = {14923--14936},
  abstract  = {This study is part of the debate on the efficiency of large versus small language models for text classification by prompting. We assess the performance of small language models in zero-shot text classification, challenging the prevailing dominance of large models. Across 15 datasets, our investigation benchmarks language models from 77M to 40B parameters using different architectures and scoring functions. Our findings reveal that small models can effectively classify texts, getting on par with or surpassing their larger counterparts. We developed and shared a comprehensive open-source repository that encapsulates our methodologies. This research underscores the notion that bigger isn{'}t always better, suggesting that resource-efficient small models may offer viable solutions for specific data classification challenges.}
}

@article{Song2012,
  author   = {Song, Lin
              and Langfelder, Peter
              and Horvath, Steve},
  title    = {Comparison of co-expression measures: mutual information, correlation, and model based indices},
  journal  = {BMC Bioinformatics},
  year     = {2012},
  month    = {Dec},
  day      = {09},
  volume   = {13},
  number   = {1},
  pages    = {328},
  abstract = {Co-expression measures are often used to define networks among genes. Mutual information (MI) is often used as a generalized correlation measure. It is not clear how much MI adds beyond standard (robust) correlation measures or regression model based association measures. Further, it is important to assess what transformations of these and other co-expression measures lead to biologically meaningful modules (clusters of genes).},
  issn     = {1471-2105},
  doi      = {10.1186/1471-2105-13-328},
  url      = {https://doi.org/10.1186/1471-2105-13-328}
}



@book{pml1Book,
  author    = {Kevin P. Murphy},
  title     = {Probabilistic Machine Learning: An introduction},
  publisher = {MIT Press},
  year      = 2022,
  url       = {probml.ai}
}

'
@techreport{He2023,
  author     = {He, Mutian and Garner, Philip N.},
  title      = {Can {ChatGPT} {Detect} {Intent}? {Evaluating} {Large} {Language} {Models} for {Spoken} {Language} {Understanding}},
  year       = {2023},
  month      = aug,
  note       = {arXiv:2305.13512 [cs, eess] type: article},
  abstract   = {Recently, large pretrained language models have demonstrated strong language understanding capabilities. This is particularly reflected in their zero-shot and in-context learning abilities on downstream tasks through prompting. To assess their impact on spoken language understanding (SLU), we evaluate several such models like ChatGPT and OPT of different sizes on multiple benchmarks. We verify the emergent ability unique to the largest models as they can reach intent classification accuracy close to that of supervised models with zero or few shots on various languages given oracle transcripts. By contrast, the results for smaller models fitting a single GPU fall far behind. We note that the error cases often arise from the annotation scheme of the dataset; responses from ChatGPT are still reasonable. We show, however, that the model is worse at slot filling, and its performance is sensitive to ASR errors, suggesting serious challenges for the application of those textual models on SLU.},
  annote     = {Comment: 6 pages, 2 figures; Accepted by Interspeech 2023},
  file       = {:he_can_2023 - Can ChatGPT Detect Intent_ Evaluating Large Language Models for Spoken Language Understanding.pdf:PDF},
  groups     = {AttempsSLU},
  keywords   = {Computer Science - Computation and Language, Computer Science - Artificial Intelligence, Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing},
  school     = {arXiv},
  shorttitle = {Can {ChatGPT} {Detect} {Intent}?},
  url        = {http://arxiv.org/abs/2305.13512},
  urldate    = {2024-04-10}
}

@inproceedings{Shen2023_promptner,
  author    = {Shen, Yongliang and Tan, Zeqi and Wu, Shuhui and Zhang, Wenqi and Zhang, Rongsheng and Xi, Yadong and Lu, Weiming and Zhuang, Yueting},
  booktitle = {Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
  title     = {{P}rompt{NER}: Prompt Locating and Typing for Named Entity Recognition},
  year      = {2023},
  address   = {Toronto, Canada},
  editor    = {Rogers, Anna and Boyd-Graber, Jordan and Okazaki, Naoaki},
  month     = jul,
  pages     = {12492--12507},
  publisher = {Association for Computational Linguistics},
  abstract  = {Prompt learning is a new paradigm for utilizing pre-trained language models and has achieved great success in many tasks. To adopt prompt learning in the NER task, two kinds of methods have been explored from a pair of symmetric perspectives, populating the template by enumerating spans to predict their entity types or constructing type-specific prompts to locate entities. However, these methods not only require a multi-round prompting manner with a high time overhead and computational cost, but also require elaborate prompt templates, that are difficult to apply in practical scenarios. In this paper, we unify entity locating and entity typing into prompt learning, and design a dual-slot multi-prompt template with the position slot and type slot to prompt locating and typing respectively. Multiple prompts can be input to the model simultaneously, and then the model extracts all entities by parallel predictions on the slots. To assign labels for the slots during training, we design a dynamic template filling mechanism that uses the extended bipartite graph matching between prompts and the ground-truth entities. We conduct experiments in various settings, including resource-rich flat and nested NER datasets and low-resource in-domain and cross-domain datasets. Experimental results show that the proposed model achieves a significant performance improvement, especially in the cross-domain few-shot setting, which outperforms the state-of-the-art model by +7.7{\%} on average.},
  doi       = {10.18653/v1/2023.acl-long.698},
  groups    = {Constrained Prompting},
  url       = {https://aclanthology.org/2023.acl-long.698}
}

@article{Ye2023,
  author         = {Ye, Feiyang and Huang, Liang and Liang, Senjie and Chi, KaiKai},
  journal        = {Information},
  title          = {Decomposed Two-Stage Prompt Learning for Few-Shot Named Entity Recognition},
  year           = {2023},
  issn           = {2078-2489},
  number         = {5},
  volume         = {14},
  abstract       = {Named entity recognition (NER) in a few-shot setting is an extremely challenging task, and most existing methods fail to account for the gap between NER tasks and pre-trained language models. Although prompt learning has been successfully applied in few-shot classification tasks, adapting to token-level classification similar to the NER task presents challenges in terms of time consumption and efficiency. In this work, we propose a decomposed prompt learning NER framework for few-shot settings, decomposing the NER task into two stages: entity locating and entity typing. In training, the location information of distant labels is used to train the entity locating model. A concise but effective prompt template is built to train the entity typing model. In inference, a pipeline approach is used to handle the entire NER task, which elegantly resolves time-consuming and inefficient problems. Specifically, a well-trained entity locating model is used to predict entity spans for each input. The input is then transformed using prompt templates, and the well-trained entity typing model is used to predict their types in a single step. Experimental results demonstrate that our framework outperforms previous prompt-based methods by an average of 2.3–12.9% in F1 score while achieving the best trade-off between accuracy and inference speed.},
  article-number = {262},
  doi            = {10.3390/info14050262},
  groups         = {Constrained Prompting},
  url            = {https://www.mdpi.com/2078-2489/14/5/262}
}

@inproceedings{Schick2021_smallLM,
  author    = {Schick, Timo and Sch{\"u}tze, Hinrich},
  booktitle = {Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies},
  title     = {It{'}s Not Just Size That Matters: Small Language Models Are Also Few-Shot Learners},
  year      = {2021},
  address   = {Online},
  editor    = {Toutanova, Kristina and Rumshisky, Anna and Zettlemoyer, Luke and Hakkani-Tur, Dilek and Beltagy, Iz and Bethard, Steven and Cotterell, Ryan and Chakraborty, Tanmoy and Zhou, Yichao},
  month     = jun,
  pages     = {2339--2352},
  publisher = {Association for Computational Linguistics},
  abstract  = {When scaled to hundreds of billions of parameters, pretrained language models such as GPT-3 (Brown et al., 2020) achieve remarkable few-shot performance. However, enormous amounts of compute are required for training and applying such big models, resulting in a large carbon footprint and making it difficult for researchers and practitioners to use them. We show that performance similar to GPT-3 can be obtained with language models that are much {``}greener{''} in that their parameter count is several orders of magnitude smaller. This is achieved by converting textual inputs into cloze questions that contain a task description, combined with gradient-based optimization; exploiting unlabeled data gives further improvements. We identify key factors required for successful natural language understanding with small language models.},
  doi       = {10.18653/v1/2021.naacl-main.185},
  groups    = {Constrained Prompting},
  url       = {https://aclanthology.org/2021.naacl-main.185}
}

@article{Liu2023_suervey_isk,
  author     = {Liu, Pengfei and Yuan, Weizhe and Fu, Jinlan and Jiang, Zhengbao and Hayashi, Hiroaki and Neubig, Graham},
  journal    = {ACM Comput. Surv.},
  title      = {Pre-train, Prompt, and Predict: A Systematic Survey of Prompting Methods in Natural Language Processing},
  year       = {2023},
  issn       = {0360-0300},
  month      = {jan},
  number     = {9},
  volume     = {55},
  abstract   = {This article surveys and organizes research works in a new paradigm in natural language processing, which we dub “prompt-based learning.” Unlike traditional supervised learning, which trains a model to take in an input x and predict an output y as P(y|x), prompt-based learning is based on language models that model the probability of text directly. To use these models to perform prediction tasks, the original input x is modified using a template into a textual string prompt x′ that has some unfilled slots, and then the language model is used to probabilistically fill the unfilled information to obtain a final string x̂, from which the final output y can be derived. This framework is powerful and attractive for a number of reasons: It allows the language model to be pre-trained on massive amounts of raw text, and by defining a new prompting function the model is able to perform few-shot or even zero-shot learning, adapting to new scenarios with few or no labeled data. In this article, we introduce the basics of this promising paradigm, describe a unified set of mathematical notations that can cover a wide variety of existing work, and organize existing work along several dimensions, e.g.,&nbsp;the choice of pre-trained language models, prompts, and tuning strategies. To make the field more accessible to interested beginners, we not only make a systematic review of existing works and a highly structured typology of prompt-based concepts but also release other resources, e.g., a website  including constantly updated survey and paperlist.},
  address    = {New York, NY, USA},
  articleno  = {195},
  doi        = {10.1145/3560815},
  groups     = {General LLM-Prompting},
  issue_date = {September 2023},
  keywords   = {Pre-trained language models, prompting},
  numpages   = {35},
  publisher  = {Association for Computing Machinery},
  url        = {https://doi.org/10.1145/3560815}
}

@misc{Qin2021,
  author        = {Libo Qin and Tianbao Xie and Wanxiang Che and Ting Liu},
  title         = {A Survey on Spoken Language Understanding: Recent Advances and New Frontiers},
  year          = {2021},
  archiveprefix = {arXiv},
  eprint        = {2103.03095},
  primaryclass  = {cs.CL}
}

@misc{Yin2024,
  author        = {Shangjian Yin and Peijie Huang and Yuhong Xu and Haojing Huang and Jiatian Chen},
  title         = {Do Large Language Model Understand Multi-Intent Spoken Language ?},
  year          = {2024},
  archiveprefix = {arXiv},
  eprint        = {2403.04481},
  groups        = {AttempsSLU},
  primaryclass  = {cs.CL}
}

'
@techreport{Wang2023a,
  author     = {Wang, Shuhe and Sun, Xiaofei and Li, Xiaoya and Ouyang, Rongbin and Wu, Fei and Zhang, Tianwei and Li, Jiwei and Wang, Guoyin},
  title      = {{GPT}-{NER}: {Named} {Entity} {Recognition} via {Large} {Language} {Models}},
  year       = {2023},
  month      = oct,
  note       = {arXiv:2304.10428 [cs] type: article},
  abstract   = {Despite the fact that large-scale Language Models (LLM) have achieved SOTA performances on a variety of NLP tasks, its performance on NER is still significantly below supervised baselines. This is due to the gap between the two tasks the NER and LLMs: the former is a sequence labeling task in nature while the latter is a text-generation model. In this paper, we propose GPT-NER to resolve this issue. GPT-NER bridges the gap by transforming the sequence labeling task to a generation task that can be easily adapted by LLMs e.g., the task of finding location entities in the input text "Columbus is a city" is transformed to generate the text sequence "@@Columbus\#\# is a city", where special tokens @@\#\# marks the entity to extract. To efficiently address the "hallucination" issue of LLMs, where LLMs have a strong inclination to over-confidently label NULL inputs as entities, we propose a self-verification strategy by prompting LLMs to ask itself whether the extracted entities belong to a labeled entity tag. We conduct experiments on five widely adopted NER datasets, and GPT-NER achieves comparable performances to fully supervised baselines, which is the first time as far as we are concerned. More importantly, we find that GPT-NER exhibits a greater ability in the low-resource and few-shot setups, when the amount of training data is extremely scarce, GPT-NER performs significantly better than supervised models. This demonstrates the capabilities of GPT-NER in real-world NER applications where the number of labeled examples is limited.},
  file       = {:Wang2023a - GPT NER_ Named Entity Recognition Via Large Language Models.pdf:PDF},
  groups     = {Tagging Prompting},
  keywords   = {Computer Science - Computation and Language},
  school     = {arXiv},
  shorttitle = {{GPT}-{NER}},
  url        = {http://arxiv.org/abs/2304.10428},
  urldate    = {2024-04-10}
}



@inproceedings{boulanger-lavergne-and-sophie-rosset-2022-generating,
  title     = {Generating unlabelled data for a tri-training approach in a low resourced {NER} task},
  author    = {Boulanger, Hugo  and
               Lavergne, Thomas  and
               Rosset, Sophie},
  booktitle = {Proceedings of the Third Workshop on Deep Learning for Low-Resource Natural Language Processing},
  month     = jul,
  year      = {2022},
  address   = {Hybrid},
  publisher = {Association for Computational Linguistics},
  url       = {https://aclanthology.org/2022.deeplo-1.4},
  doi       = {10.18653/v1/2022.deeplo-1.4},
  pages     = {30--37},
  abstract  = {t}
}

@misc{wen_active_2022,
  title     = {Active {Self}-{Semi}-{Supervised} {Learning} for {Few} {Labeled} {Samples} {Fast} {Training}},
  url       = {http://arxiv.org/abs/2203.04560},
  abstract  = {Faster training and fewer annotations are two key issues for applying deep models to various practical domains. Now, semi-supervised learning has achieved great success in training with few annotations. However, low-quality labeled samples produced by random sampling make it difficult to continue to reduce the number of annotations. In this paper we propose an active self-semi-supervised training framework that bootstraps semi-supervised models with good prior pseudo-labels, where the priors are obtained by label propagation over self-supervised features. Because the accuracy of the prior is not only affected by the quality of features, but also by the selection of the labeled samples. We develop active learning and label propagation strategies to obtain better prior pseudo-labels. Consequently, our framework can greatly improve the performance of models with few annotations and greatly reduce the training time. Experiments on three semi-supervised learning benchmarks demonstrate effectiveness. Our method achieves similar accuracy to standard semi-supervised approaches in about 1/3 of the training time, and even outperform them when fewer annotations are available (84.10{\textbackslash}\% in CIFAR-10 with 10 labels).},
  urldate   = {2022-07-23},
  publisher = {arXiv},
  author    = {Wen, Ziting and Pizarro, Oscar and Williams, Stefan},
  month     = mar,
  year      = {2022},
  note      = {arXiv:2203.04560 [cs]},
  keywords  = {Computer Science - Computer Vision and Pattern Recognition},
  file      = {arXiv Fulltext PDF:C\:\\Users\\PierreLepagnol\\Zotero\\storage\\FLYIXM6P\\Wen et al. - 2022 - Active Self-Semi-Supervised Learning for Few Label.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\PierreLepagnol\\Zotero\\storage\\F75RL7J2\\2203.html:text/html}
}

@misc{goled_weak_2021,
  title      = {Weak {Supervision}: {The} {Art} {Of} {Training} {ML} {Models} {From} {Noisy} {Data}},
  shorttitle = {Weak {Supervision}},
  url        = {https://analyticsindiamag.com/weak-supervision-the-art-of-training-ml-models-from-noisy-data/},
  abstract   = {Weak supervision is an ML branch where limited sources can be used to label large amounts of training data in a supervised setting.},
  language   = {en-US},
  urldate    = {2022-07-23},
  journal    = {Analytics India Magazine},
  author     = {Goled, Shraddha},
  month      = may,
  year       = {2021},
  file       = {Snapshot:C\:\\Users\\PierreLepagnol\\Zotero\\storage\\4674CSTW\\weak-supervision-the-art-of-training-ml-models-from-noisy-data.html:text/html}
}

@misc{shin_universalizing_2022,
  title     = {Universalizing {Weak} {Supervision}},
  url       = {http://arxiv.org/abs/2112.03865},
  abstract  = {Weak supervision (WS) frameworks are a popular way to bypass hand-labeling large datasets for training data-hungry models. These approaches synthesize multiple noisy but cheaply-acquired estimates of labels into a set of high-quality pseudolabels for downstream training. However, the synthesis technique is specific to a particular kind of label, such as binary labels or sequences, and each new label type requires manually designing a new synthesis algorithm. Instead, we propose a universal technique that enables weak supervision over any label type while still offering desirable properties, including practical flexibility, computational efficiency, and theoretical guarantees. We apply this technique to important problems previously not tackled by WS frameworks including learning to rank, regression, and learning in hyperbolic space. Theoretically, our synthesis approach produces a consistent estimators for learning some challenging but important generalizations of the exponential family model. Experimentally, we validate our framework and show improvement over baselines in diverse settings including real-world learning-to-rank and regression problems along with learning on hyperbolic manifolds.},
  urldate   = {2022-07-23},
  publisher = {arXiv},
  author    = {Shin, Changho and Li, Winfred and Vishwakarma, Harit and Roberts, Nicholas and Sala, Frederic},
  month     = mar,
  year      = {2022},
  note      = {arXiv:2112.03865 [cs]},
  keywords  = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence},
  file      = {arXiv Fulltext PDF:C\:\\Users\\PierreLepagnol\\Zotero\\storage\\Z5IWSBFY\\Shin et al. - 2022 - Universalizing Weak Supervision.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\PierreLepagnol\\Zotero\\storage\\MEYCN28I\\2112.html:text/html}
}

@misc{zhang_survey_2022,
  title     = {A {Survey} on {Programmatic} {Weak} {Supervision}},
  url       = {http://arxiv.org/abs/2202.05433},
  abstract  = {Labeling training data has become one of the major roadblocks to using machine learning. Among various weak supervision paradigms, programmatic weak supervision (PWS) has achieved remarkable success in easing the manual labeling bottleneck by programmatically synthesizing training labels from multiple potentially noisy supervision sources. This paper presents a comprehensive survey of recent advances in PWS. In particular, we give a brief introduction of the PWS learning paradigm, and review representative approaches for each component within PWS's learning workflow. In addition, we discuss complementary learning paradigms for tackling limited labeled data scenarios and how these related approaches can be used in conjunction with PWS. Finally, we identify several critical challenges that remain under-explored in the area to hopefully inspire future research directions in the field.},
  urldate   = {2022-07-23},
  publisher = {arXiv},
  author    = {Zhang, Jieyu and Hsieh, Cheng-Yu and Yu, Yue and Zhang, Chao and Ratner, Alexander},
  month     = feb,
  year      = {2022},
  note      = {arXiv:2202.05433 [cs, stat]},
  keywords  = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Statistics - Applications},
  file      = {arXiv Fulltext PDF:C\:\\Users\\PierreLepagnol\\Zotero\\storage\\8D5LY9YY\\Zhang et al. - 2022 - A Survey on Programmatic Weak Supervision.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\PierreLepagnol\\Zotero\\storage\\MSLNDACH\\2202.html:text/html}
}

@article{kartchner_rule-enhanced_2022,
  title    = {Rule-{Enhanced} {Active} {Learning} for {Semi}-{Automated} {Weak} {Supervision}},
  volume   = {3},
  issn     = {2673-2688},
  url      = {https://www.mdpi.com/2673-2688/3/1/13},
  doi      = {10.3390/ai3010013},
  abstract = {A major bottleneck preventing the extension of deep learning systems to new domains is the prohibitive cost of acquiring sufﬁcient training labels. Alternatives such as weak supervision, active learning, and ﬁne-tuning of pretrained models reduce this burden but require substantial human input to select a highly informative subset of instances or to curate labeling functions. REGAL (Rule-Enhanced Generative Active Learning) is an improved framework for weakly supervised text classiﬁcation that performs active learning over labeling functions rather than individual instances. REGAL interactively creates high-quality labeling patterns from raw text, enabling a single annotator to accurately label an entire dataset after initialization with three keywords for each class. Experiments demonstrate that REGAL extracts up to 3 times as many high-accuracy labeling functions from text as current state-of-the-art methods for interactive weak supervision, enabling REGAL to dramatically reduce the annotation burden of writing labeling functions for weak supervision. Statistical analysis reveals REGAL performs equal or signiﬁcantly better than interactive weak supervision for ﬁve of six commonly used natural language processing (NLP) baseline datasets.},
  language = {en},
  number   = {1},
  urldate  = {2022-07-23},
  journal  = {AI},
  author   = {Kartchner, David and Nakajima An, Davi and Ren, Wendi and Zhang, Chao and Mitchell, Cassie S.},
  month    = mar,
  year     = {2022},
  pages    = {211--228},
  file     = {Kartchner et al. - 2022 - Rule-Enhanced Active Learning for Semi-Automated W.pdf:C\:\\Users\\PierreLepagnol\\Zotero\\storage\\K6W7QML4\\Kartchner et al. - 2022 - Rule-Enhanced Active Learning for Semi-Automated W.pdf:application/pdf}
}

@article{berti-equille_reinforcement_nodate,
  title    = {Reinforcement {Learning} for {Data} {Cleaning} and {Data} {Preparation}},
  language = {en},
  author   = {Berti-Equille, Laure},
  pages    = {36},
  file     = {Berti-Equille - Reinforcement Learning for Data Cleaning and Data .pdf:C\:\\Users\\PierreLepagnol\\Zotero\\storage\\V4RNHU5I\\Berti-Equille - Reinforcement Learning for Data Cleaning and Data .pdf:application/pdf}
}

@misc{vaswani_attention_2017,
  title     = {Attention {Is} {All} {You} {Need}},
  url       = {http://arxiv.org/abs/1706.03762},
  doi       = {10.48550/arXiv.1706.03762},
  abstract  = {The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.},
  urldate   = {2022-08-17},
  publisher = {arXiv},
  author    = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N. and Kaiser, Lukasz and Polosukhin, Illia},
  month     = dec,
  year      = {2017},
  note      = {arXiv:1706.03762 [cs]},
  keywords  = {Computer Science - Computation and Language, Computer Science - Machine Learning},
  file      = {arXiv Fulltext PDF:C\:\\Users\\PierreLepagnol\\Zotero\\storage\\R52NLPR4\\Vaswani et al. - 2017 - Attention Is All You Need.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\PierreLepagnol\\Zotero\\storage\\TNLX6G7N\\1706.html:text/html}
}

@article{wang_survey_2019,
  title      = {A {Survey} of {Zero}-{Shot} {Learning}: {Settings}, {Methods}, and {Applications}},
  volume     = {10},
  issn       = {2157-6904, 2157-6912},
  shorttitle = {A {Survey} of {Zero}-{Shot} {Learning}},
  url        = {https://dl.acm.org/doi/10.1145/3293318},
  doi        = {10.1145/3293318},
  abstract   = {Most machine-learning methods focus on classifying instances whose classes have already been seen in training. In practice, many applications require classifying instances whose classes have not been seen previously. Zero-shot learning is a powerful and promising learning paradigm, in which the classes covered by training instances and the classes we aim to classify are disjoint. In this paper, we provide a comprehensive survey of zero-shot learning. First of all, we provide an overview of zero-shot learning. According to the data utilized in model optimization, we classify zero-shot learning into three learning settings. Second, we describe different semantic spaces adopted in existing zero-shot learning works. Third, we categorize existing zero-shot learning methods and introduce representative methods under each category. Fourth, we discuss different applications of zero-shot learning. Finally, we highlight promising future research directions of zero-shot learning.},
  language   = {en},
  number     = {2},
  urldate    = {2022-09-01},
  journal    = {ACM Transactions on Intelligent Systems and Technology},
  author     = {Wang, Wei and Zheng, Vincent W. and Yu, Han and Miao, Chunyan},
  month      = mar,
  year       = {2019},
  pages      = {1--37}
}

@article{boschini_continual_2022,
  title    = {Continual {Semi}-{Supervised} {Learning} through {Contrastive} {Interpolation} {Consistency}},
  volume   = {162},
  issn     = {01678655},
  url      = {http://arxiv.org/abs/2108.06552},
  doi      = {10.1016/j.patrec.2022.08.006},
  abstract = {Continual Learning (CL) investigates how to train Deep Networks on a stream of tasks without incurring forgetting. CL settings proposed in literature assume that every incoming example is paired with ground-truth annotations. However, this clashes with many real-world applications: gathering labeled data, which is in itself tedious and expensive, becomes infeasible when data flow as a stream. This work explores Continual Semi-Supervised Learning (CSSL): here, only a small fraction of labeled input examples are shown to the learner. We assess how current CL methods (e.g.: EWC, LwF, iCaRL, ER, GDumb, DER) perform in this novel and challenging scenario, where overfitting entangles forgetting. Subsequently, we design a novel CSSL method that exploits metric learning and consistency regularization to leverage unlabeled examples while learning. We show that our proposal exhibits higher resilience to diminishing supervision and, even more surprisingly, relying only on 25\% supervision suffices to outperform SOTA methods trained under full supervision.},
  urldate  = {2022-09-02},
  journal  = {Pattern Recognition Letters},
  author   = {Boschini, Matteo and Buzzega, Pietro and Bonicelli, Lorenzo and Porrello, Angelo and Calderara, Simone},
  month    = oct,
  year     = {2022},
  note     = {arXiv:2108.06552 [cs, stat]},
  keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
  pages    = {9--14},
  file     = {arXiv Fulltext PDF:C\:\\Users\\PierreLepagnol\\Zotero\\storage\\BW3VJS9M\\Boschini et al. - 2022 - Continual Semi-Supervised Learning through Contras.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\PierreLepagnol\\Zotero\\storage\\CKV53XB7\\2108.html:text/html}
}

@article{ratner_snorkel_2017,
  title      = {Snorkel: {Rapid} {Training} {Data} {Creation} with {Weak} {Supervision}},
  volume     = {11},
  issn       = {2150-8097},
  shorttitle = {Snorkel},
  url        = {http://arxiv.org/abs/1711.10160},
  doi        = {10.14778/3157794.3157797},
  abstract   = {Labeling training data is increasingly the largest bottleneck in deploying machine learning systems. We present Snorkel, a first-of-its-kind system that enables users to train state-of-the-art models without hand labeling any training data. Instead, users write labeling functions that express arbitrary heuristics, which can have unknown accuracies and correlations. Snorkel denoises their outputs without access to ground truth by incorporating the first end-to-end implementation of our recently proposed machine learning paradigm, data programming. We present a flexible interface layer for writing labeling functions based on our experience over the past year collaborating with companies, agencies, and research labs. In a user study, subject matter experts build models 2.8x faster and increase predictive performance an average 45.5\% versus seven hours of hand labeling. We study the modeling tradeoffs in this new setting and propose an optimizer for automating tradeoff decisions that gives up to 1.8x speedup per pipeline execution. In two collaborations, with the U.S. Department of Veterans Affairs and the U.S. Food and Drug Administration, and on four open-source text and image data sets representative of other deployments, Snorkel provides 132\% average improvements to predictive performance over prior heuristic approaches and comes within an average 3.60\% of the predictive performance of large hand-curated training sets.},
  number     = {3},
  urldate    = {2022-09-05},
  journal    = {Proceedings of the VLDB Endowment},
  author     = {Ratner, Alexander and Bach, Stephen H. and Ehrenberg, Henry and Fries, Jason and Wu, Sen and Ré, Christopher},
  month      = nov,
  year       = {2017},
  note       = {arXiv:1711.10160 [cs, stat]},
  keywords   = {Computer Science - Machine Learning, Statistics - Machine Learning},
  pages      = {269--282},
  file       = {arXiv Fulltext PDF:C\:\\Users\\PierreLepagnol\\Zotero\\storage\\TQ5HSQ7C\\Ratner et al. - 2017 - Snorkel Rapid Training Data Creation with Weak Su.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\PierreLepagnol\\Zotero\\storage\\JZZVEIV4\\1711.html:text/html}
}


@misc{ren_survey_2021,
  title     = {A {Survey} of {Deep} {Active} {Learning}},
  url       = {http://arxiv.org/abs/2009.00236},
  doi       = {10.48550/arXiv.2009.00236},
  abstract  = {Active learning (AL) attempts to maximize the performance gain of the model by marking the fewest samples. Deep learning (DL) is greedy for data and requires a large amount of data supply to optimize massive parameters, so that the model learns how to extract high-quality features. In recent years, due to the rapid development of internet technology, we are in an era of information torrents and we have massive amounts of data. In this way, DL has aroused strong interest of researchers and has been rapidly developed. Compared with DL, researchers have relatively low interest in AL. This is mainly because before the rise of DL, traditional machine learning requires relatively few labeled samples. Therefore, early AL is difficult to reflect the value it deserves. Although DL has made breakthroughs in various fields, most of this success is due to the publicity of the large number of existing annotation datasets. However, the acquisition of a large number of high-quality annotated datasets consumes a lot of manpower, which is not allowed in some fields that require high expertise, especially in the fields of speech recognition, information extraction, medical images, etc. Therefore, AL has gradually received due attention. A natural idea is whether AL can be used to reduce the cost of sample annotations, while retaining the powerful learning capabilities of DL. Therefore, deep active learning (DAL) has emerged. Although the related research has been quite abundant, it lacks a comprehensive survey of DAL. This article is to fill this gap, we provide a formal classification method for the existing work, and a comprehensive and systematic overview. In addition, we also analyzed and summarized the development of DAL from the perspective of application. Finally, we discussed the confusion and problems in DAL, and gave some possible development directions for DAL.},
  urldate   = {2022-09-06},
  publisher = {arXiv},
  author    = {Ren, Pengzhen and Xiao, Yun and Chang, Xiaojun and Huang, Po-Yao and Li, Zhihui and Gupta, Brij B. and Chen, Xiaojiang and Wang, Xin},
  month     = dec,
  year      = {2021},
  note      = {arXiv:2009.00236 [cs, stat]},
  keywords  = {Computer Science - Machine Learning, Statistics - Machine Learning},
  file      = {arXiv Fulltext PDF:C\:\\Users\\PierreLepagnol\\Zotero\\storage\\BGGZFSB6\\Ren et al. - 2021 - A Survey of Deep Active Learning.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\PierreLepagnol\\Zotero\\storage\\Z5QC3D2R\\2009.html:text/html}
}

@misc{Muennighoff2022,
  author        = {Niklas Muennighoff and Thomas Wang and Lintang Sutawika and Adam Roberts and Stella Biderman and Teven Le Scao and M Saiful Bari and Sheng Shen and Zheng-Xin Yong and Hailey Schoelkopf and Xiangru Tang and Dragomir Radev and Alham Fikri Aji and Khalid Almubarak and Samuel Albanie and Zaid Alyafeai and Albert Webson and Edward Raff and Colin Raffel},
  title         = {Crosslingual Generalization through Multitask Finetuning},
  year          = {2022},
  archiveprefix = {arXiv},
  eprint        = {2211.01786},
  primaryclass  = {cs.CL}
}

@inproceedings{Labrak2023,
  author    = {Labrak, Yanis and Bazoge, Adrien and Dufour, Richard and Rouvier, Mickael and Morin, Emmanuel and Daille, Béatrice and Gourraud, Pierre-Antoine},
  booktitle = {Proceedings of the 61th Annual Meeting of the Association for Computational Linguistics (ACL'23), Long Paper},
  title     = {{DrBERT: A Robust Pre-trained Model in French for Biomedical and Clinical domains}},
  year      = {2023},
  address   = {Toronto, Canada},
  month     = july,
  publisher = {Association for Computational Linguistics}
}


@inproceedings{Shah2022,
  author     = {Shah, Raj and Chawla, Kunal and Eidnani, Dheeraj and Shah, Agam and Du, Wendi and Chava, Sudheer and Raman, Natraj and Smiley, Charese and Chen, Jiaao and Yang, Diyi},
  booktitle  = {Proceedings of the 2022 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing}},
  title      = {When {FLUE} {Meets} {FLANG}: {Benchmarks} and {Large} {Pretrained} {Language} {Model} for {Financial} {Domain}},
  year       = {2022},
  address    = {Abu Dhabi, United Arab Emirates},
  month      = dec,
  pages      = {2322--2335},
  publisher  = {Association for Computational Linguistics},
  abstract   = {Pre-trained language models have shown impressive performance on a variety of tasks and domains. Previous research on financial language models usually employs a generic training scheme to train standard model architectures, without completely leveraging the richness of the financial data. We propose a novel domain specific Financial LANGuage model (FLANG) which uses financial keywords and phrases for better masking, together with span boundary objective and in-filing objective. Additionally, the evaluation benchmarks in the field have been limited. To this end, we contribute the Financial Language Understanding Evaluation (FLUE), an open-source comprehensive suite of benchmarks for the financial domain. These include new benchmarks across 5 NLP tasks in financial domain as well as common benchmarks used in the previous research. Experiments on these benchmarks suggest that our model outperforms those in prior literature on a variety of NLP tasks. Our models, code and benchmark data will be made publicly available on Github and Huggingface.},
  file       = {:Shah2022 - When FLUE Meets FLANG_ Benchmarks and Large Pretrained Language Model for Financial Domain.pdf:PDF},
  shorttitle = {When {FLUE} {Meets} {FLANG}},
  url        = {https://aclanthology.org/2022.emnlp-main.148},
  urldate    = {2023-06-20}
}


@techreport{Zhou2023,
  author     = {Zhou, Chunting and Liu, Pengfei and Xu, Puxin and Iyer, Srini and Sun, Jiao and Mao, Yuning and Ma, Xuezhe and Efrat, Avia and Yu, Ping and Yu, Lili and Zhang, Susan and Ghosh, Gargi and Lewis, Mike and Zettlemoyer, Luke and Levy, Omer},
  title      = {{LIMA}: {Less} {Is} {More} for {Alignment}},
  year       = {2023},
  month      = may,
  note       = {arXiv:2305.11206 [cs] type: article},
  abstract   = {Large language models are trained in two stages: (1) unsupervised pretraining from raw text, to learn general-purpose representations, and (2) large scale instruction tuning and reinforcement learning, to better align to end tasks and user preferences. We measure the relative importance of these two stages by training LIMA, a 65B parameter LLaMa language model fine-tuned with the standard supervised loss on only 1,000 carefully curated prompts and responses, without any reinforcement learning or human preference modeling. LIMA demonstrates remarkably strong performance, learning to follow specific response formats from only a handful of examples in the training data, including complex queries that range from planning trip itineraries to speculating about alternate history. Moreover, the model tends to generalize well to unseen tasks that did not appear in the training data. In a controlled human study, responses from LIMA are either equivalent or strictly preferred to GPT-4 in 43\% of cases; this statistic is as high as 58\% when compared to Bard and 65\% versus DaVinci003, which was trained with human feedback. Taken together, these results strongly suggest that almost all knowledge in large language models is learned during pretraining, and only limited instruction tuning data is necessary to teach models to produce high quality output.},
  doi        = {10.48550/arXiv.2305.11206},
  file       = {:Zhou2023 - LIMA_ Less Is More for Alignment.pdf:PDF},
  keywords   = {Computer Science - Computation and Language, Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
  school     = {arXiv},
  shorttitle = {{LIMA}},
  url        = {http://arxiv.org/abs/2305.11206},
  urldate    = {2023-06-20}
}

@techreport{Gao2021,
  author   = {Gao, Tianyu and Fisch, Adam and Chen, Danqi},
  title    = {Making {Pre}-trained {Language} {Models} {Better} {Few}-shot {Learners}},
  year     = {2021},
  month    = jun,
  note     = {arXiv:2012.15723 [cs] type: article},
  abstract = {The recent GPT-3 model (Brown et al., 2020) achieves remarkable few-shot performance solely by leveraging a natural-language prompt and a few task demonstrations as input context. Inspired by their findings, we study few-shot learning in a more practical scenario, where we use smaller language models for which fine-tuning is computationally efficient. We present LM-BFF--better few-shot fine-tuning of language models--a suite of simple and complementary techniques for fine-tuning language models on a small number of annotated examples. Our approach includes (1) prompt-based fine-tuning together with a novel pipeline for automating prompt generation; and (2) a refined strategy for dynamically and selectively incorporating demonstrations into each context. Finally, we present a systematic evaluation for analyzing few-shot performance on a range of NLP tasks, including classification and regression. Our experiments demonstrate that our methods combine to dramatically outperform standard fine-tuning procedures in this low resource setting, achieving up to 30\% absolute improvement, and 11\% on average across all tasks. Our approach makes minimal assumptions on task resources and domain expertise, and hence constitutes a strong task-agnostic method for few-shot learning.},
  annote   = {Comment: Accepted to ACL 2021. The code is publicly available at https://github.com/princeton-nlp/LM-BFF},
  file     = {:Gao2021 - Making Pre Trained Language Models Better Few Shot Learners.pdf:PDF},
  keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning},
  school   = {arXiv},
  url      = {http://arxiv.org/abs/2012.15723},
  urldate  = {2023-06-20}
}


@article{Wei2021,
  author   = {Wei, Jason and Bosma, Maarten and Zhao, Vincent and Guu, Kelvin and Yu, A. and Lester, Brian and Du, Nan and Dai, Andrew M. and Le, Quoc V.},
  journal  = {ArXiv},
  title    = {Finetuned {Language} {Models} {Are} {Zero}-{Shot} {Learners}},
  year     = {2021},
  month    = sep,
  abstract = {This paper explores a simple method for improving the zero-shot learning abilities of language models. We show that instruction tuning -- finetuning language models on a collection of tasks described via instructions -- substantially improves zero-shot performance on unseen tasks. We take a 137B parameter pretrained language model and instruction-tune it on over 60 NLP tasks verbalized via natural language instruction templates. We evaluate this instruction-tuned model, which we call FLAN, on unseen task types. FLAN substantially improves the performance of its unmodified counterpart and surpasses zero-shot 175B GPT-3 on 20 of 25 tasks that we evaluate. FLAN even outperforms few-shot GPT-3 by a large margin on ANLI, RTE, BoolQ, AI2-ARC, OpenbookQA, and StoryCloze. Ablation studies reveal that number of finetuning datasets, model scale, and natural language instructions are key to the success of instruction tuning.},
  annote   = {[TLDR] It is shown that instruction tuning -- finetuning language models on a collection of tasks described via instructions -- substantially improves zero-shot performance on unseen tasks and outperforms few-shot GPT-3 by a large margin.},
  file     = {:Wei2021 - Finetuned Language Models Are Zero Shot Learners.html:URL;:Wei2021 - Finetuned Language Models Are Zero Shot Learners.pdf:PDF},
  url      = {https://www.semanticscholar.org/paper/Finetuned-Language-Models-Are-Zero-Shot-Learners-Wei-Bosma/ff0b2681d7b05e16c46dfb71d980cc2f605907cd},
  urldate  = {2023-06-22}
}


@techreport{Wei2023,
  author   = {Wei, Jason and Wang, Xuezhi and Schuurmans, Dale and Bosma, Maarten and Ichter, Brian and Xia, Fei and Chi, Ed and Le, Quoc and Zhou, Denny},
  title    = {Chain-of-{Thought} {Prompting} {Elicits} {Reasoning} in {Large} {Language} {Models}},
  year     = {2023},
  month    = jan,
  note     = {arXiv:2201.11903 [cs] type: article},
  abstract = {We explore how generating a chain of thought -- a series of intermediate reasoning steps -- significantly improves the ability of large language models to perform complex reasoning. In particular, we show how such reasoning abilities emerge naturally in sufficiently large language models via a simple method called chain of thought prompting, where a few chain of thought demonstrations are provided as exemplars in prompting. Experiments on three large language models show that chain of thought prompting improves performance on a range of arithmetic, commonsense, and symbolic reasoning tasks. The empirical gains can be striking. For instance, prompting a 540B-parameter language model with just eight chain of thought exemplars achieves state of the art accuracy on the GSM8K benchmark of math word problems, surpassing even finetuned GPT-3 with a verifier.},
  file     = {:Wei2023 - Chain of Thought Prompting Elicits Reasoning in Large Language Models.pdf:PDF},
  keywords = {Computer Science - Computation and Language, Computer Science - Artificial Intelligence},
  school   = {arXiv},
  url      = {http://arxiv.org/abs/2201.11903},
  urldate  = {2023-06-22}
}


@techreport{Shin2020,
  author     = {Shin, Taylor and Razeghi, Yasaman and Logan IV, Robert L. and Wallace, Eric and Singh, Sameer},
  title      = {{AutoPrompt}: {Eliciting} {Knowledge} from {Language} {Models} with {Automatically} {Generated} {Prompts}},
  year       = {2020},
  month      = nov,
  note       = {arXiv:2010.15980 [cs] type: article},
  abstract   = {The remarkable success of pretrained language models has motivated the study of what kinds of knowledge these models learn during pretraining. Reformulating tasks as fill-in-the-blanks problems (e.g., cloze tests) is a natural approach for gauging such knowledge, however, its usage is limited by the manual effort and guesswork required to write suitable prompts. To address this, we develop AutoPrompt, an automated method to create prompts for a diverse set of tasks, based on a gradient-guided search. Using AutoPrompt, we show that masked language models (MLMs) have an inherent capability to perform sentiment analysis and natural language inference without additional parameters or finetuning, sometimes achieving performance on par with recent state-of-the-art supervised models. We also show that our prompts elicit more accurate factual knowledge from MLMs than the manually created prompts on the LAMA benchmark, and that MLMs can be used as relation extractors more effectively than supervised relation extraction models. These results demonstrate that automatically generated prompts are a viable parameter-free alternative to existing probing methods, and as pretrained LMs become more sophisticated and capable, potentially a replacement for finetuning.},
  annote     = {Comment: v2: Fixed error in Figure 2},
  doi        = {10.48550/arXiv.2010.15980},
  file       = {:Shin2020 - AutoPrompt_ Eliciting Knowledge from Language Models with Automatically Generated Prompts.pdf:PDF},
  keywords   = {Computer Science - Computation and Language, Computer Science - Machine Learning},
  priority   = {prio1},
  school     = {arXiv},
  shorttitle = {{AutoPrompt}},
  url        = {http://arxiv.org/abs/2010.15980},
  urldate    = {2023-06-22}
}


@techreport{Wallace2021,
  author   = {Wallace, Eric and Feng, Shi and Kandpal, Nikhil and Gardner, Matt and Singh, Sameer},
  title    = {Universal {Adversarial} {Triggers} for {Attacking} and {Analyzing} {NLP}},
  year     = {2021},
  month    = jan,
  note     = {arXiv:1908.07125 [cs] type: article},
  abstract = {Adversarial examples highlight model vulnerabilities and are useful for evaluation and interpretation. We define universal adversarial triggers: input-agnostic sequences of tokens that trigger a model to produce a specific prediction when concatenated to any input from a dataset. We propose a gradient-guided search over tokens which finds short trigger sequences (e.g., one word for classification and four words for language modeling) that successfully trigger the target prediction. For example, triggers cause SNLI entailment accuracy to drop from 89.94\% to 0.55\%, 72\% of "why" questions in SQuAD to be answered "to kill american people", and the GPT-2 language model to spew racist output even when conditioned on non-racial contexts. Furthermore, although the triggers are optimized using white-box access to a specific model, they transfer to other models for all tasks we consider. Finally, since triggers are input-agnostic, they provide an analysis of global model behavior. For instance, they confirm that SNLI models exploit dataset biases and help to diagnose heuristics learned by reading comprehension models.},
  annote   = {Comment: EMNLP 2019},
  file     = {:Wallace2021 - Universal Adversarial Triggers for Attacking and Analyzing NLP.pdf:PDF},
  keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning},
  priority = {prio1},
  school   = {arXiv},
  url      = {http://arxiv.org/abs/1908.07125},
  urldate  = {2023-06-22}
}


@techreport{Hambardzumyan2021,
  author     = {Hambardzumyan, Karen and Khachatrian, Hrant and May, Jonathan},
  title      = {{WARP}: {Word}-level {Adversarial} {ReProgramming}},
  year       = {2021},
  month      = jun,
  note       = {arXiv:2101.00121 [cs] type: article},
  abstract   = {Transfer learning from pretrained language models recently became the dominant approach for solving many NLP tasks. A common approach to transfer learning for multiple tasks that maximize parameter sharing trains one or more task-specific layers on top of the language model. In this paper, we present an alternative approach based on adversarial reprogramming, which extends earlier work on automatic prompt generation. Adversarial reprogramming attempts to learn task-specific word embeddings that, when concatenated to the input text, instruct the language model to solve the specified task. Using up to 25K trainable parameters per task, this approach outperforms all existing methods with up to 25M trainable parameters on the public leaderboard of the GLUE benchmark. Our method, initialized with task-specific human-readable prompts, also works in a few-shot setting, outperforming GPT-3 on two SuperGLUE tasks with just 32 training samples.},
  annote     = {Comment: Accepted ACL 2021 Long Paper},
  file       = {:Hambardzumyan2021 - WARP_ Word Level Adversarial ReProgramming.pdf:PDF},
  keywords   = {Computer Science - Computation and Language},
  school     = {arXiv},
  shorttitle = {{WARP}},
  url        = {http://arxiv.org/abs/2101.00121},
  urldate    = {2023-06-22}
}

@misc{touvron_llama_2023,
  title      = {{LLaMA}: {Open} and {Efficient} {Foundation} {Language} {Models}},
  shorttitle = {{LLaMA}},
  url        = {http://arxiv.org/abs/2302.13971},
  doi        = {10.48550/arXiv.2302.13971},
  abstract   = {We introduce LLaMA, a collection of foundation language models ranging from 7B to 65B parameters. We train our models on trillions of tokens, and show that it is possible to train state-of-the-art models using publicly available datasets exclusively, without resorting to proprietary and inaccessible datasets. In particular, LLaMA-13B outperforms GPT-3 (175B) on most benchmarks, and LLaMA-65B is competitive with the best models, Chinchilla-70B and PaLM-540B. We release all our models to the research community.},
  urldate    = {2023-09-29},
  publisher  = {arXiv},
  author     = {Touvron, Hugo and others},
  month      = feb,
  year       = {2023},
  note       = {arXiv:2302.13971 [cs]},
  keywords   = {Computer Science - Computation and Language}
}

@misc{touvron_llama_2023-1,
  title      = {Llama 2: {Open} {Foundation} and {Fine}-{Tuned} {Chat} {Models}},
  shorttitle = {Llama 2},
  url        = {http://arxiv.org/abs/2307.09288},
  doi        = {10.48550/arXiv.2307.09288},
  abstract   = {In this work, we develop and release Llama 2, a collection of pretrained and fine-tuned large language models (LLMs) ranging in scale from 7 billion to 70 billion parameters. Our fine-tuned LLMs, called Llama 2-Chat, are optimized for dialogue use cases. Our models outperform open-source chat models on most benchmarks we tested, and based on our human evaluations for helpfulness and safety, may be a suitable substitute for closed-source models. We provide a detailed description of our approach to fine-tuning and safety improvements of Llama 2-Chat in order to enable the community to build on our work and contribute to the responsible development of LLMs.},
  urldate    = {2023-09-29},
  publisher  = {arXiv},
  author     = {Touvron, Hugo and others},
  month      = jul,
  year       = {2023},
  note       = {arXiv:2307.09288 [cs]},
  keywords   = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language}
}

@inproceedings{schick_exploiting_2021,
  address   = {Online},
  title     = {Exploiting {Cloze}-{Questions} for {Few}-{Shot} {Text} {Classification} and {Natural} {Language} {Inference}},
  url       = {https://aclanthology.org/2021.eacl-main.20},
  doi       = {10.18653/v1/2021.eacl-main.20},
  abstract  = {Some NLP tasks can be solved in a fully unsupervised fashion by providing a pretrained language model with “task descriptions” in natural language (e.g., Radford et al., 2019). While this approach underperforms its supervised counterpart, we show in this work that the two ideas can be combined: We introduce Pattern-Exploiting Training (PET), a semi-supervised training procedure that reformulates input examples as cloze-style phrases to help language models understand a given task. These phrases are then used to assign soft labels to a large set of unlabeled examples. Finally, standard supervised training is performed on the resulting training set. For several tasks and languages, PET outperforms supervised training and strong semi-supervised approaches in low-resource settings by a large margin.},
  urldate   = {2023-10-08},
  booktitle = {Proceedings of the 16th {Conference} of the {European} {Chapter} of the {Association} for {Computational} {Linguistics}: {Main} {Volume}},
  publisher = {Association for Computational Linguistics},
  author    = {Schick, Timo and Schütze, Hinrich},
  month     = apr,
  year      = {2021},
  pages     = {255--269}
}

@inproceedings{ma_frustratingly_2021,
  address   = {Online},
  title     = {Frustratingly {Simple} {Few}-{Shot} {Slot} {Tagging}},
  url       = {https://aclanthology.org/2021.findings-acl.88},
  doi       = {10.18653/v1/2021.findings-acl.88},
  urldate   = {2023-10-08},
  booktitle = {Findings of the {Association} for {Computational} {Linguistics}: {ACL}-{IJCNLP} 2021},
  publisher = {Association for Computational Linguistics},
  author    = {Ma, Jianqiang and Yan, Zeyu and Li, Chang and Zhang, Yang},
  month     = aug,
  year      = {2021},
  pages     = {1028--1033}
}

@inproceedings{hou_learning_2021,
  address    = {Online},
  title      = {Learning to {Bridge} {Metric} {Spaces}: {Few}-shot {Joint} {Learning} of {Intent} {Detection} and {Slot} {Filling}},
  shorttitle = {Learning to {Bridge} {Metric} {Spaces}},
  url        = {https://aclanthology.org/2021.findings-acl.282},
  doi        = {10.18653/v1/2021.findings-acl.282},
  urldate    = {2023-10-08},
  booktitle  = {Findings of the {Association} for {Computational} {Linguistics}: {ACL}-{IJCNLP} 2021},
  publisher  = {Association for Computational Linguistics},
  author     = {Hou, Yutai and Lai, Yongkui and Chen, Cheng and Che, Wanxiang and Liu, Ting},
  month      = aug,
  year       = {2021},
  pages      = {3190--3200}
}

@inproceedings{wang_slot_2022,
  address   = {Gyeongju, Republic of Korea},
  title     = {Slot {Dependency} {Modeling} for {Zero}-{Shot} {Cross}-{Domain} {Dialogue} {State} {Tracking}},
  url       = {https://aclanthology.org/2022.coling-1.42},
  urldate   = {2023-08-04},
  booktitle = {Proceedings of the 29th {International} {Conference} on {Computational} {Linguistics}},
  publisher = {International Committee on Computational Linguistics},
  author    = {Wang, Qingyue and Cao, Yanan and Li, Piji and Fu, Yanhe and Lin, Zheng and Guo, Li},
  month     = oct,
  year      = {2022},
  pages     = {510--520}
}

@inproceedings{hu_-context_2022,
  address   = {Abu Dhabi, United Arab Emirates},
  title     = {In-{Context} {Learning} for {Few}-{Shot} {Dialogue} {State} {Tracking}},
  url       = {https://aclanthology.org/2022.findings-emnlp.193},
  doi       = {10.18653/v1/2022.findings-emnlp.193},
  abstract  = {Collecting and annotating task-oriented dialogues is time-consuming and costly. Thus, zero and few shot learning for dialogue tasks presents an exciting opportunity. In this work, we propose an in-context (IC) learning framework for zero-shot and few-shot learning dialogue state tracking (DST), where a large pretrained language model (LM) takes a test instance and a few exemplars as input, and directly decodes the dialogue state without any parameter updates. This approach is more flexible and scalable than prior DST work when adapting to new domains and scenarios. To better leverage a tabular domain description in the LM prompt, we reformulate DST into a text-to-SQL problem. We also propose a novel approach to retrieve annotated dialogues as exemplars. Empirical results on MultiWOZ show that our method IC-DST substantially outperforms previous fine-tuned state-of-the-art models in few-shot settings. In addition, we test IC-DST in zero-shot settings, in which the model only takes a fixed task instruction as input, finding that it outperforms previous zero-shot methods by a large margin.},
  urldate   = {2023-10-09},
  booktitle = {Findings of the {Association} for {Computational} {Linguistics}: {EMNLP} 2022},
  publisher = {Association for Computational Linguistics},
  author    = {Hu, Yushi and Lee, Chia-Hsuan and Xie, Tianbao and Yu, Tao and Smith, Noah A. and Ostendorf, Mari},
  month     = dec,
  year      = {2022},
  pages     = {2627--2643}
}


@misc{hung2022dstod,
  title         = {DS-TOD: Efficient Domain Specialization for Task Oriented Dialog},
  author        = {Chia-Chien Hung and Anne Lauscher and Simone Paolo Ponzetto and Goran Glavaš},
  year          = {2022},
  eprint        = {2110.08395},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL}
}


@misc{liu2022fewshot,
  title         = {Few-Shot Parameter-Efficient Fine-Tuning is Better and Cheaper than In-Context Learning},
  author        = {Haokun Liu and Derek Tam and Mohammed Muqeeth and Jay Mohta and Tenghao Huang and Mohit Bansal and Colin Raffel},
  year          = {2022},
  eprint        = {2205.05638},
  archiveprefix = {arXiv},
  primaryclass  = {cs.LG}
}


@misc{li2023robust,
  title         = {Robust Instruction Optimization for Large Language Models with Distribution Shifts},
  author        = {Moxin Li and Wenjie Wang and Fuli Feng and Jizhi Zhang and Tat-Seng Chua},
  year          = {2023},
  eprint        = {2305.13954},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL}
}


@misc{rawte_troubling_2023,
  title     = {The {Troubling} {Emergence} of {Hallucination} in {Large} {Language} {Models} -- {An} {Extensive} {Definition}, {Quantification}, and {Prescriptive} {Remediations}},
  url       = {http://arxiv.org/abs/2310.04988},
  doi       = {10.48550/arXiv.2310.04988},
  abstract  = {The recent advancements in Large Language Models (LLMs) have garnered widespread acclaim for their remarkable emerging capabilities. However, the issue of hallucination has parallelly emerged as a by-product, posing significant concerns. While some recent endeavors have been made to identify and mitigate different types of hallucination, there has been a limited emphasis on the nuanced categorization of hallucination and associated mitigation methods. To address this gap, we offer a fine-grained discourse on profiling hallucination based on its degree, orientation, and category, along with offering strategies for alleviation. As such, we define two overarching orientations of hallucination: (i) factual mirage (FM) and (ii) silver lining (SL). To provide a more comprehensive understanding, both orientations are further sub-categorized into intrinsic and extrinsic, with three degrees of severity - (i) mild, (ii) moderate, and (iii) alarming. We also meticulously categorize hallucination into six types: (i) acronym ambiguity, (ii) numeric nuisance, (iii) generated golem, (iv) virtual voice, (v) geographic erratum, and (vi) time wrap. Furthermore, we curate HallucInation eLiciTation (HILT), a publicly available dataset comprising of 75,000 samples generated using 15 contemporary LLMs along with human annotations for the aforementioned categories. Finally, to establish a method for quantifying and to offer a comparative spectrum that allows us to evaluate and rank LLMs based on their vulnerability to producing hallucinations, we propose Hallucination Vulnerability Index (HVI). We firmly believe that HVI holds significant value as a tool for the wider NLP community, with the potential to serve as a rubric in AI-related policy-making. In conclusion, we propose two solution strategies for mitigating hallucinations.},
  urldate   = {2023-10-19},
  publisher = {arXiv},
  author    = {Rawte, Vipula and Chakraborty, Swagata and Pathak, Agnibh and Sarkar, Anubhav and Tonmoy, S. M. Towhidul Islam and Chadha, Aman and Sheth, Amit P. and Das, Amitava},
  month     = oct,
  year      = {2023},
  note      = {arXiv:2310.04988 [cs]},
  keywords  = {Computer Science - Artificial Intelligence}
}

@inproceedings{su_multi-task_2022,
  address   = {Dublin, Ireland},
  title     = {Multi-{Task} {Pre}-{Training} for {Plug}-and-{Play} {Task}-{Oriented} {Dialogue} {System}},
  url       = {https://aclanthology.org/2022.acl-long.319},
  doi       = {10.18653/v1/2022.acl-long.319},
  abstract  = {Pre-trained language models have been recently shown to benefit task-oriented dialogue (TOD) systems. Despite their success, existing methods often formulate this task as a cascaded generation problem which can lead to error accumulation across different sub-tasks and greater data annotation overhead. In this study, we present PPTOD, a unified plug-and-play model for task-oriented dialogue. In addition, we introduce a new dialogue multi-task pre-training strategy that allows the model to learn the primary TOD task completion skills from heterogeneous dialog corpora. We extensively test our model on three benchmark TOD tasks, including end-to-end dialogue modelling, dialogue state tracking, and intent classification. Experimental results show that PPTOD achieves new state of the art on all evaluated tasks in both high-resource and low-resource scenarios. Furthermore, comparisons against previous SOTA methods show that the responses generated by PPTOD are more factually correct and semantically coherent as judged by human annotators.},
  urldate   = {2023-10-20},
  booktitle = {Proceedings of the 60th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics} ({Volume} 1: {Long} {Papers})},
  publisher = {Association for Computational Linguistics},
  author    = {Su, Yixuan and Shu, Lei and Mansimov, Elman and Gupta, Arshit and Cai, Deng and Lai, Yi-An and Zhang, Yi},
  month     = may,
  year      = {2022},
  pages     = {4661--4676}
}

@inproceedings{xie_unifiedskg_2022,
  address    = {Abu Dhabi, United Arab Emirates},
  title      = {{UnifiedSKG}: {Unifying} and {Multi}-{Tasking} {Structured} {Knowledge} {Grounding} with {Text}-to-{Text} {Language} {Models}},
  shorttitle = {{UnifiedSKG}},
  url        = {https://aclanthology.org/2022.emnlp-main.39},
  doi        = {10.18653/v1/2022.emnlp-main.39},
  abstract   = {Structured knowledge grounding (SKG) leverages structured knowledge to complete user requests, such as semantic parsing over databases and question answering over knowledge bases. Since the inputs and outputs of SKG tasks are heterogeneous, they have been studied separately by different communities, which limits systematic and compatible research on SKG. In this paper, we overcome this limitation by proposing the UnifiedSKG framework, which unifies 21 SKG tasks into a text-to-text format, aiming to promote systematic SKG research, instead of being exclusive to a single task, domain, or dataset. We use UnifiedSKG to benchmark T5 with different sizes and show that T5, with simple modifications when necessary, achieves state-of-the-art performance on almost all of the 21 tasks. We further demonstrate that multi-task prefix-tuning improves the performance on most tasks, largely improving the overall performance. UnifiedSKG also facilitates the investigation of zero-shot and few-shot learning, and we show that T0, GPT-3, and Codex struggle in zero-shot and few-shot learning for SKG. We also use UnifiedSKG to conduct a series of controlled experiments on structured knowledge encoding variants across SKG tasks. UnifiedSKG is easily extensible to more tasks, and it is open-sourced at https://github.com/hkunlp/unifiedskg.},
  urldate    = {2023-10-20},
  booktitle  = {Proceedings of the 2022 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing}},
  publisher  = {Association for Computational Linguistics},
  author     = {Xie, Tianbao and Wu, Chen Henry and Shi, Peng and Zhong, Ruiqi and Scholak, Torsten and Yasunaga, Michihiro and Wu, Chien-Sheng and Zhong, Ming and Yin, Pengcheng and Wang, Sida I. and Zhong, Victor and Wang, Bailin and Li, Chengzu and Boyle, Connor and Ni, Ansong and Yao, Ziyu and Radev, Dragomir and Xiong, Caiming and Kong, Lingpeng and Zhang, Rui and Smith, Noah A. and Zettlemoyer, Luke and Yu, Tao},
  month      = dec,
  year       = {2022},
  pages      = {602--631}
}


@languageresource{Speecon,
  author       = {{Speecon Consortium}},
  title        = {Catalan Speecon database},
  publisher    = {Speecon Project, distributed via ELRA: ELRA-Id S0327},
  year         = {2011},
  series       = {Speecon resources},
  edition      = {1.0},
  islrn        = {935-211-147-357-5},
  organization = {SpeeCon}
}

@languageresource{EMILLE,
  author       = {{Anthony McEnery and others}},
  title        = {The EMILLE/CIIL Corpus},
  publisher    = {distributed via ELRA: ELRA-Id W0037},
  year         = {2004},
  islrn        = {039-846-040-604-0},
  organization = {EMILLE (Enabling Minority Language Engineering) Project}
}

@Languageresource{Orientel,
  author    = {{Khalid Choukri and Niklas Paullson}},
  islrn     = {613-578-868-832-2},
  publisher = {distributed via ELRA: ELRA-Id ELRA-S0183},
  title     = {The OrienTel Moroccan MCA (Modern Colloquial Arabic) database},
  year      = {2004},
}

@languageresource{bigscience/xP3,
  title     = {xP3: Crosslingual generalization through multitask finetuning},
  author    = {Muennighoff, Niklas and Wang, Thomas and Sutawika, Lintang and Roberts, Adam and Biderman, Stella and Scao, Teven Le and Bari, M Saiful and Shen, Sheng and Yong, Zheng-Xin and Schoelkopf, Hailey and others},
  publisher = {BigScience, distributed via Hugging Face},
  url       = {https://huggingface.co/datasets/bigscience/xP3},
  year      = {2022}
}

@languageresource{xu2023wizardlm,
  title     = {Wizardlm: Empowering large language models to follow complex instructions},
  author    = {Xu, Can and Sun, Qingfeng and Zheng, Kai and Geng, Xiubo and Zhao, Pu and Feng, Jiazhan and Tao, Chongyang and Jiang, Daxin},
  publisher = {WizardLM, distributed via Hugging Face},
  url       = {https://huggingface.co/datasets/WizardLM/WizardLM_evol_instruct_70k},
  year      = {2023}
}

@languageresource{xu_baize_2023,
  title      = {Baize: {An} {Open}-{Source} {Chat} {Model} with {Parameter}-{Efficient} {Tuning} on {Self}-{Chat} {Data}},
  shorttitle = {Baize},
  url        = {http://arxiv.org/abs/2304.01196},
  doi        = {10.48550/arXiv.2304.01196},
  abstract   = {Chat models, such as ChatGPT, have shown impressive capabilities and have been rapidly adopted across numerous domains. However, these models are only accessible through a restricted API, creating barriers for new research and progress in the field. We propose a pipeline that can automatically generate a high-quality multi-turn chat corpus by leveraging ChatGPT to engage in a conversation with itself. Subsequently, we employ parameter-efficient tuning to enhance LLaMA, an open-source large language model. The resulting model, named Baize, demonstrates good performance in multi-turn dialogues with guardrails that minimize potential risks. Furthermore, we propose a new technique called Self-Distill with Feedback, to further improve the performance of the Baize models with feedback from ChatGPT. The Baize models and data are released for research purposes only at https://github.com/project-baize/baize-chatbot. An online demo is also available at https://huggingface.co/spaces/project-baize/chat-with-baize.},
  urldate    = {2023-09-29},
  publisher  = {arXiv},
  author     = {Xu, Canwen and Guo, Daya and Duan, Nan and McAuley, Julian},
  month      = may,
  year       = {2023},
  note       = {arXiv:2304.01196 [cs]},
  keywords   = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language}
}


@languageresource{coucke2018snips,
  title   = {Snips voice platform: an embedded spoken language understanding system for private-by-design voice interfaces},
  author  = {Coucke, Alice and Saade, Alaa and Ball, Adrien and Bluche, Th{\'e}odore and Caulier, Alexandre and Leroy, David and Doumouro, Cl{\'e}ment and Gisselbrecht, Thibault and Caltagirone, Francesco and Lavril, Thibaut and others},
  journal = {arXiv preprint arXiv:1805.10190},
  year    = {2018}
}

@languageresource{fitzgerald2022massive,
  title   = {Massive: A 1m-example multilingual natural language understanding dataset with 51 typologically-diverse languages},
  author  = {FitzGerald, Jack and Hench, Christopher and Peris, Charith and Mackie, Scott and Rottmann, Kay and Sanchez, Ana and Nash, Aaron and Urbach, Liam and Kakarala, Vishesh and Singh, Richa and others},
  journal = {arXiv preprint arXiv:2204.08582},
  year    = {2022}
}

@Languageresource{budzianowski2018multiwoz,
  author  = {Budzianowski, Pawe{\l} and Wen, Tsung-Hsien and Tseng, Bo-Hsiang and Casanueva, Inigo and Ultes, Stefan and Ramadan, Osman and Ga{\v{s}}i{\'c}, Milica},
  groups  = {SLU},
  journal = {arXiv preprint arXiv:1810.00278},
  title   = {Multiwoz--a large-scale multi-domain wizard-of-oz dataset for task-oriented dialogue modelling},
  year    = {2018},
}

@misc{zhang_enhancing_2023,
  title     = {Enhancing {Performance} on {Seen} and {Unseen} {Dialogue} {Scenarios} using {Retrieval}-{Augmented} {End}-to-{End} {Task}-{Oriented} {System}},
  url       = {http://arxiv.org/abs/2308.08169},
  doi       = {10.48550/arXiv.2308.08169},
  abstract  = {End-to-end task-oriented dialogue (TOD) systems have achieved promising performance by leveraging sophisticated natural language understanding and natural language generation capabilities of pre-trained models. This work enables the TOD systems with more flexibility through a simple cache. The cache provides the flexibility to dynamically update the TOD systems and handle both existing and unseen dialogue scenarios. Towards this end, we first fine-tune a retrieval module to effectively retrieve the most relevant information entries from the cache. We then train end-to-end TOD models that can refer to and ground on both dialogue history and retrieved information during TOD generation. The cache is straightforward to construct, and the backbone models of TOD systems are compatible with existing pre-trained generative models. Extensive experiments demonstrate the superior performance of our framework, with a notable improvement in non-empty joint goal accuracy by 6.7\% compared to strong baselines.},
  urldate   = {2023-10-30},
  publisher = {arXiv},
  author    = {Zhang, Jianguo and Roller, Stephen and Qian, Kun and Liu, Zhiwei and Meng, Rui and Heinecke, Shelby and Wang, Huan and Savarese, Silvio and Xiong, Caiming},
  month     = aug,
  year      = {2023},
  note      = {arXiv:2308.08169 [cs]},
  keywords  = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language}
}

@misc{feng_towards_2023,
  title     = {Towards {LLM}-driven {Dialogue} {State} {Tracking}},
  url       = {http://arxiv.org/abs/2310.14970},
  doi       = {10.48550/arXiv.2310.14970},
  abstract  = {Dialogue State Tracking (DST) is of paramount importance in ensuring accurate tracking of user goals and system actions within task-oriented dialogue systems. The emergence of large language models (LLMs) such as GPT3 and ChatGPT has sparked considerable interest in assessing their efficacy across diverse applications. In this study, we conduct an initial examination of ChatGPT's capabilities in DST. Our evaluation uncovers the exceptional performance of ChatGPT in this task, offering valuable insights to researchers regarding its capabilities and providing useful directions for designing and enhancing dialogue systems. Despite its impressive performance, ChatGPT has significant limitations including its closed-source nature, request restrictions, raising data privacy concerns, and lacking local deployment capabilities. To address these concerns, we present LDST, an LLM-driven DST framework based on smaller, open-source foundation models. By utilizing a novel domain-slot instruction tuning method, LDST achieves performance on par with ChatGPT. Comprehensive evaluations across three distinct experimental settings, we find that LDST exhibits remarkable performance improvements in both zero-shot and few-shot setting compared to previous SOTA methods. The source code is provided for reproducibility.},
  urldate   = {2023-10-30},
  publisher = {arXiv},
  author    = {Feng, Yujie and Lu, Zexin and Liu, Bo and Zhan, Liming and Wu, Xiao-Ming},
  month     = oct,
  year      = {2023},
  note      = {arXiv:2310.14970 [cs]},
  keywords  = {Computer Science - Computation and Language}
}

@misc{feng_fantastic_2023,
  title      = {Fantastic {Rewards} and {How} to {Tame} {Them}: {A} {Case} {Study} on {Reward} {Learning} for {Task}-oriented {Dialogue} {Systems}},
  shorttitle = {Fantastic {Rewards} and {How} to {Tame} {Them}},
  url        = {https://arxiv.org/abs/2302.10342v1},
  abstract   = {When learning task-oriented dialogue (ToD) agents, reinforcement learning (RL) techniques can naturally be utilized to train dialogue strategies to achieve user-specific goals. Prior works mainly focus on adopting advanced RL techniques to train the ToD agents, while the design of the reward function is not well studied. This paper aims at answering the question of how to efficiently learn and leverage a reward function for training end-to-end (E2E) ToD agents. Specifically, we introduce two generalized objectives for reward-function learning, inspired by the classical learning-to-rank literature. Further, we utilize the learned reward function to guide the training of the E2E ToD agent. With the proposed techniques, we achieve competitive results on the E2E response-generation task on the Multiwoz 2.0 dataset. Source code and checkpoints are publicly released at https://github.com/Shentao-YANG/Fantastic\_Reward\_ICLR2023.},
  language   = {en},
  urldate    = {2023-10-30},
  journal    = {arXiv.org},
  author     = {Feng, Yihao and Yang, Shentao and Zhang, Shujian and Zhang, Jianguo and Xiong, Caiming and Zhou, Mingyuan and Wang, Huan},
  month      = feb,
  year       = {2023}
}

@article{qin_modularized_2023,
  title    = {Modularized {Pre}-{Training} for {End}-to-{End} {Task}-{Oriented} {Dialogue}},
  volume   = {31},
  issn     = {2329-9304},
  url      = {https://ieeexplore.ieee.org/abstract/document/10043710},
  doi      = {10.1109/TASLP.2023.3244503},
  abstract = {Pre-training for end-to-end task-oriented dialogue systems (EToDs) is a challenging task due to its unique knowledge base query (accuracy) need and lack of sufficient training data (fluency). In this paper, we try to mitigate the above challenges by introducing a modularized pre-training framework for EToDs, which achieves to effectively improve both accuracy and fluency of EToDs through a pre-training paradigm. The core insight is a modular design by decomposing EToDs into a generation (fluency) module and a knowledge-retriever (accuracy) module, which allows us to optimize each module by pre-training these two sub-modules with different well-designed pre-training tasks, respectively. In addition, such a modularized paradigm enables us to make full use of large amounts of KB-free dialogue corpus for the pre-training generation module, which can alleviate the insufficient training problem. Furthermore, we introduce a new consistency-guided data augmentation (CGDA) strategy to cope with the data scarcity problem to better pre-train the knowledge-retriever module. Finally, we fine-tune the pre-trained generation module and knowledge-retriever module jointly. Experimental results on three datasets show that our model achieve superior performance in terms of both fluency and accuracy. To our knowledge, this is the first work to explore modularized pre-training methods for EToDs.},
  urldate  = {2023-10-30},
  journal  = {IEEE/ACM Transactions on Audio, Speech, and Language Processing},
  author   = {Qin, Libo and Xu, Xiao and Wang, Lehan and Zhang, Yue and Che, Wanxiang},
  year     = {2023},
  note     = {Conference Name: IEEE/ACM Transactions on Audio, Speech, and Language Processing},
  pages    = {1601--1610}
}

@article{chen_opal_2023,
  title      = {{OPAL}: {Ontology}-{Aware} {Pretrained} {Language} {Model} for {End}-to-{End} {Task}-{Oriented} {Dialogue}},
  volume     = {11},
  shorttitle = {{OPAL}},
  url        = {https://aclanthology.org/2023.tacl-1.5},
  doi        = {10.1162/tacl_a_00534},
  abstract   = {This paper presents an ontology-aware pretrained language model (OPAL) for end-to-end task-oriented dialogue (TOD). Unlike chit-chat dialogue models, task-oriented dialogue models fulfill at least two task-specific modules: Dialogue state tracker (DST) and response generator (RG). The dialogue state consists of the domain-slot-value triples, which are regarded as the user's constraints to search the domain-related databases. The large-scale task-oriented dialogue data with the annotated structured dialogue state usually are inaccessible. It prevents the development of the pretrained language model for the task-oriented dialogue. We propose a simple yet effective pretraining method to alleviate this problem, which consists of two pretraining phases. The first phase is to pretrain on large-scale contextual text data, where the structured information of the text is extracted by the information extracting tool. To bridge the gap between the pretraining method and downstream tasks, we design two pretraining tasks: ontology-like triple recovery and next-text generation, which simulates the DST and RG, respectively. The second phase is to fine-tune the pretrained model on the TOD data. The experimental results show that our proposed method achieves an exciting boost and obtains competitive performance even without any TOD data on CamRest676 and MultiWOZ benchmarks.},
  urldate    = {2023-10-30},
  journal    = {Transactions of the Association for Computational Linguistics},
  author     = {Chen, Zhi and Liu, Yuncong and Chen, Lu and Zhu, Su and Wu, Mengyue and Yu, Kai},
  year       = {2023},
  note       = {Place: Cambridge, MA
                Publisher: MIT Press},
  pages      = {68--84}
}

@misc{hung_ds-tod_2021,
  title      = {{DS}-{TOD}: {Efficient} {Domain} {Specialization} for {Task} {Oriented} {Dialog}},
  shorttitle = {{DS}-{TOD}},
  url        = {https://arxiv.org/abs/2110.08395v2},
  abstract   = {Recent work has shown that self-supervised dialog-specific pretraining on large conversational datasets yields substantial gains over traditional language modeling (LM) pretraining in downstream task-oriented dialog (TOD). These approaches, however, exploit general dialogic corpora (e.g., Reddit) and thus presumably fail to reliably embed domain-specific knowledge useful for concrete downstream TOD domains. In this work, we investigate the effects of domain specialization of pretrained language models (PLMs) for TOD. Within our DS-TOD framework, we first automatically extract salient domain-specific terms, and then use them to construct DomainCC and DomainReddit -- resources that we leverage for domain-specific pretraining, based on (i) masked language modeling (MLM) and (ii) response selection (RS) objectives, respectively. We further propose a resource-efficient and modular domain specialization by means of domain adapters -- additional parameter-light layers in which we encode the domain knowledge. Our experiments with prominent TOD tasks -- dialog state tracking (DST) and response retrieval (RR) -- encompassing five domains from the MultiWOZ benchmark demonstrate the effectiveness of DS-TOD. Moreover, we show that the light-weight adapter-based specialization (1) performs comparably to full fine-tuning in single domain setups and (2) is particularly suitable for multi-domain specialization, where besides advantageous computational footprint, it can offer better TOD performance.},
  language   = {en},
  urldate    = {2023-10-09},
  journal    = {arXiv.org},
  author     = {Hung, Chia-Chien and Lauscher, Anne and Ponzetto, Simone Paolo and Glavaš, Goran},
  month      = oct,
  year       = {2021}
}

@inproceedings{li_prefix-tuning_2021,
  address    = {Online},
  title      = {Prefix-{Tuning}: {Optimizing} {Continuous} {Prompts} for {Generation}},
  shorttitle = {Prefix-{Tuning}},
  url        = {https://aclanthology.org/2021.acl-long.353},
  doi        = {10.18653/v1/2021.acl-long.353},
  abstract   = {Fine-tuning is the de facto way of leveraging large pretrained language models for downstream tasks. However, fine-tuning modifies all the language model parameters and therefore necessitates storing a full copy for each task. In this paper, we propose prefix-tuning, a lightweight alternative to fine-tuning for natural language generation tasks, which keeps language model parameters frozen and instead optimizes a sequence of continuous task-specific vectors, which we call the prefix. Prefix-tuning draws inspiration from prompting for language models, allowing subsequent tokens to attend to this prefix as if it were “virtual tokens”. We apply prefix-tuning to GPT-2 for table-to-text generation and to BART for summarization. We show that by learning only 0.1\% of the parameters, prefix-tuning obtains comparable performance in the full data setting, outperforms fine-tuning in low-data settings, and extrapolates better to examples with topics that are unseen during training.},
  urldate    = {2023-10-09},
  booktitle  = {Proceedings of the 59th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics} and the 11th {International} {Joint} {Conference} on {Natural} {Language} {Processing} ({Volume} 1: {Long} {Papers})},
  publisher  = {Association for Computational Linguistics},
  author     = {Li, Xiang Lisa and Liang, Percy},
  month      = aug,
  year       = {2021},
  pages      = {4582--4597}
}

@inproceedings{shafi_fine-tuned_2023,
  title     = {Fine-{Tuned} {BERT} with {Attention}-{Based} {Bi}-{GRU}-{CapsNet} {Framework} for {Joint} {Intent} {Recognition} and {Slot} {Filing}},
  url       = {https://ieeexplore.ieee.org/abstract/document/10141744?casa_token=l3NnhH4Ab6QAAAAA:FEneagq56UQwaY-V3Als9BFaJzLxdrLgxgBuQO6ZgjjAXwoVuslWFpkS3AeEHxds7bYUnBWjzwo},
  doi       = {10.1109/InCACCT57535.2023.10141744},
  abstract  = {In the field of natural language processing (NLP), the two most prominent research areas are slot tagging and intent recognition. Modern joint learning strategies examine the link between slot-tag identification and intent classification, utilising the shared knowledge between the two tasks for their collective advantage.However, methods that combine various variants of pre-trained Bidirectional Encoder Representations from Transformers [BERT] models with attention based capsule networks for joint slot-tag identification and intent detection have not been fully explored.This study proposes a multi-stage framework trained on different versions of BERT models ({\textbackslash}mathrmB{\textbackslash}mathrmE{\textbackslash}mathrmR{\textbackslash}mathrmT\_{\textbackslash}mathrmb{\textbackslash}mathrma{\textbackslash}mathrms{\textbackslash}mathrme and {\textbackslash}mathrmB{\textbackslash}mathrmE{\textbackslash}mathrmR{\textbackslash}mathrmT\_{\textbackslash}mathrml{\textbackslash}mathrma{\textbackslash}mathrmr{\textbackslash}mathrmg{\textbackslash}mathrme) with Bidirectional Gated Recurrent Unit [Bi-GRU] and self attention mechanism as intent detection decoder to capture the underlying information and to discover the explicit links.While the capsule network, in accordance with the dynamic routing algorithm, acts as a slot filler decoder in predicting intents and slots and representing the semantic and syntactic relationships. The experimental findings demonstrate that the proposed approach enhances semantic frame accuracy at the sentence level, outperforming various baseline methodologies by a significant margin with a 1.2\% improvement in the intent Flscore and 3.24\% in the slot Fl-score, relative to the previous state-of-the-art models on the SNIPS datasets.},
  urldate   = {2023-10-08},
  booktitle = {2023 {International} {Conference} on {Advancement} in {Computation} \& {Computer} {Technologies} ({InCACCT})},
  author    = {Shafi, Nahida and Chachoo, Manzoor Ahmed},
  month     = may,
  year      = {2023},
  pages     = {369--374}
}

@inproceedings{fitzgerald_alexa_2022,
  address    = {New York, NY, USA},
  series     = {{KDD} '22},
  title      = {Alexa {Teacher} {Model}: {Pretraining} and {Distilling} {Multi}-{Billion}-{Parameter} {Encoders} for {Natural} {Language} {Understanding} {Systems}},
  isbn       = {978-1-4503-9385-0},
  shorttitle = {Alexa {Teacher} {Model}},
  url        = {https://dl.acm.org/doi/10.1145/3534678.3539173},
  doi        = {10.1145/3534678.3539173},
  abstract   = {We present results from a large-scale experiment on pretraining encoders with non-embedding parameter counts ranging from 700M to 9.3B, their subsequent distillation into smaller models ranging from 17M-170M parameters, and their application to the Natural Language Understanding (NLU) component of a virtual assistant system. Though we train using 70\% spoken-form data, our teacher models perform comparably to XLM-R and mT5 when evaluated on the written-form Cross-lingual Natural Language Inference (XNLI) corpus. We perform a second stage of pretraining on our teacher models using in-domain data from our system, improving error rates by 3.86\% relative for intent classification and 7.01\% relative for slot filling. We find that even a 170M-parameter model distilled from our Stage 2 teacher model has 2.88\% better intent classification and 7.69\% better slot filling error rates when compared to the 2.3B-parameter teacher trained only on public data (Stage 1), emphasizing the importance of in-domain data for pretraining. When evaluated offline using labeled NLU data, our 17M-parameter Stage 2 distilled model outperforms both XLM-R Base (85M params) and DistillBERT (42M params) by 4.23\% to 6.14\%, respectively. Finally, we present results from a full virtual assistant experimentation platform, where we find that models trained using our pretraining and distillation pipeline outperform models distilled from 85M-parameter teachers by 3.74\%-4.91\% on an automatic measurement of full-system user dissatisfaction.},
  urldate    = {2023-10-08},
  booktitle  = {Proceedings of the 28th {ACM} {SIGKDD} {Conference} on {Knowledge} {Discovery} and {Data} {Mining}},
  publisher  = {Association for Computing Machinery},
  author     = {FitzGerald, Jack and Ananthakrishnan, Shankar and Arkoudas, Konstantine and Bernardi, Davide and Bhagia, Abhishek and Delli Bovi, Claudio and Cao, Jin and Chada, Rakesh and Chauhan, Amit and Chen, Luoxin and Dwarakanath, Anurag and Dwivedi, Satyam and Gojayev, Turan and Gopalakrishnan, Karthik and Gueudre, Thomas and Hakkani-Tur, Dilek and Hamza, Wael and Hüser, Jonathan J. and Jose, Kevin Martin and Khan, Haidar and Liu, Beiye and Lu, Jianhua and Manzotti, Alessandro and Natarajan, Pradeep and Owczarzak, Karolina and Oz, Gokmen and Palumbo, Enrico and Peris, Charith and Prakash, Chandana Satya and Rawls, Stephen and Rosenbaum, Andy and Shenoy, Anjali and Soltan, Saleh and Sridhar, Mukund Harakere and Tan, Lizhen and Triefenbach, Fabian and Wei, Pan and Yu, Haiyang and Zheng, Shuai and Tur, Gokhan and Natarajan, Prem},
  month      = aug,
  year       = {2022},
  keywords   = {distributed training, knowledge distillation, model pretraining, natural language understanding, self-attention, transformers, virtual assistant, voice a.i.},
  pages      = {2893--2902}
}

@article{han_ptr_2022,
  title      = {{PTR}: {Prompt} {Tuning} with {Rules} for {Text} {Classification}},
  volume     = {3},
  issn       = {2666-6510},
  shorttitle = {{PTR}},
  url        = {https://www.sciencedirect.com/science/article/pii/S2666651022000183},
  doi        = {10.1016/j.aiopen.2022.11.003},
  abstract   = {Recently, prompt tuning has been widely applied to stimulate the rich knowledge in pre-trained language models (PLMs) to serve NLP tasks. Although prompt tuning has achieved promising results on some few-class classification tasks, such as sentiment classification and natural language inference, manually designing prompts is cumbersome. Meanwhile, generating prompts automatically is also difficult and time-consuming. Therefore, obtaining effective prompts for complex many-class classification tasks still remains a challenge. In this paper, we propose to encode the prior knowledge of a classification task into rules, then design sub-prompts according to the rules, and finally combine the sub-prompts to handle the task. We name this Prompt Tuning method with Rules “PTR”. Compared with existing prompt-based methods, PTR achieves a good trade-off between effectiveness and efficiency in building prompts. We conduct experiments on three many-class classification tasks, including relation classification, entity typing, and intent classification. The results show that PTR outperforms both vanilla and prompt tuning baselines, indicating the effectiveness of utilizing rules for prompt tuning. The source code of PTR is available at https://github.com/thunlp/PTR.},
  urldate    = {2023-10-08},
  journal    = {AI Open},
  author     = {Han, Xu and Zhao, Weilin and Ding, Ning and Liu, Zhiyuan and Sun, Maosong},
  month      = jan,
  year       = {2022},
  keywords   = {Pre-trained language models, Prompt tuning},
  pages      = {182--192}
}

@misc{budzianowski_multiwoz_2020,
  title     = {{MultiWOZ} -- {A} {Large}-{Scale} {Multi}-{Domain} {Wizard}-of-{Oz} {Dataset} for {Task}-{Oriented} {Dialogue} {Modelling}},
  url       = {http://arxiv.org/abs/1810.00278},
  doi       = {10.48550/arXiv.1810.00278},
  abstract  = {Even though machine learning has become the major scene in dialogue research community, the real breakthrough has been blocked by the scale of data available. To address this fundamental obstacle, we introduce the Multi-Domain Wizard-of-Oz dataset (MultiWOZ), a fully-labeled collection of human-human written conversations spanning over multiple domains and topics. At a size of \$10\$k dialogues, it is at least one order of magnitude larger than all previous annotated task-oriented corpora. The contribution of this work apart from the open-sourced dataset labelled with dialogue belief states and dialogue actions is two-fold: firstly, a detailed description of the data collection procedure along with a summary of data structure and analysis is provided. The proposed data-collection pipeline is entirely based on crowd-sourcing without the need of hiring professional annotators; secondly, a set of benchmark results of belief tracking, dialogue act and response generation is reported, which shows the usability of the data and sets a baseline for future studies.},
  urldate   = {2023-10-08},
  publisher = {arXiv},
  author    = {Budzianowski, Paweł and Wen, Tsung-Hsien and Tseng, Bo-Hsiang and Casanueva, Iñigo and Ultes, Stefan and Ramadan, Osman and Gašić, Milica},
  month     = apr,
  year      = {2020},
  note      = {arXiv:1810.00278 [cs]},
  keywords  = {Computer Science - Computation and Language}
}

% and : and Teven Le Scao and Angela Fan and Christopher Akiki and Ellie Pavlick and Suzana Ilić and Daniel Hesslow and Roman Castagné and Alexandra Sasha Luccioni and François Yvon and Matthias Gallé and Jonathan Tow and Alexander M. Rush and Stella Biderman and Albert Webson and Pawan Sasanka Ammanamanchi and Thomas Wang and Benoît Sagot and Niklas Muennighoff and Albert Villanova del Moral and Olatunji Ruwase and Rachel Bawden and Stas Bekman and Angelina McMillan-Major and Iz Beltagy and Huu Nguyen and Lucile Saulnier and Samson Tan and Pedro Ortiz Suarez and Victor Sanh and Hugo Laurençon and Yacine Jernite and Julien Launay and Margaret Mitchell and Colin Raffel and Aaron Gokaslan and Adi Simhi and Aitor Soroa and Alham Fikri Aji and Amit Alfassy and Anna Rogers and Ariel Kreisberg Nitzav and Canwen Xu and Chenghao Mou and Chris Emezue and Christopher Klamm and Colin Leong and Daniel van Strien and David Ifeoluwa Adelani and Dragomir Radev and Eduardo González Ponferrada and Efrat Levkovizh and Ethan Kim and Eyal Bar Natan and Francesco De Toni and Gérard Dupont and Germán Kruszewski and Giada Pistilli and Hady Elsahar and Hamza Benyamina and Hieu Tran and Ian Yu and Idris Abdulmumin and Isaac Johnson and Itziar Gonzalez-Dios and Javier de la Rosa and Jenny Chim and Jesse Dodge and Jian Zhu and Jonathan Chang and Jörg Frohberg and Joseph Tobing and Joydeep Bhattacharjee and Khalid Almubarak and Kimbo Chen and Kyle Lo and Leandro Von Werra and Leon Weber and Long Phan and Loubna Ben allal and Ludovic Tanguy and Manan Dey and Manuel Romero Muñoz and Maraim Masoud and María Grandury and Mario Šaško and Max Huang and Maximin Coavoux and Mayank Singh and Mike Tian-Jian Jiang and Minh Chien Vu and Mohammad A. Jauhar and Mustafa Ghaleb and Nishant Subramani and Nora Kassner and Nurulaqilla Khamis and Olivier Nguyen and Omar Espejel and Ona de Gibert and Paulo Villegas and Peter Henderson and Pierre Colombo and Priscilla Amuok and Quentin Lhoest and Rheza Harliman and Rishi Bommasani and Roberto Luis López and Rui Ribeiro and Salomey Osei and Sampo Pyysalo and Sebastian Nagel and Shamik Bose and Shamsuddeen Hassan Muhammad and Shanya Sharma and Shayne Longpre and Somaieh Nikpoor and Stanislav Silberberg and Suhas Pai and Sydney Zink and Tiago Timponi Torrent and Timo Schick and Tristan Thrush and Valentin Danchev and Vassilina Nikoulina and Veronika Laippala and Violette Lepercq and Vrinda Prabhu and Zaid Alyafeai and Zeerak Talat and Arun Raja and Benjamin Heinzerling and Chenglei Si and Davut Emre Taşar and Elizabeth Salesky and Sabrina J. Mielke and Wilson Y. Lee and Abheesht Sharma and Andrea Santilli and Antoine Chaffin and Arnaud Stiegler and Debajyoti Datta and Eliza Szczechla and Gunjan Chhablani and Han Wang and Harshit Pandey and Hendrik Strobelt and Jason Alan Fries and Jos Rozen and Leo Gao and Lintang Sutawika and M Saiful Bari and Maged S. Al-shaibani and Matteo Manica and Nihal Nayak and Ryan Teehan and Samuel Albanie and Sheng Shen and Srulik Ben-David and Stephen H. Bach and Taewoon Kim and Tali Bers and Thibault Fevry and Trishala Neeraj and Urmish Thakker and Vikas Raunak and Xiangru Tang and Zheng-Xin Yong and Zhiqing Sun and Shaked Brody and Yallow Uri and Hadar Tojarieh and Adam Roberts and Hyung Won Chung and Jaesung Tae and Jason Phang and Ofir Press and Conglong Li and Deepak Narayanan and Hatim Bourfoune and Jared Casper and Jeff Rasley and Max Ryabinin and Mayank Mishra and Minjia Zhang and Mohammad Shoeybi and Myriam Peyrounette and Nicolas Patry and Nouamane Tazi and Omar Sanseviero and Patrick von Platen and Pierre Cornette and Pierre François Lavallée and Rémi Lacroix and Samyam Rajbhandari and Sanchit Gandhi and Shaden Smith and Stéphane Requena and Suraj Patil and Tim Dettmers and Ahmed Baruwa and Amanpreet Singh and Anastasia Cheveleva and Anne-Laure Ligozat and Arjun Subramonian and Aurélie Névéol and Charles Lovering and Dan Garrette and Deepak Tunuguntla and Ehud Reiter and Ekaterina Taktasheva and Ekaterina Voloshina and Eli Bogdanov and Genta Indra Winata and Hailey Schoelkopf and Jan-Christoph Kalo and Jekaterina Novikova and Jessica Zosa Forde and Jordan Clive and Jungo Kasai and Ken Kawamura and Liam Hazan and Marine Carpuat and Miruna Clinciu and Najoung Kim and Newton Cheng and Oleg Serikov and Omer Antverg and Oskar van der Wal and Rui Zhang and Ruochen Zhang and Sebastian Gehrmann and Shachar Mirkin and Shani Pais and Tatiana Shavrina and Thomas Scialom and Tian Yun and Tomasz Limisiewicz and Verena Rieser and Vitaly Protasov and Vladislav Mikhailov and Yada Pruksachatkun and Yonatan Belinkov and Zachary Bamberger and Zdeněk Kasner and Alice Rueda and Amanda Pestana and Amir Feizpour and Ammar Khan and Amy Faranak and Ana Santos and Anthony Hevia and Antigona Unldreaj and Arash Aghagol and Arezoo Abdollahi and Aycha Tammour and Azadeh HajiHosseini and Bahareh Behroozi and Benjamin Ajibade and Bharat Saxena and Carlos Muñoz Ferrandis and Daniel McDuff and Danish Contractor and David Lansky and Davis David and Douwe Kiela and Duong A. Nguyen and Edward Tan and Emi Baylor and Ezinwanne Ozoani and Fatima Mirza and Frankline Ononiwu and Habib Rezanejad and Hessie Jones and Indrani Bhattacharya and Irene Solaiman and Irina Sedenko and Isar Nejadgholi and Jesse Passmore and Josh Seltzer and Julio Bonis Sanz and Livia Dutra and Mairon Samagaio and Maraim Elbadri and Margot Mieskes and Marissa Gerchick and Martha Akinlolu and Michael McKenna and Mike Qiu and Muhammed Ghauri and Mykola Burynok and Nafis Abrar and Nazneen Rajani and Nour Elkott and Nour Fahmy and Olanrewaju Samuel and Ran An and Rasmus Kromann and Ryan Hao and Samira Alizadeh and Sarmad Shubber and Silas Wang and Sourav Roy and Sylvain Viguier and Thanh Le and Tobi Oyebade and Trieu Le and Yoyo Yang and Zach Nguyen and Abhinav Ramesh Kashyap and Alfredo Palasciano and Alison Callahan and Anima Shukla and Antonio Miranda-Escalada and Ayush Singh and Benjamin Beilharz and Bo Wang and Caio Brito and Chenxi Zhou and Chirag Jain and Chuxin Xu and Clémentine Fourrier and Daniel León Periñán and Daniel Molano and Dian Yu and Enrique Manjavacas and Fabio Barth and Florian Fuhrimann and Gabriel Altay and Giyaseddin Bayrak and Gully Burns and Helena U. Vrabec and Imane Bello and Ishani Dash and Jihyun Kang and John Giorgi and Jonas Golde and Jose David Posada and Karthik Rangasai Sivaraman and Lokesh Bulchandani and Lu Liu and Luisa Shinzato and Madeleine Hahn de Bykhovetz and Maiko Takeuchi and Marc Pàmies and Maria A Castillo and Marianna Nezhurina and Mario Sänger and Matthias Samwald and Michael Cullan and Michael Weinberg and Michiel De Wolf and Mina Mihaljcic and Minna Liu and Moritz Freidank and Myungsun Kang and Natasha Seelam and Nathan Dahlberg and Nicholas Michio Broad and Nikolaus Muellner and Pascale Fung and Patrick Haller and Ramya Chandrasekhar and Renata Eisenberg and Robert Martin and Rodrigo Canalli and Rosaline Su and Ruisi Su and Samuel Cahyawijaya and Samuele Garda and Shlok S Deshmukh and Shubhanshu Mishra and Sid Kiblawi and Simon Ott and Sinee Sang-aroonsiri and Srishti Kumar and Stefan Schweter and Sushil Bharati and Tanmay Laud and Théo Gigant and Tomoya Kainuma and Wojciech Kusa and Yanis Labrak and Yash Shailesh Bajaj and Yash Venkatraman and Yifan Xu and Yingxin Xu and Yu Xu and Zhe Tan and Zhongli Xie and Zifan Ye and Mathilde Bras and Younes Belkada and Thomas Wolf
@misc{workshop2023bloom,
  title         = {BLOOM: A 176B-Parameter Open-Access Multilingual Language Model},
  author        = {BigScience Workshop},
  year          = {2023},
  eprint        = {2211.05100},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL}
}

@article{muennighoff2022crosslingual,
  title   = {Crosslingual generalization through multitask finetuning},
  author  = {Muennighoff, Niklas and Wang, Thomas and Sutawika, Lintang and Roberts, Adam and Biderman, Stella and Scao, Teven Le and Bari, M Saiful and Shen, Sheng and Yong, Zheng-Xin and Schoelkopf, Hailey and others},
  journal = {arXiv preprint arXiv:2211.01786},
  year    = {2022}
}

@misc{abdin2024phi3,
  title         = {Phi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone},
  author        = {Marah Abdin and Sam Ade Jacobs and Ammar Ahmad Awan and Jyoti Aneja and Ahmed Awadallah and Hany Awadalla and Nguyen Bach and Amit Bahree and Arash Bakhtiari and Jianmin Bao and Harkirat Behl and Alon Benhaim and Misha Bilenko and Johan Bjorck and Sébastien Bubeck and Qin Cai and Martin Cai and Caio César Teodoro Mendes and Weizhu Chen and Vishrav Chaudhary and Dong Chen and Dongdong Chen and Yen-Chun Chen and Yi-Ling Chen and Parul Chopra and Xiyang Dai and Allie Del Giorno and Gustavo de Rosa and Matthew Dixon and Ronen Eldan and Victor Fragoso and Dan Iter and Mei Gao and Min Gao and Jianfeng Gao and Amit Garg and Abhishek Goswami and Suriya Gunasekar and Emman Haider and Junheng Hao and Russell J. Hewett and Jamie Huynh and Mojan Javaheripi and Xin Jin and Piero Kauffmann and Nikos Karampatziakis and Dongwoo Kim and Mahoud Khademi and Lev Kurilenko and James R. Lee and Yin Tat Lee and Yuanzhi Li and Yunsheng Li and Chen Liang and Lars Liden and Ce Liu and Mengchen Liu and Weishung Liu and Eric Lin and Zeqi Lin and Chong Luo and Piyush Madan and Matt Mazzola and Arindam Mitra and Hardik Modi and Anh Nguyen and Brandon Norick and Barun Patra and Daniel Perez-Becker and Thomas Portet and Reid Pryzant and Heyang Qin and Marko Radmilac and Corby Rosset and Sambudha Roy and Olatunji Ruwase and Olli Saarikivi and Amin Saied and Adil Salim and Michael Santacroce and Shital Shah and Ning Shang and Hiteshi Sharma and Swadheen Shukla and Xia Song and Masahiro Tanaka and Andrea Tupini and Xin Wang and Lijuan Wang and Chunyu Wang and Yu Wang and Rachel Ward and Guanhua Wang and Philipp Witte and Haiping Wu and Michael Wyatt and Bin Xiao and Can Xu and Jiahang Xu and Weijian Xu and Sonali Yadav and Fan Yang and Jianwei Yang and Ziyi Yang and Yifan Yang and Donghan Yu and Lu Yuan and Chengruidong Zhang and Cyril Zhang and Jianwen Zhang and Li Lyna Zhang and Yi Zhang and Yue Zhang and Yunan Zhang and Xiren Zhou},
  year          = {2024},
  eprint        = {2404.14219},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL}
}

@article{llama3modelcard,
  title  = {Llama 3 Model Card},
  author = {AI@Meta},
  year   = {2024},
  url    = {https://github.com/meta-llama/llama3/blob/main/MODEL_CARD.md}
}

@inproceedings{alavoine:hal-04523286,
  title       = {{New Semantic Task for the French Spoken Language Understanding MEDIA Benchmark}},
  author      = {Alavoine, Nad{\`e}ge and Laperriere, Ga{\"e}lle and Servan, Christophe and Ghannay, Sahar and Rosset, Sophie},
  url         = {https://hal.science/hal-04523286},
  booktitle   = {{The 2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation (LREC-COLING 2024)}},
  address     = {Torino, Italy},
  series      = {The 2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation (LREC-COLING 2024)},
  year        = {2024},
  month       = May,
  keywords    = {Benchmark Dataset ; Spoken Language Understanding ; Joint Intent Detection And Slot-filling ; Tri-training},
  pdf         = {https://hal.science/hal-04523286/file/main.pdf},
  hal_id      = {hal-04523286},
  hal_version = {v1}
}

@inproceedings{bonneaumaynard05_interspeech,
  author    = {H. Bonneau-Maynard and Sophie Rosset and C. Ayache and A. Kuhn and Djamel Mostefa},
  title     = {{Semantic annotation of the French media dialog corpus}},
  year      = 2005,
  booktitle = {Proc. Interspeech 2005},
  pages     = {3457--3460},
  doi       = {10.21437/Interspeech.2005-312},
  issn      = {2308-457X}
}

@misc{fitzgerald_massive_2022,
  title      = {{MASSIVE}: {A} {1M}-{Example} {Multilingual} {Natural} {Language} {Understanding} {Dataset} with 51 {Typologically}-{Diverse} {Languages}},
  shorttitle = {{MASSIVE}},
  url        = {https://arxiv.org/abs/2204.08582v2},
  abstract   = {We present the MASSIVE dataset--Multilingual Amazon Slu resource package (SLURP) for Slot-filling, Intent classification, and Virtual assistant Evaluation. MASSIVE contains 1M realistic, parallel, labeled virtual assistant utterances spanning 51 languages, 18 domains, 60 intents, and 55 slots. MASSIVE was created by tasking professional translators to localize the English-only SLURP dataset into 50 typologically diverse languages from 29 genera. We also present modeling results on XLM-R and mT5, including exact match accuracy, intent classification accuracy, and slot-filling F1 score. We have released our dataset, modeling code, and models publicly.},
  language   = {en},
  urldate    = {2023-10-08},
  journal    = {arXiv.org},
  author     = {FitzGerald, Jack and Hench, Christopher and Peris, Charith and Mackie, Scott and Rottmann, Kay and Sanchez, Ana and Nash, Aaron and Urbach, Liam and Kakarala, Vishesh and Singh, Richa and Ranganath, Swetha and Crist, Laurie and Britan, Misha and Leeuwis, Wouter and Tur, Gokhan and Natarajan, Prem},
  month      = apr,
  year       = {2022}
}

@misc{coucke_snips_2018,
  title      = {Snips {Voice} {Platform}: an embedded {Spoken} {Language} {Understanding} system for private-by-design voice interfaces},
  shorttitle = {Snips {Voice} {Platform}},
  url        = {https://arxiv.org/abs/1805.10190v3},
  abstract   = {This paper presents the machine learning architecture of the Snips Voice Platform, a software solution to perform Spoken Language Understanding on microprocessors typical of IoT devices. The embedded inference is fast and accurate while enforcing privacy by design, as no personal user data is ever collected. Focusing on Automatic Speech Recognition and Natural Language Understanding, we detail our approach to training high-performance Machine Learning models that are small enough to run in real-time on small devices. Additionally, we describe a data generation procedure that provides sufficient, high-quality training data without compromising user privacy.},
  language   = {en},
  urldate    = {2023-10-08},
  journal    = {arXiv.org},
  author     = {Coucke, Alice and Saade, Alaa and Ball, Adrien and Bluche, Théodore and Caulier, Alexandre and Leroy, David and Doumouro, Clément and Gisselbrecht, Thibault and Caltagirone, Francesco and Lavril, Thibaut and Primet, Maël and Dureau, Joseph},
  month      = may,
  year       = {2018}
}

@article{radford_improving_nodate,
  title    = {Improving {Language} {Understanding} by {Generative} {Pre}-{Training}},
  abstract = {Natural language understanding comprises a wide range of diverse tasks such as textual entailment, question answering, semantic similarity assessment, and document classiﬁcation. Although large unlabeled text corpora are abundant, labeled data for learning these speciﬁc tasks is scarce, making it challenging for discriminatively trained models to perform adequately. We demonstrate that large gains on these tasks can be realized by generative pre-training of a language model on a diverse corpus of unlabeled text, followed by discriminative ﬁne-tuning on each speciﬁc task. In contrast to previous approaches, we make use of task-aware input transformations during ﬁne-tuning to achieve effective transfer while requiring minimal changes to the model architecture. We demonstrate the effectiveness of our approach on a wide range of benchmarks for natural language understanding. Our general task-agnostic model outperforms discriminatively trained models that use architectures speciﬁcally crafted for each task, signiﬁcantly improving upon the state of the art in 9 out of the 12 tasks studied. For instance, we achieve absolute improvements of 8.9\% on commonsense reasoning (Stories Cloze Test), 5.7\% on question answering (RACE), and 1.5\% on textual entailment (MultiNLI).},
  language = {en},
  author   = {Radford, Alec and Narasimhan, Karthik and Salimans, Tim and Sutskever, Ilya}
}

@article{Groeneveld2023OLMo,
  title   = {OLMo: Accelerating the Science of Language Models},
  author  = {Groeneveld, Dirk and Beltagy, Iz and Walsh, Pete and Bhagia, Akshita and Kinney, Rodney and Tafjord, Oyvind and Jha, Ananya Harsh and Ivison, Hamish and Magnusson, Ian and Wang, Yizhong and Arora, Shane and Atkinson, David and Authur, Russell and Chandu, Khyathi and Cohan, Arman and Dumas, Jennifer and Elazar, Yanai and Gu, Yuling and Hessel, Jack and Khot, Tushar and Merrill, William and Morrison, Jacob and Muennighoff, Niklas and Naik, Aakanksha and Nam, Crystal and Peters, Matthew E. and Pyatkin, Valentina and Ravichander, Abhilasha and Schwenk, Dustin and Shah, Saurabh and Smith, Will and Subramani, Nishant and Wortsman, Mitchell and Dasigi, Pradeep and Lambert, Nathan and Richardson, Kyle and Dodge, Jesse and Lo, Kyle and Soldaini, Luca and Smith, Noah A. and Hajishirzi, Hannaneh},
  journal = {Preprint},
  year    = {2024}
}

@misc{noauthor_38993608_nodate,
  title    = {38993608 · {Efficient} {Few}-{Shot} {Learning} {Without} {Prompts}},
  url      = {https://slideslive.com/embed/presentation/38993608?embed_parent_url=https%3A%2F%2Fnips.cc%2Fvirtual%2F2022%2F59466&embed_origin=https%3A%2F%2Fnips.cc&embed_container_id=presentation-embed-38993608&auto_load=true&auto_play=false&zoom_ratio=&disable_fullscreen=false&locale=en&vertical_enabled=true&vertical_enabled_on_mobile=false&allow_hidden_controls_when_paused=true&fit_to_viewport=true&custom_user_id=&user_uuid=421bd679-642d-4ed8-b63d-1163e03f52fc},
  abstract = {Professional Conference Recording},
  language = {en-US},
  urldate  = {2023-09-29},
  journal  = {SlidesLive}
}

@misc{faysse2024croissantllm,
  title         = {CroissantLLM: A Truly Bilingual French-English Language Model},
  author        = {Manuel Faysse and Patrick Fernandes and Nuno M. Guerreiro and António Loison and Duarte M. Alves and Caio Corro and Nicolas Boizard and João Alves and Ricardo Rei and Pedro H. Martins and Antoni Bigata Casademunt and François Yvon and André F. T. Martins and Gautier Viaud and Céline Hudelot and Pierre Colombo},
  year          = {2024},
  eprint        = {2402.00786},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL}
}

@misc{liu_ia3_2022,
  title     = {{IA3}: {Few}-{Shot} {Parameter}-{Efficient} {Fine}-{Tuning} is {Better} and {Cheaper} than {In}-{Context} {Learning}},
  url       = {http://arxiv.org/abs/2205.05638},
  doi       = {10.48550/arXiv.2205.05638},
  abstract  = {Few-shot in-context learning (ICL) enables pre-trained language models to perform a previously-unseen task without any gradient-based training by feeding a small number of training examples as part of the input. ICL incurs substantial computational, memory, and storage costs because it involves processing all of the training examples every time a prediction is made. Parameter-efficient fine-tuning (PEFT) (e.g. adapter modules, prompt tuning, sparse update methods, etc.) offers an alternative paradigm where a small set of parameters are trained to enable a model to perform the new task. In this paper, we rigorously compare few-shot ICL and PEFT and demonstrate that the latter offers better accuracy as well as dramatically lower computational costs. Along the way, we introduce a new PEFT method called (IA)\${\textasciicircum}3\$ that scales activations by learned vectors, attaining stronger performance while only introducing a relatively tiny amount of new parameters. We also propose a simple recipe based on the T0 model called T-Few that can be applied to new tasks without task-specific tuning or modifications. We validate the effectiveness of T-Few on completely unseen tasks by applying it to the RAFT benchmark, attaining super-human performance for the first time and outperforming the state-of-the-art by 6\% absolute. All of the code used in our experiments is publicly available.},
  urldate   = {2023-09-25},
  publisher = {arXiv},
  author    = {Liu, Haokun and Tam, Derek and Muqeeth, Mohammed and Mohta, Jay and Huang, Tenghao and Bansal, Mohit and Raffel, Colin},
  month     = aug,
  year      = {2022},
  note      = {arXiv:2205.05638 [cs]},
  keywords  = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning}
}

@misc{hosseini-asl_simple_2022,
  title     = {A {Simple} {Language} {Model} for {Task}-{Oriented} {Dialogue}},
  url       = {http://arxiv.org/abs/2005.00796},
  abstract  = {Task-oriented dialogue is often decomposed into three tasks: understanding user input, deciding actions, and generating a response. While such decomposition might suggest a dedicated model for each sub-task, we find a simple, unified approach leads to state-of-the-art performance on the MultiWOZ dataset. SimpleTOD is a simple approach to task-oriented dialogue that uses a single, causal language model trained on all sub-tasks recast as a single sequence prediction problem. This allows SimpleTOD to fully leverage transfer learning from pre-trained, open domain, causal language models such as GPT-2. SimpleTOD improves over the prior state-of-the-art in joint goal accuracy for dialogue state tracking, and our analysis reveals robustness to noisy annotations in this setting. SimpleTOD also improves the main metrics used to evaluate action decisions and response generation in an end-to-end setting: inform rate by 8.1 points, success rate by 9.7 points, and combined score by 7.2 points.},
  urldate   = {2023-09-19},
  publisher = {arXiv},
  author    = {Hosseini-Asl, Ehsan and McCann, Bryan and Wu, Chien-Sheng and Yavuz, Semih and Socher, Richard},
  month     = apr,
  year      = {2022},
  note      = {arXiv:2005.00796 [cs]},
  keywords  = {Computer Science - Computation and Language}
}

@article{ramakrishnan_long-term_2022,
  title      = {Long-term {Control} for {Dialogue} {Generation}: {Methods} and {Evaluation}},
  copyright  = {Creative Commons Attribution 4.0 International},
  shorttitle = {Long-term {Control} for {Dialogue} {Generation}},
  url        = {https://arxiv.org/abs/2205.07352},
  doi        = {10.48550/ARXIV.2205.07352},
  abstract   = {Current approaches for controlling dialogue response generation are primarily focused on high-level attributes like style, sentiment, or topic. In this work, we focus on constrained long-term dialogue generation, which involves more fine-grained control and requires a given set of control words to appear in generated responses. This setting requires a model to not only consider the generation of these control words in the immediate context, but also produce utterances that will encourage the generation of the words at some time in the (possibly distant) future. We define the problem of constrained long-term control for dialogue generation, identify gaps in current methods for evaluation, and propose new metrics that better measure long-term control. We also propose a retrieval-augmented method that improves performance of long-term controlled generation via logit modification techniques. We show through experiments on three task-oriented dialogue datasets that our metrics better assess dialogue control relative to current alternatives and that our method outperforms state-of-the-art constrained generation baselines.},
  urldate    = {2023-09-11},
  author     = {Ramakrishnan, Ramya and Narangodage, Hashan Buddhika and Schilman, Mauro and Weinberger, Kilian Q. and McDonald, Ryan},
  year       = {2022},
  keywords   = {Artificial Intelligence (cs.AI), Computation and Language (cs.CL), FOS: Computer and information sciences}
}

@article{chen_action-based_2021,
  title      = {Action-{Based} {Conversations} {Dataset}: {A} {Corpus} for {Building} {More} {In}-{Depth} {Task}-{Oriented} {Dialogue} {Systems}},
  copyright  = {Creative Commons Attribution 4.0 International},
  shorttitle = {Action-{Based} {Conversations} {Dataset}},
  url        = {https://arxiv.org/abs/2104.00783},
  doi        = {10.48550/ARXIV.2104.00783},
  abstract   = {Existing goal-oriented dialogue datasets focus mainly on identifying slots and values. However, customer support interactions in reality often involve agents following multi-step procedures derived from explicitly-defined company policies as well. To study customer service dialogue systems in more realistic settings, we introduce the Action-Based Conversations Dataset (ABCD), a fully-labeled dataset with over 10K human-to-human dialogues containing 55 distinct user intents requiring unique sequences of actions constrained by policies to achieve task success. We propose two additional dialog tasks, Action State Tracking and Cascading Dialogue Success, and establish a series of baselines involving large-scale, pre-trained language models on this dataset. Empirical results demonstrate that while more sophisticated networks outperform simpler models, a considerable gap (50.8\% absolute accuracy) still exists to reach human-level performance on ABCD.},
  urldate    = {2023-09-11},
  author     = {Chen, Derek and Chen, Howard and Yang, Yi and Lin, Alex and Yu, Zhou},
  year       = {2021},
  keywords   = {Computation and Language (cs.CL), FOS: Computer and information sciences}
}

@Misc{pan_preliminary_2023,
  author    = {Pan, Wenbo and Chen, Qiguang and Xu, Xiao and Che, Wanxiang and Qin, Libo},
  month     = apr,
  note      = {arXiv:2304.04256 [cs]},
  title     = {A {Preliminary} {Evaluation} of {ChatGPT} for {Zero}-shot {Dialogue} {Understanding}},
  year      = {2023},
  abstract  = {Zero-shot dialogue understanding aims to enable dialogue to track the user's needs without any training data, which has gained increasing attention. In this work, we investigate the understanding ability of ChatGPT for zero-shot dialogue understanding tasks including spoken language understanding (SLU) and dialogue state tracking (DST). Experimental results on four popular benchmarks reveal the great potential of ChatGPT for zero-shot dialogue understanding. In addition, extensive analysis shows that ChatGPT benefits from the multi-turn interactive prompt in the DST task but struggles to perform slot filling for SLU. Finally, we summarize several unexpected behaviors of ChatGPT in dialogue understanding tasks, hoping to provide some insights for future research on building zero-shot dialogue understanding systems with Large Language Models (LLMs).},
  doi       = {10.48550/arXiv.2304.04256},
  keywords  = {Computer Science - Computation and Language, To Read},
  publisher = {arXiv},
  url       = {http://arxiv.org/abs/2304.04256},
  urldate   = {2023-08-27},
}

@misc{weld_tri-level_2023,
  title     = {Tri-level {Joint} {Natural} {Language} {Understanding} for {Multi}-turn {Conversational} {Datasets}},
  url       = {http://arxiv.org/abs/2305.17729},
  doi       = {10.48550/arXiv.2305.17729},
  abstract  = {Natural language understanding typically maps single utterances to a dual level semantic frame, sentence level intent and slot labels at the word level. The best performing models force explicit interaction between intent detection and slot filling. We present a novel tri-level joint natural language understanding approach, adding domain, and explicitly exchange semantic information between all levels. This approach enables the use of multi-turn datasets which are a more natural conversational environment than single utterance. We evaluate our model on two multi-turn datasets for which we are the first to conduct joint slot-filling and intent detection. Our model outperforms state-of-the-art joint models in slot filling and intent detection on multi-turn data sets. We provide an analysis of explicit interaction locations between the layers. We conclude that including domain information improves model performance.},
  urldate   = {2023-08-27},
  publisher = {arXiv},
  author    = {Weld, Henry and Hu, Sijia and Long, Siqu and Poon, Josiah and Han, Soyeon Caren},
  month     = may,
  year      = {2023},
  note      = {arXiv:2305.17729 [cs]},
  keywords  = {Computer Science - Computation and Language}
}

@misc{tavares_task_2023,
  title     = {Task {Conditioned} {BERT} for {Joint} {Intent} {Detection} and {Slot}-filling},
  url       = {http://arxiv.org/abs/2308.06165},
  doi       = {10.48550/arXiv.2308.06165},
  abstract  = {Dialogue systems need to deal with the unpredictability of user intents to track dialogue state and the heterogeneity of slots to understand user preferences. In this paper we investigate the hypothesis that solving these challenges as one unified model will allow the transfer of parameter support data across the different tasks. The proposed principled model is based on a Transformer encoder, trained on multiple tasks, and leveraged by a rich input that conditions the model on the target inferences. Conditioning the Transformer encoder on multiple target inferences over the same corpus, i.e., intent and multiple slot types, allows learning richer language interactions than a single-task model would be able to. In fact, experimental results demonstrate that conditioning the model on an increasing number of dialogue inference tasks leads to improved results: on the MultiWOZ dataset, the joint intent and slot detection can be improved by 3.2{\textbackslash}\% by conditioning on intent, 10.8{\textbackslash}\% by conditioning on slot and 14.4{\textbackslash}\% by conditioning on both intent and slots. Moreover, on real conversations with Farfetch costumers, the proposed conditioned BERT can achieve high joint-goal and intent detection performance throughout a dialogue.},
  urldate   = {2023-08-27},
  publisher = {arXiv},
  author    = {Tavares, Diogo and Azevedo, Pedro and Semedo, David and Sousa, Ricardo and Magalhães, João},
  month     = aug,
  year      = {2023},
  note      = {arXiv:2308.06165 [cs]},
  keywords  = {Computer Science - Computation and Language, To Read}
}

@inproceedings{won_break_2023,
  address    = {Toronto, Canada},
  title      = {{BREAK}: {Breaking} the {Dialogue} {State} {Tracking} {Barrier} with {Beam} {Search} and {Re}-ranking},
  shorttitle = {{BREAK}},
  url        = {https://aclanthology.org/2023.acl-long.159},
  abstract   = {Despite the recent advances in dialogue state tracking (DST), the joint goal accuracy (JGA) of the existing methods on MultiWOZ 2.1 still remains merely 60\%. In our preliminary error analysis, we find that beam search produces a pool of candidates that is likely to include the correct dialogue state. Motivated by this observation, we introduce a novel framework, called BREAK (Beam search and RE-rAnKing), that achieves outstanding performance on DST. BREAK performs DST in two stages: (i) generating k-best dialogue state candidates with beam search and (ii) re-ranking the candidates to select the correct dialogue state. This simple yet powerful framework shows state-of-the-art performance on all versions of MultiWOZ and M2M datasets. Most notably, we push the joint goal accuracy to 80-90\% on MultiWOZ 2.1-2.4, which is an improvement of 23.6\%, 26.3\%, 21.7\%, and 10.8\% over the previous best-performing models, respectively. The data and code will be available at https://github.com/tony-won/DST-BREAK},
  urldate    = {2023-08-04},
  booktitle  = {Proceedings of the 61st {Annual} {Meeting} of the {Association} for {Computational} {Linguistics} ({Volume} 1: {Long} {Papers})},
  publisher  = {Association for Computational Linguistics},
  author     = {Won, Seungpil and Kwak, Heeyoung and Shin, Joongbo and Han, Janghoon and Jung, Kyomin},
  month      = jul,
  year       = {2023},
  keywords   = {To Read},
  pages      = {2832--2846}
}

@misc{wang_luna_2022,
  title      = {{LUNA}: {Learning} {Slot}-{Turn} {Alignment} for {Dialogue} {State} {Tracking}},
  shorttitle = {{LUNA}},
  url        = {http://arxiv.org/abs/2205.02550},
  doi        = {10.48550/arXiv.2205.02550},
  abstract   = {Dialogue state tracking (DST) aims to predict the current dialogue state given the dialogue history. Existing methods generally exploit the utterances of all dialogue turns to assign value for each slot. This could lead to suboptimal results due to the information introduced from irrelevant utterances in the dialogue history, which may be useless and can even cause confusion. To address this problem, we propose LUNA, a sLot-tUrN Alignment enhanced approach. It first explicitly aligns each slot with its most relevant utterance, then further predicts the corresponding value based on this aligned utterance instead of all dialogue utterances. Furthermore, we design a slot ranking auxiliary task to learn the temporal correlation among slots which could facilitate the alignment. Comprehensive experiments are conducted on multi-domain task-oriented dialogue datasets, i.e., MultiWOZ 2.0, MultiWOZ 2.1, and MultiWOZ 2.2. The results show that LUNA achieves new state-of-the-art results on these datasets.},
  urldate    = {2023-08-27},
  publisher  = {arXiv},
  author     = {Wang, Yifan and Zhao, Jing and Bao, Junwei and Duan, Chaoqun and Wu, Youzheng and He, Xiaodong},
  month      = may,
  year       = {2022},
  note       = {arXiv:2205.02550 [cs]},
  keywords   = {Computer Science - Computation and Language, To Read}
}

@inproceedings{mosharrof_toward_2023,
  title     = {Toward {Open}-domain {Slot} {Filling} via {Self}-supervised {Co}-training},
  url       = {http://arxiv.org/abs/2303.13801},
  doi       = {10.1145/3543507.3583541},
  abstract  = {Slot filling is one of the critical tasks in modern conversational systems. The majority of existing literature employs supervised learning methods, which require labeled training data for each new domain. Zero-shot learning and weak supervision approaches, among others, have shown promise as alternatives to manual labeling. Nonetheless, these learning paradigms are significantly inferior to supervised learning approaches in terms of performance. To minimize this performance gap and demonstrate the possibility of open-domain slot filling, we propose a Self-supervised Co-training framework, called SCot, that requires zero in-domain manually labeled training examples and works in three phases. Phase one acquires two sets of complementary pseudo labels automatically. Phase two leverages the power of the pre-trained language model BERT, by adapting it for the slot filling task using these sets of pseudo labels. In phase three, we introduce a self-supervised cotraining mechanism, where both models automatically select highconfidence soft labels to further improve the performance of the other in an iterative fashion. Our thorough evaluations show that SCot outperforms state-of-the-art models by 45.57\% and 37.56\% on SGD and MultiWoZ datasets, respectively. Moreover, our proposed framework SCot achieves comparable performance when compared to state-of-the-art fully supervised models.},
  urldate   = {2023-08-27},
  booktitle = {Proceedings of the {ACM} {Web} {Conference} 2023},
  author    = {Mosharrof, Adib and Fereidouni, Moghis and Siddique, A. B.},
  month     = apr,
  year      = {2023},
  note      = {arXiv:2303.13801 [cs]},
  keywords  = {Computer Science - Computation and Language, Computer Science - Machine Learning, To Read},
  pages     = {1928--1937}
}

@article{luo_zero-shot_2023,
  title     = {Zero-{Shot} {Slot} {Filling} with {Slot}-{Prefix} {Prompting} and {Attention} {Relationship} {Descriptor}},
  volume    = {37},
  copyright = {Copyright (c) 2023 Association for the Advancement of Artificial Intelligence},
  issn      = {2374-3468},
  url       = {https://ojs.aaai.org/index.php/AAAI/article/view/26566},
  doi       = {10.1609/aaai.v37i11.26566},
  abstract  = {This paper addresses zero-shot slot filling, which tries to build a system that can generalize to unseen slot types without any training data. The key to zero-shot slot-filling is to match the tokens from the utterance with the semantic definition of the slot without training data in the target domain. This paper tackles this problem by devising a scheme to fully leverage pre-trained language models (PLMs). To this end, we propose a new prompting scheme that utilizes both learnable tokens and slot names to guide the model to focus on the relevant text spans for a given slot. Furthermore, we use attention values between tokens to form a feature descriptor for each token, which is motivated by the fact that the attention value in a PLM naturally characterizes various relationships, e.g., syntactic or semantic, between tokens. By further consolidating those features with an additional transformer-based aggregation module, we create a simple-but-effective zero-shot slot filling system that can achieve significantly better performance than the previous methods, as demonstrated by our experimental studies.},
  language  = {en},
  number    = {11},
  urldate   = {2023-08-27},
  journal   = {Proceedings of the AAAI Conference on Artificial Intelligence},
  author    = {Luo, Qiaoyang and Liu, Lingqiao},
  month     = jun,
  year      = {2023},
  note      = {Number: 11},
  keywords  = {SNLP: Language Models, To Read},
  pages     = {13344--13352}
}

@inproceedings{tavares_learning_2023,
  address   = {New York, NY, USA},
  series    = {{SIGIR} '23},
  title     = {Learning to {Ask} {Questions} for {Zero}-shot {Dialogue} {State} {Tracking}},
  isbn      = {978-1-4503-9408-6},
  url       = {https://dl.acm.org/doi/10.1145/3539618.3592010},
  doi       = {10.1145/3539618.3592010},
  abstract  = {We present a method for performing zero-shot Dialogue State Tracking (DST) by casting the task as a learning-to-ask-questions framework. The framework learns to pair the best question generation (QG) strategy with in-domain question answering (QA) methods to extract slot values from a dialogue without any human intervention. A novel self-supervised QA pretraining step using in-domain data is essential to learn the structure without requiring any slot-filling annotations. Moreover, we show that QG methods need to be aligned with the same grammatical person used in the dialogue. Empirical evaluation on the MultiWOZ 2.1 dataset demonstrates that our approach, when used alongside robust QA models, outperforms existing zero-shot methods in the challenging task of zero-shot cross domain adaptation-given a comparable amount of domain knowledge during data creation. Finally, we analyze the impact of the types of questions used, and demonstrate that the algorithmic approach outperforms template-based question generation.},
  urldate   = {2023-08-27},
  booktitle = {Proceedings of the 46th {International} {ACM} {SIGIR} {Conference} on {Research} and {Development} in {Information} {Retrieval}},
  publisher = {Association for Computing Machinery},
  author    = {Tavares, Diogo and Semedo, David and Rudnicky, Alexander and Magalhaes, Joao},
  month     = jul,
  year      = {2023},
  keywords  = {To Read, dialogue state tracking, question answering, zero-shot},
  pages     = {2118--2122}
}

@article{ji_survey_2023,
  title    = {Survey of {Hallucination} in {Natural} {Language} {Generation}},
  volume   = {55},
  issn     = {0360-0300, 1557-7341},
  url      = {http://arxiv.org/abs/2202.03629},
  doi      = {10.1145/3571730},
  abstract = {Natural Language Generation (NLG) has improved exponentially in recent years thanks to the development of sequence-to-sequence deep learning technologies such as Transformer-based language models. This advancement has led to more fluent and coherent NLG, leading to improved development in downstream tasks such as abstractive summarization, dialogue generation and data-to-text generation. However, it is also apparent that deep learning based generation is prone to hallucinate unintended text, which degrades the system performance and fails to meet user expectations in many real-world scenarios. To address this issue, many studies have been presented in measuring and mitigating hallucinated texts, but these have never been reviewed in a comprehensive manner before. In this survey, we thus provide a broad overview of the research progress and challenges in the hallucination problem in NLG. The survey is organized into two parts: (1) a general overview of metrics, mitigation methods, and future directions; and (2) an overview of task-specific research progress on hallucinations in the following downstream tasks, namely abstractive summarization, dialogue generation, generative question answering, data-to-text generation, machine translation, and visual-language generation. This survey serves to facilitate collaborative efforts among researchers in tackling the challenge of hallucinated texts in NLG.},
  number   = {12},
  urldate  = {2023-08-04},
  journal  = {ACM Computing Surveys},
  author   = {Ji, Ziwei and Lee, Nayeon and Frieske, Rita and Yu, Tiezheng and Su, Dan and Xu, Yan and Ishii, Etsuko and Bang, Yejin and Dai, Wenliang and Madotto, Andrea and Fung, Pascale},
  month    = dec,
  year     = {2023},
  note     = {arXiv:2202.03629 [cs]},
  keywords = {A.1, Computer Science - Computation and Language},
  pages    = {1--38}
}

@article{yang_domain-transfer_2023,
  title     = {A {Domain}-{Transfer} {Meta} {Task} {Design} {Paradigm} for {Few}-{Shot} {Slot} {Tagging}},
  volume    = {37},
  copyright = {Copyright (c) 2023 Association for the Advancement of Artificial Intelligence},
  issn      = {2374-3468},
  url       = {https://ojs.aaai.org/index.php/AAAI/article/view/26626},
  doi       = {10.1609/aaai.v37i11.26626},
  abstract  = {Few-shot slot tagging is an important task in dialogue systems and attracts much attention of researchers. Most previous few-shot slot tagging methods utilize meta-learning procedure for training and strive to construct a large number of different meta tasks to simulate the testing situation of insufficient data. However, there is a widespread phenomenon of overlap slot between two domains in slot tagging. Traditional meta tasks ignore this special phenomenon and cannot simulate such realistic few-shot slot tagging scenarios. It violates the basic principle of meta-learning which the meta task is consistent with the real testing task, leading to historical information forgetting problem. In this paper, we introduce a novel domain-transfer meta task design paradigm to tackle this problem. We distribute a basic domain to each target domain based on the coincidence degree of slot labels between these two domains. Unlike classic meta tasks which only rely on small samples of target domain, our meta tasks aim to correctly infer the class of target domain query samples based on both abundant data in basic domain and scarce data in target domain. To accomplish our meta task, we propose a Task Adaptation Network to effectively transfer the historical information from the basic domain to the target domain. We carry out sufficient experiments on the benchmark slot tagging dataset SNIPS and the name entity recognition dataset NER. Results demonstrate that our proposed model outperforms previous methods and achieves the state-of-the-art performance.},
  language  = {en},
  number    = {11},
  urldate   = {2023-08-04},
  journal   = {Proceedings of the AAAI Conference on Artificial Intelligence},
  author    = {Yang, Fengyi and Zhou, Xi and Yang, Yating and Ma, Bo and Dong, Rui and Atawulla, Abibulla},
  month     = jun,
  year      = {2023},
  note      = {Number: 11},
  keywords  = {SNLP: Text Mining},
  pages     = {13887--13895}
}

@inproceedings{cheng_is_2022,
  address    = {Abu Dhabi, United Arab Emirates},
  title      = {Is {MultiWOZ} a {Solved} {Task}? {An} {Interactive} {TOD} {Evaluation} {Framework} with {User} {Simulator}},
  shorttitle = {Is {MultiWOZ} a {Solved} {Task}?},
  url        = {https://aclanthology.org/2022.findings-emnlp.90},
  abstract   = {Task-Oriented Dialogue (TOD) systems are drawing more and more attention in recent studies.Current methods focus on constructing pre-trained models or fine-tuning strategies while the evaluation of TOD is limited by a policy mismatch problem.That is, during evaluation, the user utterances are from the annotated dataset while these utterances should interact with previous responses which can have many alternatives besides annotated texts.Therefore, in this work, we propose an interactive evaluation framework for TOD. We first build a goal-oriented user simulator based on pre-trained models and then use the user simulator to interact with the dialogue system to generate dialogues.Besides, we introduce a sentence-level and a session-level score to measure the sentence fluency and session coherence in the interactive evaluation. Experimental results show that RL-based TOD systems trained by our proposed user simulator can achieve nearly 98\% inform and success rates in the interactive evaluation of MultiWOZ dataset and the proposed scores measure the response quality besides the inform and success rates.We are hoping that our work will encourage simulator-based interactive evaluations in the TOD task.},
  urldate    = {2023-08-04},
  booktitle  = {Findings of the {Association} for {Computational} {Linguistics}: {EMNLP} 2022},
  publisher  = {Association for Computational Linguistics},
  author     = {Cheng, Qinyuan and Li, Linyang and Quan, Guofeng and Gao, Feng and Mou, Xiaofeng and Qiu, Xipeng},
  month      = dec,
  year       = {2022},
  pages      = {1248--1259}
}

@misc{abercrombie_mirages_2023,
  title      = {Mirages: {On} {Anthropomorphism} in {Dialogue} {Systems}},
  shorttitle = {Mirages},
  url        = {http://arxiv.org/abs/2305.09800},
  doi        = {10.48550/arXiv.2305.09800},
  abstract   = {Automated dialogue or conversational systems are anthropomorphised by developers and personified by users. While a degree of anthropomorphism is inevitable, conscious and unconscious design choices can guide users to personify them to varying degrees. Encouraging users to relate to automated systems as if they were human can lead to transparency and trust issues, and high risk scenarios caused by over-reliance on their outputs. As a result, natural language processing researchers have begun to investigate factors that induce personification and develop resources to mitigate such effects. However, these efforts are fragmented, and many aspects of anthropomorphism have yet to be considered. In this paper, we discuss the linguistic factors that contribute to the anthropomorphism of dialogue systems and the harms that can arise, arguing that it can reinforce stereotypes of gender roles and notions of acceptable language. We recommend that future efforts towards developing dialogue systems take particular care in their design, development, release, and description; and attend to the many linguistic cues that can elicit personification by users.},
  urldate    = {2023-08-04},
  publisher  = {arXiv},
  author     = {Abercrombie, Gavin and Curry, Amanda Cercas and Dinkar, Tanvi and Talat, Zeerak},
  month      = may,
  year       = {2023},
  note       = {arXiv:2305.09800 [cs]},
  keywords   = {Computer Science - Computation and Language}
}

@misc{li_guiding_2023,
  title     = {Guiding {Large} {Language} {Models} via {Directional} {Stimulus} {Prompting}},
  url       = {http://arxiv.org/abs/2302.11520},
  doi       = {10.48550/arXiv.2302.11520},
  abstract  = {We introduce a novel prompting framework called Directional Stimulus Prompting for guiding black-box large language models (LLMs) toward desired outputs. The framework introduces a new component called directional stimulus into the prompt, providing more fine-grained guidance and control over LLMs. The directional stimulus serves as hints or cues for each input query to guide LLMs toward the desired output, such as keywords that the desired summary should include for summarization. We utilize a small tunable model (e.g., T5) to generate such directional stimulus for each query, allowing us to optimize black-box LLMs by optimizing a small policy model. This policy model can be trained through 1) supervised fine-tuning using labeled data and 2) reinforcement learning from offline or online rewards to explore directional stimulus that better aligns LLMs with desired behaviors. We evaluate our framework on summarization and dialogue response generation tasks. Experimental results show that our framework consistently improves ChatGPT's performance over standard prompting with a small collection of training data, and reinforcement learning further improves the performance. Notably, on the MultWOZ dataset, our framework enables ChatGPT to achieve a remarkable 41.4\% improvement in its combined score with only 80 dialogues, matching or even surpassing the performance of some fully trained state-of-the-art models. We have made our code publicly available.},
  urldate   = {2023-08-04},
  publisher = {arXiv},
  author    = {Li, Zekun and Peng, Baolin and He, Pengcheng and Galley, Michel and Gao, Jianfeng and Yan, Xifeng},
  month     = jul,
  year      = {2023},
  note      = {arXiv:2302.11520 [cs]},
  keywords  = {Computer Science - Computation and Language}
}

@inproceedings{basu_strategies_2022,
  address   = {Seattle, USA},
  title     = {Strategies to {Improve} {Few}-shot {Learning} for {Intent} {Classification} and {Slot}-{Filling}},
  url       = {https://aclanthology.org/2022.suki-1.3},
  doi       = {10.18653/v1/2022.suki-1.3},
  abstract  = {Intent classification (IC) and slot filling (SF) are two fundamental tasks in modern Natural Language Understanding (NLU) systems. Collecting and annotating large amounts of data to train deep learning models for such systems are not scalable. This problem can be addressed by learning from few examples using fast supervised meta-learning techniques such as prototypical networks. In this work, we systematically investigate how contrastive learning and data augmentation methods can benefit these existing meta-learning pipelines for jointly modelled IC/SF tasks. Through extensive experiments across standard IC/SF benchmarks (SNIPS and ATIS), we show that our proposed approaches outperform standard meta-learning methods: contrastive losses as a regularizer in conjunction with prototypical networks consistently outperform the existing state-of-the-art for both IC and SF tasks, while data augmentation strategies primarily improve few-shot IC by a significant margin},
  urldate   = {2023-08-04},
  booktitle = {Proceedings of the {Workshop} on {Structured} and {Unstructured} {Knowledge} {Integration} ({SUKI})},
  publisher = {Association for Computational Linguistics},
  author    = {Basu, Samyadeep and Sharaf, Amr and Ip Kiun Chong, Karine and Fischer, Alex and Rohra, Vishal and Amoake, Michael and El-Hammamy, Hazem and Nosakhare, Ehi and Ramani, Vijay and Han, Benjamin},
  month     = jul,
  year      = {2022},
  pages     = {17--25}
}

@inproceedings{aksu_prompter_2023,
  address    = {Toronto, Canada},
  title      = {Prompter: {Zero}-shot {Adaptive} {Prefixes} for {Dialogue} {State} {Tracking} {Domain} {Adaptation}},
  shorttitle = {Prompter},
  url        = {https://aclanthology.org/2023.acl-long.252},
  abstract   = {A challenge in the Dialogue State Tracking (DST) field is adapting models to new domains without using any supervised data — zero-shot domain adaptation. Parameter-Efficient Transfer Learning (PETL) has the potential to address this problem due to its robustness. However, it has yet to be applied to the zero-shot scenarios, as it is not clear how to apply it unsupervisedly. Our method, Prompter, uses descriptions of target domain slots to generate dynamic prefixes that are concatenated to the key and values at each layer's self-attention mechanism. This allows for the use of prefix-tuning in zero-shot. Prompter outperforms previous methods on both the MultiWOZ and SGD benchmarks. In generating prefixes, our analyses find that Prompter not only utilizes the semantics of slot descriptions but also how often the slots appear together in conversation. Moreover, Prompter's gains are due to its improved ability to distinguish ”none”-valued dialogue slots, compared against baselines.},
  urldate    = {2023-08-04},
  booktitle  = {Proceedings of the 61st {Annual} {Meeting} of the {Association} for {Computational} {Linguistics} ({Volume} 1: {Long} {Papers})},
  publisher  = {Association for Computational Linguistics},
  author     = {Aksu, Ibrahim Taha and Kan, Min-Yen and Chen, Nancy},
  month      = jul,
  year       = {2023},
  pages      = {4588--4603}
}

@inproceedings{jacqmin_you_2022,
  address    = {Edinburgh, UK},
  title      = {“{Do} you follow me?”: {A} {Survey} of {Recent} {Approaches} in {Dialogue} {State} {Tracking}},
  shorttitle = {“{Do} you follow me?},
  url        = {https://aclanthology.org/2022.sigdial-1.33},
  abstract   = {While communicating with a user, a task-oriented dialogue system has to track the user's needs at each turn according to the conversation history. This process called dialogue state tracking (DST) is crucial because it directly informs the downstream dialogue policy. DST has received a lot of interest in recent years with the text-to-text paradigm emerging as the favored approach. In this review paper, we first present the task and its associated datasets. Then, considering a large number of recent publications, we identify highlights and advances of research in 2021-2022. Although neural approaches have enabled significant progress, we argue that some critical aspects of dialogue systems such as generalizability are still underexplored. To motivate future studies, we propose several research avenues.},
  urldate    = {2023-08-04},
  booktitle  = {Proceedings of the 23rd {Annual} {Meeting} of the {Special} {Interest} {Group} on {Discourse} and {Dialogue}},
  publisher  = {Association for Computational Linguistics},
  author     = {Jacqmin, Léo and Rojas Barahona, Lina M. and Favre, Benoit},
  month      = sep,
  year       = {2022},
  pages      = {336--350}
}

@misc{su_selective_2022,
  title     = {Selective {Annotation} {Makes} {Language} {Models} {Better} {Few}-{Shot} {Learners}},
  url       = {http://arxiv.org/abs/2209.01975},
  doi       = {10.48550/arXiv.2209.01975},
  abstract  = {Many recent approaches to natural language tasks are built on the remarkable abilities of large language models. Large language models can perform in-context learning, where they learn a new task from a few task demonstrations, without any parameter updates. This work examines the implications of in-context learning for the creation of datasets for new natural language tasks. Departing from recent in-context learning methods, we formulate an annotation-efficient, two-step framework: selective annotation that chooses a pool of examples to annotate from unlabeled data in advance, followed by prompt retrieval that retrieves task examples from the annotated pool at test time. Based on this framework, we propose an unsupervised, graph-based selective annotation method, voke-k, to select diverse, representative examples to annotate. Extensive experiments on 10 datasets (covering classification, commonsense reasoning, dialogue, and text/code generation) demonstrate that our selective annotation method improves the task performance by a large margin. On average, vote-k achieves a 12.9\%/11.4\% relative gain under an annotation budget of 18/100, as compared to randomly selecting examples to annotate. Compared to state-of-the-art supervised finetuning approaches, it yields similar performance with 10-100x less annotation cost across 10 tasks. We further analyze the effectiveness of our framework in various scenarios: language models with varying sizes, alternative selective annotation methods, and cases where there is a test data domain shift. We hope that our studies will serve as a basis for data annotations as large language models are increasingly applied to new tasks. Our code is available at https://github.com/HKUNLP/icl-selective-annotation.},
  urldate   = {2023-08-04},
  publisher = {arXiv},
  author    = {Su, Hongjin and Kasai, Jungo and Wu, Chen Henry and Shi, Weijia and Wang, Tianlu and Xin, Jiayi and Zhang, Rui and Ostendorf, Mari and Zettlemoyer, Luke and Smith, Noah A. and Yu, Tao},
  month     = sep,
  year      = {2022},
  note      = {arXiv:2209.01975 [cs]},
  keywords  = {Computer Science - Computation and Language}
}

@inproceedings{sreedhar_prompt_2022,
  address   = {Abu Dhabi, Beijing (Hybrid)},
  title     = {Prompt {Learning} for {Domain} {Adaptation} in {Task}-{Oriented} {Dialogue}},
  url       = {https://aclanthology.org/2022.seretod-1.4},
  abstract  = {Conversation designers continue to face significant obstacles when creating productionquality task-oriented dialogue systems. The complexity and cost involved in schema development and data collection is often a major barrier for such designers, limiting their ability to create natural, user-friendly experiences. We frame the classification of user intent as the generation of a canonical form, a lightweight semantic representation using natural language. We show that canonical forms offer a promising alternative to traditional methods for intent classification. By tuning soft prompts for a frozen large language model, we show that canonical forms generalize very well to new, unseen domains in a zero- or few-shot setting. The method is also sample-efficient, reducing the complexity and effort of developing new task-oriented dialogue domains.},
  urldate   = {2023-08-04},
  booktitle = {Proceedings of the {Towards} {Semi}-{Supervised} and {Reinforced} {Task}-{Oriented} {Dialog} {Systems} ({SereTOD})},
  publisher = {Association for Computational Linguistics},
  author    = {Sreedhar, Makesh Narsimhan and Parisien, Christopher},
  month     = dec,
  year      = {2022},
  pages     = {24--30}
}

@inproceedings{hou_inverse_2022,
  address   = {Dublin, Ireland},
  title     = {Inverse is {Better}! {Fast} and {Accurate} {Prompt} for {Few}-shot {Slot} {Tagging}},
  url       = {https://aclanthology.org/2022.findings-acl.53},
  doi       = {10.18653/v1/2022.findings-acl.53},
  abstract  = {Prompting methods recently achieve impressive success in few-shot learning. These methods modify input samples with prompt sentence pieces, and decode label tokens to map samples to corresponding labels. However, such a paradigm is very inefficient for the task of slot tagging. Since slot tagging samples are multiple consecutive words in a sentence, the prompting methods have to enumerate all n-grams token spans to find all the possible slots, which greatly slows down the prediction. To tackle this, we introduce an inverse paradigm for prompting. Different from the classic prompts mapping tokens to labels, we reversely predict slot values given slot types. Such inverse prompting only requires a one-turn prediction for each slot type and greatly speeds up the prediction. Besides, we propose a novel Iterative Prediction Strategy, from which the model learns to refine predictions by considering the relations between different slot types. We find, somewhat surprisingly, the proposed method not only predicts faster but also significantly improves the effect (improve over 6.1 F1-scores on 10-shot setting) and achieves new state-of-the-art performance.},
  urldate   = {2023-09-18},
  booktitle = {Findings of the {Association} for {Computational} {Linguistics}: {ACL} 2022},
  publisher = {Association for Computational Linguistics},
  author    = {Hou, Yutai and Chen, Cheng and Luo, Xianzhen and Li, Bohan and Che, Wanxiang},
  month     = may,
  year      = {2022},
  pages     = {637--647}
}

@inproceedings{rosenbaum_linguist_2022,
  address    = {Gyeongju, Republic of Korea},
  title      = {{LINGUIST}: {Language} {Model} {Instruction} {Tuning} to {Generate} {Annotated} {Utterances} for {Intent} {Classification} and {Slot} {Tagging}},
  shorttitle = {{LINGUIST}},
  url        = {https://aclanthology.org/2022.coling-1.18},
  abstract   = {We present LINGUIST, a method for generating annotated data for Intent Classification and Slot Tagging (IC+ST), via fine-tuning AlexaTM 5B, a 5-billion-parameter multilingual sequence-to-sequence (seq2seq) model, on a flexible instruction prompt. In a 10-shot novel intent setting for the SNIPS dataset, LINGUIST surpasses state-of-the-art approaches (Back-Translation and Example Extrapolation) by a wide margin, showing absolute improvement for the target intents of +1.9 points on IC Recall and +2.5 points on ST F1 Score. In the zero-shot cross-lingual setting of the mATIS++ dataset, LINGUIST out-performs a strong baseline of Machine Translation with Slot Alignment by +4.14 points absolute on ST F1 Score across 6 languages, while matching performance on IC. Finally, we verify our results on an internal large-scale multilingual dataset for conversational agent IC+ST and show significant improvements over a baseline which uses Back-Translation, Paraphrasing and Slot Catalog Resampling. To our knowledge, we are the first to demonstrate instruction fine-tuning of a large-scale seq2seq model to control the outputs of multilingual intent- and slot-labeled data generation.},
  urldate    = {2023-09-19},
  booktitle  = {Proceedings of the 29th {International} {Conference} on {Computational} {Linguistics}},
  publisher  = {International Committee on Computational Linguistics},
  author     = {Rosenbaum, Andy and Soltan, Saleh and Hamza, Wael and Versley, Yannick and Boese, Markus},
  month      = oct,
  year       = {2022},
  pages      = {218--241}
}

@misc{chen_bert_2019,
  title     = {{BERT} for {Joint} {Intent} {Classification} and {Slot} {Filling}},
  url       = {http://arxiv.org/abs/1902.10909},
  abstract  = {Intent classification and slot filling are two essential tasks for natural language understanding. They often suffer from small-scale human-labeled training data, resulting in poor generalization capability, especially for rare words. Recently a new language representation model, BERT (Bidirectional Encoder Representations from Transformers), facilitates pre-training deep bidirectional representations on large-scale unlabeled corpora, and has created state-of-the-art models for a wide variety of natural language processing tasks after simple fine-tuning. However, there has not been much effort on exploring BERT for natural language understanding. In this work, we propose a joint intent classification and slot filling model based on BERT. Experimental results demonstrate that our proposed model achieves significant improvement on intent classification accuracy, slot filling F1, and sentence-level semantic frame accuracy on several public benchmark datasets, compared to the attention-based recurrent neural network models and slot-gated models.},
  urldate   = {2023-09-28},
  publisher = {arXiv},
  author    = {Chen, Qian and Zhuo, Zhu and Wang, Wen},
  month     = feb,
  year      = {2019},
  note      = {arXiv:1902.10909 [cs]},
  keywords  = {Computer Science - Computation and Language}
}

@inproceedings{lin_selective_2023,
  address   = {Dubrovnik, Croatia},
  title     = {Selective {In}-{Context} {Data} {Augmentation} for {Intent} {Detection} using {Pointwise} {V}-{Information}},
  url       = {https://aclanthology.org/2023.eacl-main.107},
  doi       = {10.18653/v1/2023.eacl-main.107},
  abstract  = {This work focuses on in-context data augmentation for intent detection. Having found that augmentation via in-context prompting of large pre-trained language models (PLMs) alone does not improve performance, we introduce a novel approach based on PLMs and pointwise V-information (PVI), a metric that can measure the usefulness of a datapoint for training a model. Our method first fine-tunes a PLM on a small seed of training data and then synthesizes new datapoints - utterances that correspond to given intents. It then employs intent-aware filtering, based on PVI, to remove datapoints that are not helpful to the downstream intent classifier. Our method is thus able to leverage the expressive power of large language models to produce diverse training data. Empirical results demonstrate that our method can produce synthetic training data that achieve state-of-the-art performance on three challenging intent detection datasets under few-shot settings (1.28\% absolute improvement in 5-shot and 1.18\% absolute in 10-shot, on average) and perform on par with the state-of-the-art in full-shot settings (within 0.01\% absolute, on average).},
  urldate   = {2023-09-29},
  booktitle = {Proceedings of the 17th {Conference} of the {European} {Chapter} of the {Association} for {Computational} {Linguistics}},
  publisher = {Association for Computational Linguistics},
  author    = {Lin, Yen-Ting and Papangelis, Alexandros and Kim, Seokhwan and Lee, Sungjin and Hazarika, Devamanyu and Namazifar, Mahdi and Jin, Di and Liu, Yang and Hakkani-Tur, Dilek},
  month     = may,
  year      = {2023},
  pages     = {1463--1476}
}

@misc{zhang2024teleclass,
  title         = {TELEClass: Taxonomy Enrichment and LLM-Enhanced Hierarchical Text Classification with Minimal Supervision},
  author        = {Yunyi Zhang and Ruozhen Yang and Xueqiang Xu and Jinfeng Xiao and Jiaming Shen and Jiawei Han},
  year          = {2024},
  eprint        = {2403.00165},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL}
}

@misc{liu2023recent_hierarchical,
  title         = {Recent Advances in Hierarchical Multi-label Text Classification: A Survey},
  author        = {Rundong Liu and Wenhan Liang and Weijun Luo and Yuxiang Song and He Zhang and Ruohua Xu and Yunfeng Li and Ming Liu},
  year          = {2023},
  eprint        = {2307.16265},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL}
}

@inproceedings{belouadi2024automatikz,
  title     = {AutomaTikZ: Text-Guided Synthesis of Scientific Vector Graphics with TikZ},
  author    = {Jonas Belouadi and Anne Lauscher and Steffen Eger},
  booktitle = {The Twelfth International Conference on Learning Representations},
  year      = {2024},
  url       = {https://openreview.net/forum?id=v3K5TVP8kZ}
}

@inproceedings{hatefi-etal-2024-promptstream-self,
  title     = {{P}rompt{S}tream: Self-Supervised News Story Discovery Using Topic-Aware Article Representations},
  author    = {Hatefi, Arezoo  and
               Eklund, Anton  and
               Forsman, Mona},
  editor    = {Calzolari, Nicoletta  and
               Kan, Min-Yen  and
               Hoste, Veronique  and
               Lenci, Alessandro  and
               Sakti, Sakriani  and
               Xue, Nianwen},
  booktitle = {Proceedings of the 2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation (LREC-COLING 2024)},
  month     = may,
  year      = {2024},
  address   = {Torino, Italia},
  publisher = {ELRA and ICCL},
  url       = {https://aclanthology.org/2024.lrec-main.1157},
  pages     = {13222--13232},
  abstract  = {Given the importance of identifying and monitoring news stories within the continuous flow of news articles, this paper presents PromptStream, a novel method for unsupervised news story discovery. In order to identify coherent and comprehensive stories across the stream, it is crucial to create article representations that incorporate as much topic-related information from the articles as possible. PromptStream constructs these article embeddings using cloze-style prompting. These representations continually adjust to the evolving context of the news stream through self-supervised learning, employing a contrastive loss and a memory of the most confident article-story assignments from the most recent days. Extensive experiments with real news datasets highlight the notable performance of our model, establishing a new state of the art. Additionally, we delve into selected news stories to reveal how the model{'}s structuring of the article stream aligns with story progression.}
}

@misc{nayak2024learning,
  title         = {Learning to Generate Instruction Tuning Datasets for Zero-Shot Task Adaptation},
  author        = {Nihal V. Nayak and Yiyang Nan and Avi Trost and Stephen H. Bach},
  year          = {2024},
  eprint        = {2402.18334},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL}
}

@inproceedings{hemphill-etal-1990-atis,
  title     = {The {ATIS} Spoken Language Systems Pilot Corpus},
  author    = {Hemphill, Charles T.  and
               Godfrey, John J.  and
               Doddington, George R.},
  booktitle = {Speech and Natural Language: Proceedings of a Workshop Held at Hidden Valley, {P}ennsylvania, June 24-27,1990},
  year      = {1990},
  url       = {https://aclanthology.org/H90-1021}
}

@InProceedings{bastianelli-etal-2020-slurp,
  author    = {Bastianelli, Emanuele and Vanzo, Andrea and Swietojanski, Pawel and Rieser, Verena},
  booktitle = {Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)},
  title     = {{SLURP}: A Spoken Language Understanding Resource Package},
  year      = {2020},
  address   = {Online},
  editor    = {Webber, Bonnie and Cohn, Trevor and He, Yulan and Liu, Yang},
  month     = nov,
  pages     = {7252--7262},
  publisher = {Association for Computational Linguistics},
  abstract  = {Spoken Language Understanding infers semantic meaning directly from audio data, and thus promises to reduce error propagation and misunderstandings in end-user applications. However, publicly available SLU resources are limited. In this paper, we release SLURP, a new SLU package containing the following: (1) A new challenging dataset in English spanning 18 domains, which is substantially bigger and linguistically more diverse than existing datasets; (2) Competitive baselines based on state-of-the-art NLU and ASR systems; (3) A new transparent metric for entity labelling which enables a detailed error analysis for identifying potential areas of improvement. SLURP is available at \url{https://github.com/pswietojanski/slurp}.},
  doi       = {10.18653/v1/2020.emnlp-main.588},
  groups    = {Datasets, SLU},
  url       = {https://aclanthology.org/2020.emnlp-main.588},
}

@misc{goodfellow_empirical_2015,
  title     = {An {Empirical} {Investigation} of {Catastrophic} {Forgetting} in {Gradient}-{Based} {Neural} {Networks}},
  url       = {http://arxiv.org/abs/1312.6211},
  doi       = {10.48550/arXiv.1312.6211},
  abstract  = {Catastrophic forgetting is a problem faced by many machine learning models and algorithms. When trained on one task, then trained on a second task, many machine learning models "forget" how to perform the first task. This is widely believed to be a serious problem for neural networks. Here, we investigate the extent to which the catastrophic forgetting problem occurs for modern neural networks, comparing both established and recent gradient-based training algorithms and activation functions. We also examine the effect of the relationship between the first task and the second task on catastrophic forgetting. We find that it is always best to train using the dropout algorithm--the dropout algorithm is consistently best at adapting to the new task, remembering the old task, and has the best tradeoff curve between these two extremes. We find that different tasks and relationships between tasks result in very different rankings of activation function performance. This suggests the choice of activation function should always be cross-validated.},
  urldate   = {2023-09-29},
  publisher = {arXiv},
  author    = {Goodfellow, Ian J. and Mirza, Mehdi and Xiao, Da and Courville, Aaron and Bengio, Yoshua},
  month     = mar,
  year      = {2015},
  note      = {arXiv:1312.6211 [cs, stat]},
  keywords  = {Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, Statistics - Machine Learning}
}

@article{weld_survey_2023,
  title    = {A {Survey} of {Joint} {Intent} {Detection} and {Slot} {Filling} {Models} in {Natural} {Language} {Understanding}},
  volume   = {55},
  issn     = {0360-0300, 1557-7341},
  url      = {https://dl.acm.org/doi/10.1145/3547138},
  doi      = {10.1145/3547138},
  abstract = {Intent classification, to identify the speaker’s intention, and slot filling, to label each token with a semantic type, are critical tasks in natural language understanding. Traditionally the two tasks have been addressed independently. More recently joint models that address the two tasks together have achieved state-of-the-art performance for each task and have shown there exists a strong relationship between the two. In this survey, we bring the coverage of methods up to 2021 including the many applications of deep learning in the field. As well as a technological survey, we look at issues addressed in the joint task and the approaches designed to address these issues. We cover datasets, evaluation metrics, and experiment design and supply a summary of reported performance on the standard datasets.},
  language = {en},
  number   = {8},
  urldate  = {2023-09-28},
  journal  = {ACM Computing Surveys},
  author   = {Weld, Henry and Huang, Xiaoqi and Long, Siqu and Poon, Josiah and Han, Soyeon Caren},
  month    = aug,
  year     = {2023},
  pages    = {1--38}
}

@inproceedings{houlsby_parameter-efficient_2019,
  title     = {Parameter-{Efficient} {Transfer} {Learning} for {NLP}},
  url       = {https://proceedings.mlr.press/v97/houlsby19a.html},
  abstract  = {Fine-tuning large pretrained models is an effective transfer mechanism in NLP. However, in the presence of many downstream tasks, fine-tuning is parameter inefficient: an entire new model is required for every task. As an alternative, we propose transfer with adapter modules. Adapter modules yield a compact and extensible model; they add only a few trainable parameters per task, and new tasks can be added without revisiting previous ones. The parameters of the original network remain fixed, yielding a high degree of parameter sharing. To demonstrate adapter’s effectiveness, we transfer the recently proposed BERT Transformer model to 262626 diverse text classification tasks, including the GLUE benchmark. Adapters attain near state-of-the-art performance, whilst adding only a few parameters per task. On GLUE, we attain within 0.80.80.8\% of the performance of full fine-tuning, adding only 3.63.63.6\% parameters per task. By contrast, fine-tuning trains 100100100\% of the parameters per task.},
  language  = {en},
  urldate   = {2023-10-09},
  booktitle = {Proceedings of the 36th {International} {Conference} on {Machine} {Learning}},
  publisher = {PMLR},
  author    = {Houlsby, Neil and Giurgiu, Andrei and Jastrzebski, Stanislaw and Morrone, Bruna and Laroussilhe, Quentin De and Gesmundo, Andrea and Attariyan, Mona and Gelly, Sylvain},
  month     = may,
  year      = {2019},
  note      = {ISSN: 2640-3498},
  pages     = {2790--2799}
}

@misc{han_bi-directional_2022,
  title     = {Bi-directional {Joint} {Neural} {Networks} for {Intent} {Classification} and {Slot} {Filling}},
  url       = {http://arxiv.org/abs/2202.13079},
  doi       = {10.48550/arXiv.2202.13079},
  abstract  = {Intent classification and slot filling are two critical tasks for natural language understanding. Traditionally the two tasks proceeded independently. However, more recently joint models for intent classification and slot filling have achieved state-of-the-art performance, and have proved that there exists a strong relationship between the two tasks. In this paper, we propose a bi-directional joint model for intent classification and slot filling, which includes a multi-stage hierarchical process via BERT and bi-directional joint natural language understanding mechanisms, including intent2slot and slot2intent, to obtain mutual performance enhancement between intent classification and slot filling. The evaluations show that our model achieves state-of-the-art results on intent classification accuracy, slot filling F1, and significantly improves sentence-level semantic frame accuracy when applied to publicly available benchmark datasets, ATIS (88.6\%) and SNIPS (92.8\%).},
  urldate   = {2023-10-06},
  publisher = {arXiv},
  author    = {Han, Soyeon Caren and Long, Siqu and Li, Huichun and Weld, Henry and Poon, Josiah},
  month     = feb,
  year      = {2022},
  note      = {arXiv:2202.13079 [cs]},
  keywords  = {Computer Science - Computation and Language}
}

@inproceedings{gupta_simple_2019,
  address   = {Stockholm, Sweden},
  title     = {Simple, {Fast}, {Accurate} {Intent} {Classification} and {Slot} {Labeling} for {Goal}-{Oriented} {Dialogue} {Systems}},
  url       = {https://aclanthology.org/W19-5906},
  doi       = {10.18653/v1/W19-5906},
  abstract  = {With the advent of conversational assistants, like Amazon Alexa, Google Now, etc., dialogue systems are gaining a lot of traction, especially in industrial setting. These systems typically consist of Spoken Language understanding component which, in turn, consists of two tasks - Intent Classification (IC) and Slot Labeling (SL). Generally, these two tasks are modeled together jointly to achieve best performance. However, this joint modeling adds to model obfuscation. In this work, we first design framework for a modularization of joint IC-SL task to enhance architecture transparency. Then, we explore a number of self-attention, convolutional, and recurrent models, contributing a large-scale analysis of modeling paradigms for IC+SL across two datasets. Finally, using this framework, we propose a class of `label-recurrent' models that otherwise non-recurrent, with a 10-dimensional representation of the label history, and show that our proposed systems are easy to interpret, highly accurate (achieving over 30\% error reduction in SL over the state-of-the-art on the Snips dataset), as well as fast, at 2x the inference and 2/3 to 1/2 the training time of comparable recurrent models, thus giving an edge in critical real-world systems.},
  urldate   = {2023-10-06},
  booktitle = {Proceedings of the 20th {Annual} {SIGdial} {Meeting} on {Discourse} and {Dialogue}},
  publisher = {Association for Computational Linguistics},
  author    = {Gupta, Arshit and Hewitt, John and Kirchhoff, Katrin},
  month     = sep,
  year      = {2019},
  pages     = {46--55}
}

@misc{liu_roberta_2019,
  title      = {{RoBERTa}: {A} {Robustly} {Optimized} {BERT} {Pretraining} {Approach}},
  shorttitle = {{RoBERTa}},
  url        = {http://arxiv.org/abs/1907.11692},
  doi        = {10.48550/arXiv.1907.11692},
  abstract   = {Language model pretraining has led to significant performance gains but careful comparison between different approaches is challenging. Training is computationally expensive, often done on private datasets of different sizes, and, as we will show, hyperparameter choices have significant impact on the final results. We present a replication study of BERT pretraining (Devlin et al., 2019) that carefully measures the impact of many key hyperparameters and training data size. We find that BERT was significantly undertrained, and can match or exceed the performance of every model published after it. Our best model achieves state-of-the-art results on GLUE, RACE and SQuAD. These results highlight the importance of previously overlooked design choices, and raise questions about the source of recently reported improvements. We release our models and code.},
  urldate    = {2023-10-08},
  publisher  = {arXiv},
  author     = {Liu, Yinhan and Ott, Myle and Goyal, Naman and Du, Jingfei and Joshi, Mandar and Chen, Danqi and Levy, Omer and Lewis, Mike and Zettlemoyer, Luke and Stoyanov, Veselin},
  month      = jul,
  year       = {2019},
  note       = {arXiv:1907.11692 [cs]},
  keywords   = {Computer Science - Computation and Language}
}

@inproceedings{parikh_exploring_2023,
  address   = {Toronto, Canada},
  title     = {Exploring {Zero} and {Few}-shot {Techniques} for {Intent} {Classification}},
  url       = {https://aclanthology.org/2023.acl-industry.71},
  abstract  = {Conversational NLU providers often need to scale to thousands of intent-classification models where new customers often face the cold-start problem. Scaling to so many customers puts a constraint on storage space as well. In this paper, we explore four different zero and few-shot intent classification approaches with this low-resource constraint: 1) domain adaptation, 2) data augmentation, 3) zero-shot intent classification using descriptions large language models (LLMs), and 4) parameter-efficient fine-tuning of instruction-finetuned language models. Our results show that all these approaches are effective to different degrees in low-resource settings. Parameter-efficient fine-tuning using T-few recipe on Flan-T5 yields the best performance even with just one sample per intent. We also show that the zero-shot method of prompting LLMs using intent descriptions is also very competitive.},
  urldate   = {2023-08-04},
  booktitle = {Proceedings of the 61st {Annual} {Meeting} of the {Association} for {Computational} {Linguistics} ({Volume} 5: {Industry} {Track})},
  publisher = {Association for Computational Linguistics},
  author    = {Parikh, Soham and Tiwari, Mitul and Tumbade, Prashil and Vohra, Quaizar},
  month     = jul,
  year      = {2023},
  pages     = {744--751}
}

@inproceedings{zhang_few-shot_2021,
  address   = {Online and Punta Cana, Dominican Republic},
  title     = {Few-{Shot} {Intent} {Detection} via {Contrastive} {Pre}-{Training} and {Fine}-{Tuning}},
  url       = {https://aclanthology.org/2021.emnlp-main.144},
  doi       = {10.18653/v1/2021.emnlp-main.144},
  abstract  = {In this work, we focus on a more challenging few-shot intent detection scenario where many intents are fine-grained and semantically similar. We present a simple yet effective few-shot intent detection schema via contrastive pre-training and fine-tuning. Specifically, we first conduct self-supervised contrastive pre-training on collected intent datasets, which implicitly learns to discriminate semantically similar utterances without using any labels. We then perform few-shot intent detection together with supervised contrastive learning, which explicitly pulls utterances from the same intent closer and pushes utterances across different intents farther. Experimental results show that our proposed method achieves state-of-the-art performance on three challenging intent detection datasets under 5-shot and 10-shot settings.},
  urldate   = {2023-09-29},
  booktitle = {Proceedings of the 2021 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing}},
  publisher = {Association for Computational Linguistics},
  author    = {Zhang, Jianguo and Bui, Trung and Yoon, Seunghyun and Chen, Xiang and Liu, Zhiwei and Xia, Congying and Tran, Quan Hung and Chang, Walter and Yu, Philip},
  month     = nov,
  year      = {2021},
  pages     = {1906--1912}
}

@inproceedings{yu_few-shot_2021,
  address   = {Online},
  title     = {Few-shot {Intent} {Classification} and {Slot} {Filling} with {Retrieved} {Examples}},
  url       = {https://aclanthology.org/2021.naacl-main.59},
  doi       = {10.18653/v1/2021.naacl-main.59},
  abstract  = {Few-shot learning arises in important practical scenarios, such as when a natural language understanding system needs to learn new semantic labels for an emerging, resource-scarce domain. In this paper, we explore retrieval-based methods for intent classification and slot filling tasks in few-shot settings. Retrieval-based methods make predictions based on labeled examples in the retrieval index that are similar to the input, and thus can adapt to new domains simply by changing the index without having to retrain the model. However, it is non-trivial to apply such methods on tasks with a complex label space like slot filling. To this end, we propose a span-level retrieval method that learns similar contextualized representations for spans with the same label via a novel batch-softmax objective. At inference time, we use the labels of the retrieved spans to construct the final structure with the highest aggregated score. Our method outperforms previous systems in various few-shot settings on the CLINC and SNIPS benchmarks.},
  urldate   = {2023-10-08},
  booktitle = {Proceedings of the 2021 {Conference} of the {North} {American} {Chapter} of the {Association} for {Computational} {Linguistics}: {Human} {Language} {Technologies}},
  publisher = {Association for Computational Linguistics},
  author    = {Yu, Dian and He, Luheng and Zhang, Yuan and Du, Xinya and Pasupat, Panupong and Li, Qi},
  month     = jun,
  year      = {2021},
  pages     = {734--749}
}

@inproceedings{krone_learning_2020,
  address   = {Online},
  title     = {Learning to {Classify} {Intents} and {Slot} {Labels} {Given} a {Handful} of {Examples}},
  url       = {https://aclanthology.org/2020.nlp4convai-1.12},
  doi       = {10.18653/v1/2020.nlp4convai-1.12},
  abstract  = {Intent classification (IC) and slot filling (SF) are core components in most goal-oriented dialogue systems. Current IC/SF models perform poorly when the number of training examples per class is small. We propose a new few-shot learning task, few-shot IC/SF, to study and improve the performance of IC and SF models on classes not seen at training time in ultra low resource scenarios. We establish a few-shot IC/SF benchmark by defining few-shot splits for three public IC/SF datasets, ATIS, TOP, and Snips. We show that two popular few-shot learning algorithms, model agnostic meta learning (MAML) and prototypical networks, outperform a fine-tuning baseline on this benchmark. Prototypical networks achieves significant gains in IC performance on the ATIS and TOP datasets, while both prototypical networks and MAML outperform the baseline with respect to SF on all three datasets. In addition, we demonstrate that joint training as well as the use of pre-trained language models, ELMo and BERT in our case, are complementary to these few-shot learning methods and yield further gains.},
  urldate   = {2023-10-08},
  booktitle = {Proceedings of the 2nd {Workshop} on {Natural} {Language} {Processing} for {Conversational} {AI}},
  publisher = {Association for Computational Linguistics},
  author    = {Krone, Jason and Zhang, Yi and Diab, Mona},
  month     = jul,
  year      = {2020},
  pages     = {96--108}
}

@misc{hudecek_are_2023,
  title     = {Are {LLMs} {All} {You} {Need} for {Task}-{Oriented} {Dialogue}?},
  url       = {http://arxiv.org/abs/2304.06556},
  doi       = {10.48550/arXiv.2304.06556},
  abstract  = {Instructions-tuned Large Language Models (LLMs) gained recently huge popularity thanks to their ability to interact with users through conversation. In this work we aim to evaluate their ability to complete multi-turn tasks and interact with external databases in the context of established task-oriented dialogue benchmarks. We show that for explicit belief state tracking, LLMs underperform compared to specialized task-specific models. Nevertheless, they show ability to guide the dialogue to successful ending if given correct slot values. Furthermore this ability improves with access to true belief state distribution or in-domain examples.},
  urldate   = {2023-08-04},
  publisher = {arXiv},
  author    = {Hudeček, Vojtěch and Dušek, Ondřej},
  month     = aug,
  year      = {2023},
  note      = {arXiv:2304.06556 [cs]},
  keywords  = {Computer Science - Computation and Language}
}

@article{bellagente2024stable,
  title   = {Stable LM 2 1.6 B Technical Report},
  author  = {Bellagente, Marco and Tow, Jonathan and Mahan, Dakota and Phung, Duy and Zhuravinskyi, Maksym and Adithyan, Reshinth and Baicoianu, James and Brooks, Ben and Cooper, Nathan and Datta, Ashish and others},
  journal = {arXiv preprint arXiv:2402.17834},
  year    = {2024}
}

@inproceedings{dey_towards_2022,
  address   = {Dublin, Ireland},
  title     = {Towards {Fair} {Evaluation} of {Dialogue} {State} {Tracking} by {Flexible} {Incorporation} of {Turn}-level {Performances}},
  url       = {https://aclanthology.org/2022.acl-short.35},
  doi       = {10.18653/v1/2022.acl-short.35},
  abstract  = {Dialogue State Tracking (DST) is primarily evaluated using Joint Goal Accuracy (JGA) defined as the fraction of turns where the ground-truth dialogue state exactly matches the prediction. Generally in DST, the dialogue state or belief state for a given turn contain all the intents shown by the user till that turn. Due to this cumulative nature of the belief state, it is difficult to get a correct prediction once a misprediction has occurred. Thus, although being a useful metric, it can be harsh at times and underestimate the true potential of a DST model. Moreover, an improvement in JGA can sometimes decrease the performance of turn-level or non-cumulative belief state prediction due to inconsistency in annotations. So, using JGA as the only metric for model selection may not be ideal for all scenarios. In this work, we discuss various evaluation metrics used for DST along with their shortcomings. To address the existing issues, we propose a new evaluation metric named Flexible Goal Accuracy (FGA). FGA is a generalized version of JGA. But unlike JGA, it tries to give penalized rewards to mispredictions that are locally correct i.e. the root cause of the error is an earlier turn. By doing so, FGA considers the performance of both cumulative and turn-level prediction flexibly and provides a better insight than the existing metrics. We also show that FGA is a better discriminator of DST model performance.},
  urldate   = {2023-08-04},
  booktitle = {Proceedings of the 60th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics} ({Volume} 2: {Short} {Papers})},
  publisher = {Association for Computational Linguistics},
  author    = {Dey, Suvodip and Kummara, Ramamohan and Desarkar, Maunendra},
  month     = may,
  year      = {2022},
  pages     = {318--324}
}

@inproceedings{wang_deepstruct_2022,
  address    = {Dublin, Ireland},
  title      = {{DeepStruct}: {Pretraining} of {Language} {Models} for {Structure} {Prediction}},
  shorttitle = {{DeepStruct}},
  url        = {https://aclanthology.org/2022.findings-acl.67},
  doi        = {10.18653/v1/2022.findings-acl.67},
  abstract   = {We introduce a method for improving the structural understanding abilities of language models. Unlike previous approaches that finetune the models with task-specific augmentation, we pretrain language models to generate structures from the text on a collection of task-agnostic corpora. Our structure pretraining enables zero-shot transfer of the learned knowledge that models have about the structure tasks. We study the performance of this approach on 28 datasets, spanning 10 structure prediction tasks including open information extraction, joint entity and relation extraction, named entity recognition, relation classification, semantic role labeling, event extraction, coreference resolution, factual probe, intent detection, and dialogue state tracking. We further enhance the pretraining with the task-specific training sets. We show that a 10B parameter language model transfers non-trivially to most tasks and obtains state-of-the-art performance on 21 of 28 datasets that we evaluate. Our code and datasets will be made publicly available.},
  urldate    = {2023-08-04},
  booktitle  = {Findings of the {Association} for {Computational} {Linguistics}: {ACL} 2022},
  publisher  = {Association for Computational Linguistics},
  author     = {Wang, Chenguang and Liu, Xiao and Chen, Zui and Hong, Haoyun and Tang, Jie and Song, Dawn},
  month      = may,
  year       = {2022},
  pages      = {803--823}
}

@misc{rastogi_towards_2020,
  title      = {Towards {Scalable} {Multi}-domain {Conversational} {Agents}: {The} {Schema}-{Guided} {Dialogue} {Dataset}},
  shorttitle = {Towards {Scalable} {Multi}-domain {Conversational} {Agents}},
  url        = {http://arxiv.org/abs/1909.05855},
  doi        = {10.48550/arXiv.1909.05855},
  abstract   = {Virtual assistants such as Google Assistant, Alexa and Siri provide a conversational interface to a large number of services and APIs spanning multiple domains. Such systems need to support an ever-increasing number of services with possibly overlapping functionality. Furthermore, some of these services have little to no training data available. Existing public datasets for task-oriented dialogue do not sufficiently capture these challenges since they cover few domains and assume a single static ontology per domain. In this work, we introduce the the Schema-Guided Dialogue (SGD) dataset, containing over 16k multi-domain conversations spanning 16 domains. Our dataset exceeds the existing task-oriented dialogue corpora in scale, while also highlighting the challenges associated with building large-scale virtual assistants. It provides a challenging testbed for a number of tasks including language understanding, slot filling, dialogue state tracking and response generation. Along the same lines, we present a schema-guided paradigm for task-oriented dialogue, in which predictions are made over a dynamic set of intents and slots, provided as input, using their natural language descriptions. This allows a single dialogue system to easily support a large number of services and facilitates simple integration of new services without requiring additional training data. Building upon the proposed paradigm, we release a model for dialogue state tracking capable of zero-shot generalization to new APIs, while remaining competitive in the regular setting.},
  urldate    = {2023-08-04},
  publisher  = {arXiv},
  author     = {Rastogi, Abhinav and Zang, Xiaoxue and Sunkara, Srinivas and Gupta, Raghav and Khaitan, Pranav},
  month      = jan,
  year       = {2020},
  note       = {arXiv:1909.05855 [cs]},
  keywords   = {Computer Science - Computation and Language}
}

@inproceedings{Liu2022,
  author     = {Liu, Xiao and Ji, Kaixuan and Fu, Yicheng and Tam, Weng and Du, Zhengxiao and Yang, Zhilin and Tang, Jie},
  booktitle  = {Proceedings of the 60th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics} ({Volume} 2: {Short} {Papers})},
  title      = {P-{Tuning}: {Prompt} {Tuning} {Can} {Be} {Comparable} to {Fine}-tuning {Across} {Scales} and {Tasks}},
  year       = {2022},
  address    = {Dublin, Ireland},
  month      = may,
  pages      = {61--68},
  publisher  = {Association for Computational Linguistics},
  abstract   = {Prompt tuning, which only tunes continuous prompts with a frozen language model, substantially reduces per-task storage and memory usage at training. However, in the context of NLU, prior work reveals that prompt tuning does not perform well for normal-sized pretrained models. We also find that existing methods of prompt tuning cannot handle hard sequence labeling tasks, indicating a lack of universality. We present a novel empirical finding that properly optimized prompt tuning can be universally effective across a wide range of model scales and NLU tasks. It matches the performance of finetuning while having only 0.1\%-3\% tuned parameters. Our method P-Tuning v2 is an implementation of Deep Prompt Tuning (CITATION) optimized and adapted for NLU. Given the universality and simplicity of P-Tuning v2, we believe it can serve as an alternative to finetuning and a strong baseline for future research.},
  doi        = {10.18653/v1/2022.acl-short.8},
  file       = {:Liu2022 - P Tuning_ Prompt Tuning Can Be Comparable to Fine Tuning across Scales and Tasks.pdf:PDF},
  shorttitle = {P-{Tuning}},
  url        = {https://aclanthology.org/2022.acl-short.8},
  urldate    = {2023-06-22}
}


@techreport{Touvron2023,
  author     = {Touvron, Hugo and Lavril, Thibaut and Izacard, Gautier and Martinet, Xavier and Lachaux, Marie-Anne and Lacroix, Timothée and Rozière, Baptiste and Goyal, Naman and Hambro, Eric and Azhar, Faisal and Rodriguez, Aurelien and Joulin, Armand and Grave, Edouard and Lample, Guillaume},
  title      = {{LLaMA}: {Open} and {Efficient} {Foundation} {Language} {Models}},
  year       = {2023},
  month      = feb,
  note       = {arXiv:2302.13971 [cs] type: article},
  abstract   = {We introduce LLaMA, a collection of foundation language models ranging from 7B to 65B parameters. We train our models on trillions of tokens, and show that it is possible to train state-of-the-art models using publicly available datasets exclusively, without resorting to proprietary and inaccessible datasets. In particular, LLaMA-13B outperforms GPT-3 (175B) on most benchmarks, and LLaMA-65B is competitive with the best models, Chinchilla-70B and PaLM-540B. We release all our models to the research community.},
  file       = {:Touvron2023 - LLaMA_ Open and Efficient Foundation Language Models.pdf:PDF},
  keywords   = {Computer Science - Computation and Language},
  school     = {arXiv},
  shorttitle = {{LLaMA}},
  url        = {http://arxiv.org/abs/2302.13971},
  urldate    = {2023-06-22}
}


@techreport{Ji2023,
  author     = {Ji, Yunjie and Deng, Yong and Gong, Yan and Peng, Yiping and Niu, Qiang and Zhang, Lei and Ma, Baochang and Li, Xiangang},
  title      = {Exploring the {Impact} of {Instruction} {Data} {Scaling} on {Large} {Language} {Models}: {An} {Empirical} {Study} on {Real}-{World} {Use} {Cases}},
  year       = {2023},
  month      = mar,
  note       = {arXiv:2303.14742 [cs] type: article},
  abstract   = {The success of ChatGPT has recently attracted numerous efforts to replicate it, with instruction-tuning strategies being a key factor in achieving remarkable results. Instruction-tuning not only significantly enhances the model's performance and generalization but also makes the model's generated results more consistent with human speech patterns. However current research rarely studies the impact of different amounts of instruction data on model performance, especially in the real-world use cases. In this paper we explore the performance of large language models based on instruction tuning across different scales of instruction data. An evaluation dataset consisting of 12 major online use cases is constructed in the experiment. With Bloomz-7B1-mt as the base model, the results show that 1) merely increasing the amount of instruction data leads to continuous improvement in tasks such as open-ended generation, 2) in tasks such as math and code, the model performance curve remains quite flat while increasing data size. We further analyze the possible causes of these phenomena and propose potential future research directions such as effectively selecting high-quality training data, scaling base models and training methods specialized for hard tasks. We will release our training and evaluation datasets, as well as model checkpoints.},
  file       = {:Ji2023 - Exploring the Impact of Instruction Data Scaling on Large Language Models_ an Empirical Study on Real World Use Cases.pdf:PDF},
  keywords   = {Computer Science - Computation and Language},
  school     = {arXiv},
  shorttitle = {Exploring the {Impact} of {Instruction} {Data} {Scaling} on {Large} {Language} {Models}},
  url        = {http://arxiv.org/abs/2303.14742},
  urldate    = {2023-06-22}
}


@techreport{Lester2021,
  author   = {Lester, Brian and Al-Rfou, Rami and Constant, Noah},
  title    = {The {Power} of {Scale} for {Parameter}-{Efficient} {Prompt} {Tuning}},
  year     = {2021},
  month    = sep,
  note     = {arXiv:2104.08691 [cs] type: article},
  abstract = {In this work, we explore "prompt tuning", a simple yet effective mechanism for learning "soft prompts" to condition frozen language models to perform specific downstream tasks. Unlike the discrete text prompts used by GPT-3, soft prompts are learned through backpropagation and can be tuned to incorporate signal from any number of labeled examples. Our end-to-end learned approach outperforms GPT-3's "few-shot" learning by a large margin. More remarkably, through ablations on model size using T5, we show that prompt tuning becomes more competitive with scale: as models exceed billions of parameters, our method "closes the gap" and matches the strong performance of model tuning (where all model weights are tuned). This finding is especially relevant in that large models are costly to share and serve, and the ability to reuse one frozen model for multiple downstream tasks can ease this burden. Our method can be seen as a simplification of the recently proposed "prefix tuning" of Li and Liang (2021), and we provide a comparison to this and other similar approaches. Finally, we show that conditioning a frozen model with soft prompts confers benefits in robustness to domain transfer, as compared to full model tuning.},
  annote   = {Comment: Accepted to EMNLP 2021},
  file     = {:Lester2021 - The Power of Scale for Parameter Efficient Prompt Tuning.pdf:PDF},
  keywords = {Computer Science - Computation and Language},
  school   = {arXiv},
  url      = {http://arxiv.org/abs/2104.08691},
  urldate  = {2023-06-22}
}


@techreport{Wang2021,
  author     = {Wang, Shuohang and Liu, Yang and Xu, Yichong and Zhu, Chenguang and Zeng, Michael},
  title      = {Want {To} {Reduce} {Labeling} {Cost}? {GPT}-3 {Can} {Help}},
  year       = {2021},
  month      = aug,
  note       = {arXiv:2108.13487 [cs] type: article},
  abstract   = {Data annotation is a time-consuming and labor-intensive process for many NLP tasks. Although there exist various methods to produce pseudo data labels, they are often task-specific and require a decent amount of labeled data to start with. Recently, the immense language model GPT-3 with 175 billion parameters has achieved tremendous improvement across many few-shot learning tasks. In this paper, we explore ways to leverage GPT-3 as a low-cost data labeler to train other models. We find that, to make the downstream model achieve the same performance on a variety of NLU and NLG tasks, it costs 50\% to 96\% less to use labels from GPT-3 than using labels from humans. Furthermore, we propose a novel framework of combining pseudo labels from GPT-3 with human labels, which leads to even better performance with limited labeling budget. These results present a cost-effective data labeling methodology that is generalizable to many practical applications.},
  annote     = {Comment: Findings of EMNLP 2021, 11 pages},
  file       = {:Wang2021 - Want to Reduce Labeling Cost_ GPT 3 Can Help.pdf:PDF},
  keywords   = {Computer Science - Computation and Language, Computer Science - Artificial Intelligence},
  school     = {arXiv},
  shorttitle = {Want {To} {Reduce} {Labeling} {Cost}?},
  url        = {http://arxiv.org/abs/2108.13487},
  urldate    = {2023-06-22}
}


@techreport{Liu2019,
  author     = {Liu, Yinhan and Ott, Myle and Goyal, Naman and Du, Jingfei and Joshi, Mandar and Chen, Danqi and Levy, Omer and Lewis, Mike and Zettlemoyer, Luke and Stoyanov, Veselin},
  title      = {{RoBERTa}: {A} {Robustly} {Optimized} {BERT} {Pretraining} {Approach}},
  year       = {2019},
  month      = jul,
  note       = {arXiv:1907.11692 [cs] type: article},
  abstract   = {Language model pretraining has led to significant performance gains but careful comparison between different approaches is challenging. Training is computationally expensive, often done on private datasets of different sizes, and, as we will show, hyperparameter choices have significant impact on the final results. We present a replication study of BERT pretraining (Devlin et al., 2019) that carefully measures the impact of many key hyperparameters and training data size. We find that BERT was significantly undertrained, and can match or exceed the performance of every model published after it. Our best model achieves state-of-the-art results on GLUE, RACE and SQuAD. These results highlight the importance of previously overlooked design choices, and raise questions about the source of recently reported improvements. We release our models and code.},
  file       = {:Liu2019 - RoBERTa_ a Robustly Optimized BERT Pretraining Approach.pdf:PDF},
  keywords   = {Computer Science - Computation and Language},
  school     = {arXiv},
  shorttitle = {{RoBERTa}},
  url        = {http://arxiv.org/abs/1907.11692},
  urldate    = {2023-06-20}
}

@misc{Taori2023,
  author       = {Rohan Taori and Ishaan Gulrajani and Tianyi Zhang and Yann Dubois and Xuechen Li and Carlos Guestrin and Percy Liang and Tatsunori B. Hashimoto},
  howpublished = {\url{https://github.com/tatsu-lab/stanford_alpaca}},
  title        = {Stanford Alpaca: An Instruction-following LLaMA model},
  year         = {2023},
  journal      = {GitHub repository},
  publisher    = {GitHub}
}

@misc{Chiang2023,
  author = {Chiang, Wei-Lin and Li, Zhuohan and Lin, Zi and Sheng, Ying and Wu, Zhanghao and Zhang, Hao and Zheng, Lianmin and Zhuang, Siyuan and Zhuang, Yonghao and Gonzalez, Joseph E. and Stoica, Ion and Xing, Eric P.},
  month  = {March},
  title  = {Vicuna: An Open-Source Chatbot Impressing GPT-4 with 90\%* ChatGPT Quality},
  year   = {2023},
  url    = {https://lmsys.org/blog/2023-03-30-vicuna/}
}

@inproceedings{Yin2022,
  author    = {Yin, Wenpeng and Li, Jia and Xiong, Caiming},
  booktitle = {Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
  title     = {{C}on{T}in{T}in: Continual Learning from Task Instructions},
  year      = {2022},
  address   = {Dublin, Ireland},
  month     = may,
  pages     = {3062--3072},
  publisher = {Association for Computational Linguistics},
  abstract  = {The mainstream machine learning paradigms for NLP often work with two underlying presumptions. First, the target task is predefined and static; a system merely needs to learn to solve it exclusively. Second, the supervision of a task mainly comes from a set of labeled examples. A question arises: how to build a system that can keep learning new tasks from their instructions?This work defines a new learning paradigm ConTinTin (Continual Learning from Task Instructions), in which a system should learn a sequence of new tasks one by one, each task is explained by a piece of textual instruction. The system is required to (i) generate the expected outputs of a new task by learning from its instruction, (ii) transfer the knowledge acquired from upstream tasks to help solve downstream tasks (i.e., forward-transfer), and (iii) retain or even improve the performance on earlier tasks after learning new tasks (i.e., backward-transfer). This new problem is studied on a stream of more than 60 tasks, each equipped with an instruction. Technically, our method InstructionSpeak contains two strategies that make full use of task instructions to improve forward-transfer and backward-transfer: one is to learn from negative outputs, the other is to re-visit instructions of previous tasks. To our knowledge, this is the first time to study ConTinTin in NLP. In addition to the problem formulation and our promising approach, this work also contributes to providing rich analyses for the community to better understand this novel learning problem.},
  doi       = {10.18653/v1/2022.acl-long.218},
  url       = {https://aclanthology.org/2022.acl-long.218}
}

@inproceedings{Scialom2022,
  author    = {Scialom, Thomas and Chakrabarty, Tuhin and Muresan, Smaranda},
  booktitle = {Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing},
  title     = {Fine-tuned Language Models are Continual Learners},
  year      = {2022},
  address   = {Abu Dhabi, United Arab Emirates},
  month     = dec,
  pages     = {6107--6122},
  publisher = {Association for Computational Linguistics},
  abstract  = {Recent work on large language models relies on the intuition that most natural language processing tasks can be described via natural language instructions and that models trained on these instructions show strong zero-shot performance on several standard datasets. However, these models even though impressive still perform poorly on a wide range of tasks outside of their respective training and evaluation sets.To address this limitation, we argue that a model should be able to keep extending its knowledge and abilities, without forgetting previous skills. In spite of the limited success of Continual Learning, we show that \textit{Fine-tuned Language Models can be continual learners}.We empirically investigate the reason for this success and conclude that Continual Learning emerges from self-supervision pre-training. Our resulting model Continual-T0 (CT0) is able to learn 8 new diverse language generation tasks, while still maintaining good performance on previous tasks, spanning in total of 70 datasets. Finally, we show that CT0 is able to combine instructions in ways it was never trained for, demonstrating some level of instruction compositionality.},
  url       = {https://aclanthology.org/2022.emnlp-main.410}
}

@inproceedings{Gupta2022,
  author    = {Gupta, Prakhar and Jiao, Cathy and Yeh, Yi-Ting and Mehri, Shikib and Eskenazi, Maxine and Bigham, Jeffrey},
  booktitle = {Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing},
  title     = {{I}nstruct{D}ial: Improving Zero and Few-shot Generalization in Dialogue through Instruction Tuning},
  year      = {2022},
  address   = {Abu Dhabi, United Arab Emirates},
  month     = dec,
  pages     = {505--525},
  publisher = {Association for Computational Linguistics},
  abstract  = {Instruction tuning is an emergent paradigm in NLP wherein natural language instructions are leveraged with language models to induce zero-shot performance on unseen tasks. Dialogue is an especially interesting area in which to explore instruction tuning because dialogue systems perform multiple kinds of tasks related to language (e.g., natural language understanding and generation, domain-specific interaction), yet instruction tuning has not been systematically explored for dialogue-related tasks. We introduce InstructDial, an instruction tuning framework for dialogue, which consists of a repository of 48 diverse dialogue tasks in a unified text-to-text format created from 59 openly available dialogue datasets. We explore cross-task generalization ability on models tuned on InstructDial across diverse dialogue tasks. Our analysis reveals that InstructDial enables good zero-shot performance on unseen datasets and tasks such as dialogue evaluation and intent detection, and even better performance in a few-shot setting. To ensure that models adhere to instructions, we introduce novel meta-tasks. We establish benchmark zero-shot and few-shot performance of models trained using the proposed framework on multiple dialogue tasks.},
  url       = {https://aclanthology.org/2022.emnlp-main.33}
}

'
@techreport{Gunasekar2023,
  author   = {Gunasekar, Suriya and Zhang, Yi and Aneja, Jyoti and Mendes, Caio César Teodoro and Del Giorno, Allie and Gopi, Sivakanth and Javaheripi, Mojan and Kauffmann, Piero and de Rosa, Gustavo and Saarikivi, Olli and Salim, Adil and Shah, Shital and Behl, Harkirat Singh and Wang, Xin and Bubeck, Sébastien and Eldan, Ronen and Kalai, Adam Tauman and Lee, Yin Tat and Li, Yuanzhi},
  title    = {Textbooks {Are} {All} {You} {Need}},
  year     = {2023},
  month    = jun,
  note     = {arXiv:2306.11644 [cs] type: article},
  abstract = {We introduce phi-1, a new large language model for code, with significantly smaller size than competing models: phi-1 is a Transformer-based model with 1.3B parameters, trained for 4 days on 8 A100s, using a selection of ``textbook quality" data from the web (6B tokens) and synthetically generated textbooks and exercises with GPT-3.5 (1B tokens). Despite this small scale, phi-1 attains pass@1 accuracy 50.6\% on HumanEval and 55.5\% on MBPP. It also displays surprising emergent properties compared to phi-1-base, our model before our finetuning stage on a dataset of coding exercises, and phi-1-small, a smaller model with 350M parameters trained with the same pipeline as phi-1 that still achieves 45\% on HumanEval.},
  annote   = {Comment: 26 pages},
  file     = {arXiv Fulltext PDF:https\://arxiv.org/pdf/2306.11644.pdf:application/pdf},
  keywords = {Computer Science - Computation and Language, Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
  school   = {arXiv},
  url      = {http://arxiv.org/abs/2306.11644},
  urldate  = {2023-06-27}
}

'
@techreport{Hestness2017,
  author   = {Hestness, Joel and Narang, Sharan and Ardalani, Newsha and Diamos, Gregory and Jun, Heewoo and Kianinejad, Hassan and Patwary, Md Mostofa Ali and Yang, Yang and Zhou, Yanqi},
  title    = {Deep {Learning} {Scaling} is {Predictable}, {Empirically}},
  year     = {2017},
  month    = dec,
  note     = {arXiv:1712.00409 [cs, stat] type: article},
  abstract = {Deep learning (DL) creates impactful advances following a virtuous recipe: model architecture search, creating large training data sets, and scaling computation. It is widely believed that growing training sets and models should improve accuracy and result in better products. As DL application domains grow, we would like a deeper understanding of the relationships between training set size, computational scale, and model accuracy improvements to advance the state-of-the-art. This paper presents a large scale empirical characterization of generalization error and model size growth as training sets grow. We introduce a methodology for this measurement and test four machine learning domains: machine translation, language modeling, image processing, and speech recognition. Our empirical results show power-law generalization error scaling across a breadth of factors, resulting in power-law exponents---the "steepness" of the learning curve---yet to be explained by theoretical work. Further, model improvements only shift the error but do not appear to affect the power-law exponent. We also show that model size scales sublinearly with data size. These scaling relationships have significant implications on deep learning research, practice, and systems. They can assist model debugging, setting accuracy targets, and decisions about data set growth. They can also guide computing system design and underscore the importance of continued computational scaling.},
  annote   = {Comment: 19 pages, 11 figures},
  doi      = {10.48550/arXiv.1712.00409},
  file     = {:Hestness2017 - Deep Learning Scaling Is Predictable, Empirically.pdf:PDF},
  keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
  school   = {arXiv},
  url      = {http://arxiv.org/abs/1712.00409},
  urldate  = {2023-06-27}
}


@inproceedings{Mirza2024,
  author    = {Mirza, Paramita and Sudhi, Viju and Sahoo, Soumya and Bhat, Sinchana Ramakanth},
  booktitle = {Proceedings of the 2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation (LREC-COLING 2024)},
  title     = {ILLUMINER: Instruction-tuned Large Language Models as Few-shot Intent Classifier and Slot Filler},
  year      = {2024},
  address   = {Torino, Italy},
  month     = may
}

@inproceedings{Alavoine2024,
  author    = {Alavoine, Nad{\`e}ge and Laperri{\`e}re, Ga{\"e}lle and Servan, Christophe and Ghannay, Sahar and Rosset, Sophie},
  booktitle = {Proceedings of the 2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation (LREC-COLING 2024)},
  title     = {New Semantic Task for the {F}rench Spoken Language Understanding {MEDIA} Benchmark},
  year      = {2024},
  address   = {Torino, Italia},
  editor    = {Calzolari, Nicoletta and Kan, Min-Yen and Hoste, Veronique and Lenci, Alessandro and Sakti, Sakriani and Xue, Nianwen},
  month     = may,
  pages     = {12227--12246},
  publisher = {ELRA and ICCL},
  abstract  = {Intent classification and slot-filling are essential tasks of Spoken Language Understanding (SLU). In most SLU systems, those tasks are realized by independent modules, but for about fifteen years, models achieving both of them jointly and exploiting their mutual enhancement have been proposed. A multilingual module using a joint model was envisioned to create a touristic dialogue system for a European project, HumanE-AI-Net. A combination of multiple datasets, including the MEDIA dataset, was suggested for training this joint model. The MEDIA SLU dataset is a French dataset distributed since 2005 by ELRA, mainly used by the French research community and free for academic research since 2020. Unfortunately, it is annotated only in slots but not intents. An enhanced version of MEDIA annotated with intents has been built to extend its use to more tasks and use cases. This paper presents the semi-automatic methodology used to obtain this enhanced version. In addition, we present the first results of SLU experiments on this enhanced dataset using joint models for intent classification and slot-filling.},
  url       = {https://aclanthology.org/2024.lrec-main.1070}
}

@inproceedings{Fei2023,
  author    = {Fei, Hao and Li, Bobo and Liu, Qian and Bing, Lidong and Li, Fei and Chua, Tat-Seng},
  booktitle = {Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)},
  title     = {Reasoning Implicit Sentiment with Chain-of-Thought Prompting},
  year      = {2023},
  address   = {Toronto, Canada},
  editor    = {Rogers, Anna and Boyd-Graber, Jordan and Okazaki, Naoaki},
  month     = {jul},
  pages     = {1171--1182},
  publisher = {Association for Computational Linguistics},
  abstract  = {While sentiment analysis systems try to determine the sentiment polarities of given targets based on the key opinion expressions in input texts, in implicit sentiment analysis (ISA) the opinion cues come in an implicit and obscure manner. Thus detecting implicit sentiment requires the common-sense and multi-hop reasoning ability to infer the latent intent of opinion. Inspired by the recent chain-of-thought (CoT) idea, in this work we introduce a Three-hop Reasoning (THOR) CoT framework to mimic the human-like reasoning process for ISA. We design a three-step prompting principle for THOR to step-by-step induce the implicit aspect, opinion, and finally the sentiment polarity. Our THOR+Flan-T5 (11B) pushes the state-of-the-art (SoTA) by over 6{\%} F1 on supervised setup. More strikingly, THOR+GPT3 (175B) boosts the SoTA by over 50{\%} F1 on zero-shot setting.},
  doi       = {10.18653/v1/2023.acl-short.101},
  group     = {2_sentiment_emotion_classification_analysis},
  groups    = {2_sentiment_emotion_classification_analysis, ACL, 2023, Reasoning, Prompting},
  url       = {https://aclanthology.org/2023.acl-short.101}
}

'
@inproceedings{Shao2023,
  author     = {Shao, Zhihong and Gong, Yeyun and Shen, Yelong and Huang, Minlie and Duan, Nan and Chen, Weizhu},
  booktitle  = {Proceedings of the 40 th International Conference on Machine Learning},
  title      = {Synthetic {Prompting}: {Generating} {Chain}-of-{Thought} {Demonstrations} for {Large} {Language} {Models}},
  year       = {2023},
  month      = jul,
  pages      = {30706--30775},
  publisher  = {PMLR},
  abstract   = {Large language models can perform various reasoning tasks by using chain-of-thought prompting, which guides them to find answers through step-by-step demonstrations. However, the quality of the prompts depends on the demonstrations given to the models, and creating many of them by hand is costly. We introduce Synthetic prompting, a method that leverages a few handcrafted examples to prompt the model to generate more examples by itself, and selects effective demonstrations to elicit better reasoning. Our method alternates between a backward and forward process to generate new examples. The backward process generates a question that match a sampled reasoning chain, so that the question is solvable and clear. The forward process produces a more detailed reasoning chain for the question, improving the quality of the example. We evaluate our method on numerical, symbolic, and algorithmic reasoning tasks, and show that it outperforms existing prompting techniques.},
  file       = {:shao_synthetic_2023 - Synthetic Prompting_ Generating Chain of Thought Demonstrations for Large Language Models.pdf:PDF},
  groups     = {ICML 2023, Prompting},
  issn       = {2640-3498},
  language   = {en},
  shorttitle = {Synthetic {Prompting}},
  url        = {https://proceedings.mlr.press/v202/shao23a.html},
  urldate    = {2023-10-22}
}

@misc{Kaplan2020,
  author        = {Jared Kaplan and Sam McCandlish and Tom Henighan and Tom B. Brown and Benjamin Chess and Rewon Child and Scott Gray and Alec Radford and Jeffrey Wu and Dario Amodei},
  title         = {Scaling Laws for Neural Language Models},
  year          = {2020},
  archiveprefix = {arXiv},
  eprint        = {2001.08361},
  primaryclass  = {cs.LG}
}

@inproceedings{Mosquera2022,
  author     = {Mosquera, Alejandro},
  booktitle  = {Proceedings of the {GermEval} 2022 {Workshop} on {Text} {Complexity} {Assessment} of {German} {Text}},
  title      = {Tackling {Data} {Drift} with {Adversarial} {Validation}: {An} {Application} for {German} {Text} {Complexity} {Estimation}},
  year       = {2022},
  address    = {Potsdam, Germany},
  month      = sep,
  pages      = {39--44},
  publisher  = {Association for Computational Linguistics},
  abstract   = {This paper describes the winning approach in the first automated German text complexity assessment shared task as part of KONVENS 2022. To solve this difficult problem, the evaluated system relies on an ensemble of regression models that successfully combines both traditional feature engineering and pre-trained resources. Moreover, the use of adversarial validation is proposed as a method for countering the data drift identified during the development phase, thus helping to select relevant models and features and avoid leaderboard overfitting. The best submission reached 0.43 mapped RMSE on the test set during the final phase of the competition.},
  file       = {Full Text PDF:https\://aclanthology.org/2022.germeval-1.7.pdf:application/pdf},
  shorttitle = {Tackling {Data} {Drift} with {Adversarial} {Validation}},
  url        = {https://aclanthology.org/2022.germeval-1.7},
  urldate    = {2023-09-25}
}

'
@inproceedings{Pilan2018,
  author    = {Pilán, Ildikó and Volodina, Elena},
  booktitle = {Proceedings of the {Workshop} on {Linguistic} {Complexity} and {Natural} {Language} {Processing}},
  title     = {Investigating the importance of linguistic complexity features across different datasets related to language learning},
  year      = {2018},
  address   = {Santa Fe, New-Mexico},
  month     = aug,
  pages     = {49--58},
  publisher = {Association for Computational Linguistics},
  abstract  = {We present the results of our investigations aiming at identifying the most informative linguistic complexity features for classifying language learning levels in three different datasets. The datasets vary across two dimensions: the size of the instances (texts vs. sentences) and the language learning skill they involve (reading comprehension texts vs. texts written by learners themselves). We present a subset of the most predictive features for each dataset, taking into consideration significant differences in their per-class mean values and show that these subsets lead not only to simpler models, but also to an improved classification performance. Furthermore, we pinpoint fourteen central features that are good predictors regardless of the size of the linguistic unit analyzed or the skills involved, which include both morpho-syntactic and lexical dimensions.},
  file      = {Full Text PDF:https\://aclanthology.org/W18-4606.pdf:application/pdf},
  url       = {https://aclanthology.org/W18-4606},
  urldate   = {2023-09-26}
}

'
@article{Alberto2015,
  author     = {Alberto, Tulio C. and Lochter, Johannes V. and Almeida, Tiago A.},
  journal    = {2015 IEEE 14th International Conference on Machine Learning and Applications (ICMLA)},
  title      = {{TubeSpam}: {Comment} {Spam} {Filtering} on {YouTube}},
  year       = {2015},
  month      = dec,
  pages      = {138--143},
  abstract   = {The profitability promoted by Google in its brand new video distribution platform YouTube has attracted an increasing number of users. However, such success has also attracted malicious users, which aim to self-promote their videos or disseminate viruses and malwares. Since YouTube offers limited tools for comment moderation, the spam volume is shockingly increasing which lead owners of famous channels to disable the comments section in their videos. Automatic comment spam filtering on YouTube is a challenge even for established classification methods, since the messages are very short and often rife with slangs, symbols and abbreviations. In this work, we have evaluated several top-performance classification techniques for such purpose. The statistical analysis of results indicate that, with 99.9\% of confidence level, decision trees, logistic regression, Bernoulli Naive Bayes, random forests, linear and Gaussian SVMs are statistically equivalent. Based on this, we have also offered the TubeSpam - an accurate online system to filter comments posted on YouTube.},
  address    = {Miami, FL},
  annote     = {[TLDR] The statistical analysis of results indicate that, with 99.9\% of confidence level, decision trees, logistic regression, Bernoulli Naive Bayes, random forests, linear and Gaussian SVMs are statistically equivalent for comment spam filtering on YouTube.},
  doi        = {10.1109/ICMLA.2015.37},
  file       = {:Alberto2015 - TubeSpam_ Comment Spam Filtering on YouTube.html:URL},
  isbn       = {9781509002870},
  publisher  = {IEEE},
  shorttitle = {{TubeSpam}},
  url        = {http://ieeexplore.ieee.org/document/7424299/},
  urldate    = {2023-09-26}
}

@article{Malo2014,
  author  = {P. Malo and A. Sinha and P. Korhonen and J. Wallenius and P. Takala},
  journal = {Journal of the Association for Information Science and Technology},
  title   = {Good debt or bad debt: Detecting semantic orientations in economic texts},
  year    = {2014},
  volume  = {65}
}

@article{Davis2016,
  author   = {Davis, Allan Peter and Grondin, Cynthia J and Johnson, Robin J and Sciaky, Daniela and King, Benjamin L and McMorran, Roy and Wiegers, Jolene and Wiegers, Thomas C and Mattingly, Carolyn J},
  journal  = {Nucleic Acids Res},
  title    = {The Comparative Toxicogenomics Database: update 2017},
  year     = {2016},
  month    = sep,
  number   = {D1},
  pages    = {D972--D978},
  volume   = {45},
  abstract = {The Comparative Toxicogenomics Database (CTD;
              http://ctdbase.org/) provides information about interactions
              between chemicals and gene products, and their relationships to
              diseases. Core CTD content (chemical-gene, chemical-disease and
              gene-disease interactions manually curated from the literature)
              are integrated with each other as well as with select external
              datasets to generate expanded networks and predict novel
              associations. Today, core CTD includes more than 30.5 million
              toxicogenomic connections relating chemicals/drugs,
              genes/proteins, diseases, taxa, Gene Ontology (GO) annotations,
              pathways, and gene interaction modules. In this update, we report
              a 33\% increase in our core data content since 2015, describe our
              new exposure module (that harmonizes exposure science information
              with core toxicogenomic data) and introduce a novel dataset of
              GO-disease inferences (that identify common molecular
              underpinnings for seemingly unrelated pathologies). These
              advancements centralize and contextualize real-world chemical
              exposures with molecular pathways to help scientists generate
              testable hypotheses in an effort to understand the etiology and
              mechanisms underlying environmentally influenced diseases.},
  address  = {England},
  language = {en}
}

'
@article{Zhang2021,
  author     = {Zhang, Jieyu and Yu, Yue and Li, Yinghao and Wang, Yujing and Yang, Yaming and Yang, Mao and Ratner, Alexander J.},
  journal    = {ArXiv},
  title      = {{WRENCH}: {A} {Comprehensive} {Benchmark} for {Weak} {Supervision}},
  year       = {2021},
  month      = sep,
  abstract   = {Recent Weak Supervision (WS) approaches have had widespread success in easing the bottleneck of labeling training data for machine learning by synthesizing labels from multiple potentially noisy supervision sources. However, proper measurement and analysis of these approaches remain a challenge. First, datasets used in existing works are often private and/or custom, limiting standardization. Second, WS datasets with the same name and base data often vary in terms of the labels and weak supervision sources used, a significant"hidden"source of evaluation variance. Finally, WS studies often diverge in terms of the evaluation protocol and ablations used. To address these problems, we introduce a benchmark platform, WRENCH, for thorough and standardized evaluation of WS approaches. It consists of 22 varied real-world datasets for classification and sequence tagging; a range of real, synthetic, and procedurally-generated weak supervision sources; and a modular, extensible framework for WS evaluation, including implementations for popular WS methods. We use WRENCH to conduct extensive comparisons over more than 120 method variants to demonstrate its efficacy as a benchmark platform. The code is available at https://github.com/JieyuZ2/wrench.},
  annote     = {[TLDR] A benchmark platform, WRENCH, for thorough and standardized evaluation of WS approaches, which consists of 22 varied real-world datasets for classification and sequence tagging; a range of real, synthetic, and procedurally-generated weak supervision sources; and a modular, extensible framework for WS evaluation, including implementations for popular WS methods.},
  file       = {:zhang_wrench__2021 - WRENCH_ a Comprehensive Benchmark for Weak Supervision.html:URL;:Zhang2021 - WRENCH_ a Comprehensive Benchmark for Weak Supervision.pdf:PDF},
  shorttitle = {{WRENCH}},
  url        = {https://www.semanticscholar.org/paper/WRENCH%3A-A-Comprehensive-Benchmark-for-Weak-Zhang-Yu/3ba529f732d3c4a31e9ce57f1c78ddf911846bf4},
  urldate    = {2023-09-26}
}

@article{Mollas2022,
  author     = {Mollas, Ioannis and Chrysopoulou, Zoe and Karlos, Stamatis and Tsoumakas, Grigorios},
  journal    = {Complex \& Intelligent Systems},
  title      = {{ETHOS}: an {Online} {Hate} {Speech} {Detection} {Dataset}},
  year       = {2022},
  issn       = {2199-4536, 2198-6053},
  month      = dec,
  note       = {arXiv:2006.08328 [cs, stat]},
  number     = {6},
  pages      = {4663--4678},
  volume     = {8},
  abstract   = {Online hate speech is a recent problem in our society that is rising at a steady pace by leveraging the vulnerabilities of the corresponding regimes that characterise most social media platforms. This phenomenon is primarily fostered by offensive comments, either during user interaction or in the form of a posted multimedia context. Nowadays, giant corporations own platforms where millions of users log in every day, and protection from exposure to similar phenomena appears to be necessary in order to comply with the corresponding legislation and maintain a high level of service quality. A robust and reliable system for detecting and preventing the uploading of relevant content will have a significant impact on our digitally interconnected society. Several aspects of our daily lives are undeniably linked to our social profiles, making us vulnerable to abusive behaviours. As a result, the lack of accurate hate speech detection mechanisms would severely degrade the overall user experience, although its erroneous operation would pose many ethical concerns. In this paper, we present 'ETHOS', a textual dataset with two variants: binary and multi-label, based on YouTube and Reddit comments validated using the Figure-Eight crowdsourcing platform. Furthermore, we present the annotation protocol used to create this dataset: an active sampling procedure for balancing our data in relation to the various aspects defined. Our key assumption is that, even gaining a small amount of labelled data from such a time-consuming process, we can guarantee hate speech occurrences in the examined material.},
  annote     = {Comment: 16 Pages, 3 Figures, 9 Tables, Submitted to the special issue on "Intelligent Systems for Safer Social Media" of Complex \& Intelligent Systems},
  doi        = {10.1007/s40747-021-00608-2},
  file       = {arXiv Fulltext PDF:https\://arxiv.org/pdf/2006.08328.pdf:application/pdf},
  keywords   = {Computer Science - Computation and Language, Computer Science - Machine Learning, Statistics - Machine Learning, I.2.6, I.2.7, I.5.4, H.2.4},
  shorttitle = {{ETHOS}},
  url        = {http://arxiv.org/abs/2006.08328},
  urldate    = {2023-10-19}
}

@inproceedings{Socher2013,
  author    = {Socher, Richard and Perelygin, Alex and Wu, Jean and Chuang, Jason and Manning, Christopher D. and Ng, Andrew and Potts, Christopher},
  booktitle = {Proceedings of the 2013 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing}},
  title     = {Recursive {Deep} {Models} for {Semantic} {Compositionality} {Over} a {Sentiment} {Treebank}},
  year      = {2013},
  address   = {Seattle, Washington, USA},
  month     = oct,
  pages     = {1631--1642},
  publisher = {Association for Computational Linguistics},
  file      = {:Socher2013 - Recursive Deep Models for Semantic Compositionality Over a Sentiment Treebank.pdf:PDF},
  url       = {https://aclanthology.org/D13-1170},
  urldate   = {2023-10-19}
}

@inproceedings{Krallinger2017,
  author   = {Krallinger, Martin and Rabal, O. and Akhondi, S. and Pérez, M. and Santamaría, J. and Rodríguez, Gael Pérez and Tsatsaronis, G. and Intxaurrondo, Ander and López, José Antonio Baso and Nandal, U. and Buel, E. V. and Chandrasekhar, A. and Rodenburg, Marleen and Lægreid, A. and Doornenbal, Marius A. and Oyarzábal, J. and Lourenço, A. and Valencia, A.},
  title    = {Overview of the {BioCreative} {VI} chemical-protein interaction {Track}},
  year     = {2017},
  abstract = {—Despite the considerable number of available systems that recognize automatically mentions of genes/proteins and chemicals in text, only a limited number of attempts were made so far to extract interactions between them. Most biomedical relation extraction systems focus on the extraction of protein-protein or gene/chemical-disease relations. The detection of interactions between drugs and proteins/genes is of key relevance for pharmacological and clinical research, playing an important role for drug discovery, understanding of molecular mechanism of adverse drug reactions, describing drug metabolism or drawing regulatory networks of importance for systems pharmacology. The BioCreative VI - ChemProt track represents the first attempt to promote the development of systems for extracting chemical-protein interactions (CPIs), of relevance for precision medicine as well as for drug discovery and basic biomedical research. The novel ChemProt corpus consists of text exhaustively annotated by hand with mentions of chemical compounds/drugs and genes/proteins, as well as 22 different types of compound-protein relations. To focus on a subset of important relations, 5 relation classes were used for evaluation purposes, including agonist, antagonist, inhibitor, activator and substrate/product relations. A total of 13 participating teams returned 45 runs for this track. Despite the biological complexity of the considered relation types, top-scoring teams could obtain an F-measure across relation classes of 64.10\%. Performance varied depending on the relation class: for the antagonist relation class the best team obtained an F-measure of 72.56\% (precision of 80.75\%, recall of 65.87\%) while for inhibition/down-regulation the best value was of 71.48\% (with a precision of 76.51\% and a recall of 67.07\%).},
  annote   = {[TLDR] The BioCreative VI - ChemProt track represents the first attempt to promote the development of systems for extracting chemical-protein interactions (CPIs), of relevance for precision medicine as well as for drug discovery and basic biomedical research.},
  file     = {:Krallinger2017 - Overview of the BioCreative VI Chemical Protein Interaction Track.html:URL;:Krallinger2017 - Overview of the BioCreative VI Chemical Protein Interaction Track.html:URL},
  url      = {https://www.semanticscholar.org/paper/Overview-of-the-BioCreative-VI-chemical-protein-Krallinger-Rabal/eed781f498b563df5a9e8a241c67d63dd1d92ad5},
  urldate  = {2023-10-19}
}

@inproceedings{Maas2011,
  author    = {Maas, Andrew L. and Daly, Raymond E. and Pham, Peter T. and Huang, Dan and Ng, Andrew Y. and Potts, Christopher},
  booktitle = {Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies - Volume 1},
  title     = {Learning Word Vectors for Sentiment Analysis},
  year      = {2011},
  address   = {USA},
  pages     = {142–150},
  publisher = {Association for Computational Linguistics},
  series    = {HLT '11},
  abstract  = {Unsupervised vector-based approaches to semantics can model rich lexical meanings, but they largely fail to capture sentiment information that is central to many word meanings and important for a wide range of NLP tasks. We present a model that uses a mix of unsupervised and supervised techniques to learn word vectors capturing semantic term--document information as well as rich sentiment content. The proposed model can leverage both continuous and multi-dimensional sentiment information as well as non-sentiment annotations. We instantiate the model to utilize the document-level sentiment polarity annotations present in many online documents (e.g. star ratings). We evaluate the model using small, widely used sentiment and subjectivity corpora and find it out-performs several previously introduced methods for sentiment classification. We also introduce a large dataset of movie reviews to serve as a more robust benchmark for work in this area.},
  isbn      = {9781932432879},
  location  = {Portland, Oregon},
  numpages  = {9}
}

'
@techreport{Liu2021,
  author     = {Liu, Pengfei and Yuan, Weizhe and Fu, Jinlan and Jiang, Zhengbao and Hayashi, Hiroaki and Neubig, Graham},
  title      = {Pre-train, {Prompt}, and {Predict}: {A} {Systematic} {Survey} of {Prompting} {Methods} in {Natural} {Language} {Processing}},
  year       = {2021},
  month      = jul,
  note       = {arXiv:2107.13586 [cs] type: article},
  abstract   = {This paper surveys and organizes research works in a new paradigm in natural language processing, which we dub "prompt-based learning". Unlike traditional supervised learning, which trains a model to take in an input x and predict an output y as P(y{\textbar}x), prompt-based learning is based on language models that model the probability of text directly. To use these models to perform prediction tasks, the original input x is modified using a template into a textual string prompt x' that has some unfilled slots, and then the language model is used to probabilistically fill the unfilled information to obtain a final string x, from which the final output y can be derived. This framework is powerful and attractive for a number of reasons: it allows the language model to be pre-trained on massive amounts of raw text, and by defining a new prompting function the model is able to perform few-shot or even zero-shot learning, adapting to new scenarios with few or no labeled data. In this paper we introduce the basics of this promising paradigm, describe a unified set of mathematical notations that can cover a wide variety of existing work, and organize existing work along several dimensions, e.g.the choice of pre-trained models, prompts, and tuning strategies. To make the field more accessible to interested beginners, we not only make a systematic review of existing works and a highly structured typology of prompt-based concepts, but also release other resources, e.g., a website http://pretrain.nlpedia.ai/ including constantly-updated survey, and paperlist.},
  annote     = {Comment: Website: http://pretrain.nlpedia.ai/},
  file       = {:Liu2021 - Pre Train, Prompt, and Predict_ a Systematic Survey of Prompting Methods in Natural Language Processing.pdf:PDF},
  keywords   = {Computer Science - Computation and Language, Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
  school     = {arXiv},
  shorttitle = {Pre-train, {Prompt}, and {Predict}},
  url        = {http://arxiv.org/abs/2107.13586},
  urldate    = {2023-06-05}
}

'
@techreport{Smith2022,
  author     = {Smith, Ryan and Fries, Jason A. and Hancock, Braden and Bach, Stephen H.},
  title      = {Language {Models} in the {Loop}: {Incorporating} {Prompting} into {Weak} {Supervision}},
  year       = {2022},
  month      = may,
  note       = {arXiv:2205.02318 [cs] type: article},
  abstract   = {We propose a new strategy for applying large pre-trained language models to novel tasks when labeled training data is limited. Rather than apply the model in a typical zero-shot or few-shot fashion, we treat the model as the basis for labeling functions in a weak supervision framework. To create a classifier, we first prompt the model to answer multiple distinct queries about an example and define how the possible responses should be mapped to votes for labels and abstentions. We then denoise these noisy label sources using the Snorkel system and train an end classifier with the resulting training data. Our experimental evaluation shows that prompting large language models within a weak supervision framework can provide significant gains in accuracy. On the WRENCH weak supervision benchmark, this approach can significantly improve over zero-shot performance, an average 19.5\% reduction in errors. We also find that this approach produces classifiers with comparable or superior accuracy to those trained from hand-engineered rules.},
  file       = {:Smith2022 - Language Models in the Loop_ Incorporating Prompting into Weak Supervision.pdf:PDF},
  keywords   = {Computer Science - Machine Learning, Computer Science - Computation and Language},
  school     = {arXiv},
  shorttitle = {Language {Models} in the {Loop}},
  url        = {http://arxiv.org/abs/2205.02318},
  urldate    = {2023-06-05}
}

'
@techreport{Hsieh2023,
  author   = {Hsieh, Cheng-Yu and Li, Chun-Liang and Yeh, Chih-Kuan and Nakhost, Hootan and Fujii, Yasuhisa and Ratner, Alexander and Krishna, Ranjay and Lee, Chen-Yu and Pfister, Tomas},
  title    = {Distilling {Step}-by-{Step}! {Outperforming} {Larger} {Language} {Models} with {Less} {Training} {Data} and {Smaller} {Model} {Sizes}},
  year     = {2023},
  month    = may,
  note     = {arXiv:2305.02301 [cs] type: article},
  abstract = {Deploying large language models (LLMs) is challenging because they are memory inefficient and compute-intensive for practical applications. In reaction, researchers train smaller task-specific models by either finetuning with human labels or distilling using LLM-generated labels. However, finetuning and distillation require large amounts of training data to achieve comparable performance to LLMs. We introduce Distilling step-by-step, a new mechanism that (a) trains smaller models that outperform LLMs, and (b) achieves so by leveraging less training data needed by finetuning or distillation. Our method extracts LLM rationales as additional supervision for small models within a multi-task training framework. We present three findings across 4 NLP benchmarks: First, compared to both finetuning and distillation, our mechanism achieves better performance with much fewer labeled/unlabeled training examples. Second, compared to LLMs, we achieve better performance using substantially smaller model sizes. Third, we reduce both the model size and the amount of data required to outperform LLMs; our 770M T5 model outperforms the 540B PaLM model using only 80\% of available data on a benchmark task.},
  annote   = {Comment: Accepted to Findings of ACL 2023},
  file     = {:Hsieh2023 - Distilling Step by Step! Outperforming Larger Language Models with Less Training Data and Smaller Model Sizes.pdf:PDF},
  keywords = {Computer Science - Computation and Language, Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
  school   = {arXiv},
  url      = {http://arxiv.org/abs/2305.02301},
  urldate  = {2023-06-05}
}

'
@techreport{Ludan2023,
  author   = {Ludan, Josh Magnus and Meng, Yixuan and Nguyen, Tai and Shah, Saurabh and Lyu, Qing and Apidianaki, Marianna and Callison-Burch, Chris},
  title    = {Explanation-based {Finetuning} {Makes} {Models} {More} {Robust} to {Spurious} {Cues}},
  year     = {2023},
  month    = may,
  note     = {arXiv:2305.04990 [cs] version: 2 type: article},
  abstract = {Large Language Models (LLMs) are so powerful that they sometimes learn correlations between labels and features that are irrelevant to the task, leading to poor generalization on out-of-distribution data. We propose explanation-based finetuning as a general approach to mitigate LLMs' reliance on spurious correlations. Unlike standard finetuning where the model only predicts the answer given the input, we finetune the model to additionally generate a free-text explanation supporting its answer. To evaluate our method, we finetune the model on artificially constructed training sets containing different types of spurious cues, and test it on a test set without these cues. Compared to standard finetuning, our method makes GPT-3 (davinci) remarkably more robust against spurious cues in terms of accuracy drop across four classification tasks: ComVE (+1.2), CREAK (+9.1), e-SNLI (+15.4), and SBIC (+6.5). The efficacy generalizes across multiple model families and scales, with greater gains for larger models. Finally, our method also works well with explanations generated by the model, implying its applicability to more datasets without human-written explanations.},
  file     = {:Ludan2023 - Explanation Based Finetuning Makes Models More Robust to Spurious Cues.pdf:PDF},
  keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning},
  school   = {arXiv},
  url      = {http://arxiv.org/abs/2305.04990},
  urldate  = {2023-06-05}
}

'
@techreport{Qin2023,
  author   = {Qin, Chengwei and Zhang, Aston and Zhang, Zhuosheng and Chen, Jiaao and Yasunaga, Michihiro and Yang, Diyi},
  title    = {Is {ChatGPT} a {General}-{Purpose} {Natural} {Language} {Processing} {Task} {Solver}?},
  year     = {2023},
  month    = feb,
  note     = {arXiv:2302.06476 [cs] type: article},
  abstract = {Spurred by advancements in scale, large language models (LLMs) have demonstrated the ability to perform a variety of natural language processing (NLP) tasks zero-shot -- i.e., without adaptation on downstream data. Recently, the debut of ChatGPT has drawn a great deal of attention from the natural language processing (NLP) community due to the fact that it can generate high-quality responses to human input and self-correct previous mistakes based on subsequent conversations. However, it is not yet known whether ChatGPT can serve as a generalist model that can perform many NLP tasks zero-shot. In this work, we empirically analyze the zero-shot learning ability of ChatGPT by evaluating it on 20 popular NLP datasets covering 7 representative task categories. With extensive empirical studies, we demonstrate both the effectiveness and limitations of the current version of ChatGPT. We find that ChatGPT performs well on many tasks favoring reasoning capabilities (e.g., arithmetic reasoning) while it still faces challenges when solving specific tasks such as sequence tagging. We additionally provide in-depth analysis through qualitative case studies.},
  file     = {:Qin2023 - Is ChatGPT a General Purpose Natural Language Processing Task Solver_.pdf:PDF},
  keywords = {Computer Science - Computation and Language, Computer Science - Artificial Intelligence},
  school   = {arXiv},
  url      = {http://arxiv.org/abs/2302.06476},
  urldate  = {2023-06-12}
}

'
@inproceedings{Brown2020,
  author    = {Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and Agarwal, Sandhini and Herbert-Voss, Ariel and Krueger, Gretchen and Henighan, Tom and Child, Rewon and Ramesh, Aditya and Ziegler, Daniel and Wu, Jeffrey and Winter, Clemens and Hesse, Chris and Chen, Mark and Sigler, Eric and Litwin, Mateusz and Gray, Scott and Chess, Benjamin and Clark, Jack and Berner, Christopher and McCandlish, Sam and Radford, Alec and Sutskever, Ilya and Amodei, Dario},
  booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
  title     = {Language {Models} are {Few}-{Shot} {Learners}},
  year      = {2020},
  pages     = {1877--1901},
  publisher = {Curran Associates, Inc.},
  volume    = {33},
  abstract  = {We demonstrate that scaling up language models greatly improves task-agnostic, few-shot performance, sometimes even becoming competitive with prior state-of-the-art fine-tuning approaches. Specifically, we train GPT-3, an autoregressive language model with 175 billion parameters, 10x more than any previous non-sparse language model, and test its performance in the few-shot setting.  For all tasks, GPT-3 is applied without any gradient updates or fine-tuning, with tasks and few-shot demonstrations specified purely via text interaction with the model.  GPT-3 achieves strong performance on many NLP datasets, including translation, question-answering, and cloze tasks. We also identify some datasets where GPT-3's few-shot learning still struggles, as well as some datasets where GPT-3 faces methodological issues related to training on large web corpora.},
  file      = {:Brown2020a - Language Models Are Few Shot Learners.pdf:PDF},
  url       = {https://proceedings.neurips.cc/paper_files/paper/2020/hash/1457c0d6bfcb4967418bfb8ac142f64a-Abstract.html},
  urldate   = {2023-06-13}
}

'
@techreport{Hoffmann2022,
  author   = {Hoffmann, Jordan and Borgeaud, Sebastian and Mensch, Arthur and Buchatskaya, Elena and Cai, Trevor and Rutherford, Eliza and Casas, Diego de Las and Hendricks, Lisa Anne and Welbl, Johannes and Clark, Aidan and Hennigan, Tom and Noland, Eric and Millican, Katie and Driessche, George van den and Damoc, Bogdan and Guy, Aurelia and Osindero, Simon and Simonyan, Karen and Elsen, Erich and Rae, Jack W. and Vinyals, Oriol and Sifre, Laurent},
  title    = {Training {Compute}-{Optimal} {Large} {Language} {Models}},
  year     = {2022},
  month    = mar,
  note     = {arXiv:2203.15556 [cs] type: article},
  abstract = {We investigate the optimal model size and number of tokens for training a transformer language model under a given compute budget. We find that current large language models are significantly undertrained, a consequence of the recent focus on scaling language models whilst keeping the amount of training data constant. By training over 400 language models ranging from 70 million to over 16 billion parameters on 5 to 500 billion tokens, we find that for compute-optimal training, the model size and the number of training tokens should be scaled equally: for every doubling of model size the number of training tokens should also be doubled. We test this hypothesis by training a predicted compute-optimal model, Chinchilla, that uses the same compute budget as Gopher but with 70B parameters and 4\${\textbackslash}times\$ more more data. Chinchilla uniformly and significantly outperforms Gopher (280B), GPT-3 (175B), Jurassic-1 (178B), and Megatron-Turing NLG (530B) on a large range of downstream evaluation tasks. This also means that Chinchilla uses substantially less compute for fine-tuning and inference, greatly facilitating downstream usage. As a highlight, Chinchilla reaches a state-of-the-art average accuracy of 67.5\% on the MMLU benchmark, greater than a 7\% improvement over Gopher.},
  file     = {:Hoffmann2022 - Training Compute Optimal Large Language Models.pdf:PDF},
  keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning},
  school   = {arXiv},
  url      = {http://arxiv.org/abs/2203.15556},
  urldate  = {2023-06-13}
}

'
@techreport{Chowdhery2022,
  author     = {Chowdhery, Aakanksha and Narang, Sharan and Devlin, Jacob and Bosma, Maarten and Mishra, Gaurav and Roberts, Adam and Barham, Paul and Chung, Hyung Won and Sutton, Charles and Gehrmann, Sebastian and Schuh, Parker and Shi, Kensen and Tsvyashchenko, Sasha and Maynez, Joshua and Rao, Abhishek and Barnes, Parker and Tay, Yi and Shazeer, Noam and Prabhakaran, Vinodkumar and Reif, Emily and Du, Nan and Hutchinson, Ben and Pope, Reiner and Bradbury, James and Austin, Jacob and Isard, Michael and Gur-Ari, Guy and Yin, Pengcheng and Duke, Toju and Levskaya, Anselm and Ghemawat, Sanjay and Dev, Sunipa and Michalewski, Henryk and Garcia, Xavier and Misra, Vedant and Robinson, Kevin and Fedus, Liam and Zhou, Denny and Ippolito, Daphne and Luan, David and Lim, Hyeontaek and Zoph, Barret and Spiridonov, Alexander and Sepassi, Ryan and Dohan, David and Agrawal, Shivani and Omernick, Mark and Dai, Andrew M. and Pillai, Thanumalayan Sankaranarayana and Pellat, Marie and Lewkowycz, Aitor and Moreira, Erica and Child, Rewon and Polozov, Oleksandr and Lee, Katherine and Zhou, Zongwei and Wang, Xuezhi and Saeta, Brennan and Diaz, Mark and Firat, Orhan and Catasta, Michele and Wei, Jason and Meier-Hellstern, Kathy and Eck, Douglas and Dean, Jeff and Petrov, Slav and Fiedel, Noah},
  title      = {{PaLM}: {Scaling} {Language} {Modeling} with {Pathways}},
  year       = {2022},
  month      = oct,
  note       = {arXiv:2204.02311 [cs] type: article},
  abstract   = {Large language models have been shown to achieve remarkable performance across a variety of natural language tasks using few-shot learning, which drastically reduces the number of task-specific training examples needed to adapt the model to a particular application. To further our understanding of the impact of scale on few-shot learning, we trained a 540-billion parameter, densely activated, Transformer language model, which we call Pathways Language Model PaLM. We trained PaLM on 6144 TPU v4 chips using Pathways, a new ML system which enables highly efficient training across multiple TPU Pods. We demonstrate continued benefits of scaling by achieving state-of-the-art few-shot learning results on hundreds of language understanding and generation benchmarks. On a number of these tasks, PaLM 540B achieves breakthrough performance, outperforming the finetuned state-of-the-art on a suite of multi-step reasoning tasks, and outperforming average human performance on the recently released BIG-bench benchmark. A significant number of BIG-bench tasks showed discontinuous improvements from model scale, meaning that performance steeply increased as we scaled to our largest model. PaLM also has strong capabilities in multilingual tasks and source code generation, which we demonstrate on a wide array of benchmarks. We additionally provide a comprehensive analysis on bias and toxicity, and study the extent of training data memorization with respect to model scale. Finally, we discuss the ethical considerations related to large language models and discuss potential mitigation strategies.},
  file       = {:Chowdhery2022 - PaLM_ Scaling Language Modeling with Pathways.pdf:PDF},
  keywords   = {Computer Science - Computation and Language},
  school     = {arXiv},
  shorttitle = {{PaLM}},
  url        = {http://arxiv.org/abs/2204.02311},
  urldate    = {2023-06-13}
}

'
@techreport{OpenAI2023,
  author   = {OpenAI},
  title    = {{GPT}-4 {Technical} {Report}},
  year     = {2023},
  month    = mar,
  note     = {arXiv:2303.08774 [cs] type: article},
  abstract = {We report the development of GPT-4, a large-scale, multimodal model which can accept image and text inputs and produce text outputs. While less capable than humans in many real-world scenarios, GPT-4 exhibits human-level performance on various professional and academic benchmarks, including passing a simulated bar exam with a score around the top 10\% of test takers. GPT-4 is a Transformer-based model pre-trained to predict the next token in a document. The post-training alignment process results in improved performance on measures of factuality and adherence to desired behavior. A core component of this project was developing infrastructure and optimization methods that behave predictably across a wide range of scales. This allowed us to accurately predict some aspects of GPT-4's performance based on models trained with no more than 1/1,000th the compute of GPT-4.},
  annote   = {Comment: 100 pages},
  doi      = {10.48550/arXiv.2303.08774},
  file     = {:OpenAI2023 - GPT 4 Technical Report.pdf:PDF},
  keywords = {Computer Science - Computation and Language, Computer Science - Artificial Intelligence},
  school   = {arXiv},
  url      = {http://arxiv.org/abs/2303.08774},
  urldate  = {2023-06-13}
}

'
@techreport{Nityasya2021,
  author   = {Nityasya, Made Nindyatama and Wibowo, Haryo Akbarianto and Prasojo, Radityo Eko and Aji, Alham Fikri},
  title    = {Costs to {Consider} in {Adopting} {NLP} for {Your} {Business}},
  year     = {2021},
  month    = apr,
  note     = {arXiv:2012.08958 [cs] type: article},
  abstract = {Recent advances in Natural Language Processing (NLP) have largely pushed deep transformer-based models as the go-to state-of-the-art technique without much regard to the production and utilization cost. Companies planning to adopt these methods into their business face difficulties because of the lack of machine, data, and human resources to build them. We compare both the performance and the cost of classical learning algorithms to the latest ones in common sequence and text labeling tasks. In our industrial datasets, we find that classical models often perform on par with deep neural ones despite the lower cost. We show the trade-off between performance gain and the cost across the models to give more insights for AI-pivoting business. Further, we call for more research into low-cost models, especially for under-resourced languages.},
  annote   = {Comment: 12 pages, 2 figures},
  file     = {:Nityasya2021 - Costs to Consider in Adopting NLP for Your Business.pdf:PDF},
  keywords = {Computer Science - Computation and Language, 68T50, I.2.7, I.2.6},
  school   = {arXiv},
  url      = {http://arxiv.org/abs/2012.08958},
  urldate  = {2023-06-13}
}

'
@techreport{Sanh2022,
  author   = {Sanh, Victor and Webson, Albert and Raffel, Colin and Bach, Stephen H. and Sutawika, Lintang and Alyafeai, Zaid and Chaffin, Antoine and Stiegler, Arnaud and Scao, Teven Le and Raja, Arun and Dey, Manan and Bari, M. Saiful and Xu, Canwen and Thakker, Urmish and Sharma, Shanya Sharma and Szczechla, Eliza and Kim, Taewoon and Chhablani, Gunjan and Nayak, Nihal and Datta, Debajyoti and Chang, Jonathan and Jiang, Mike Tian-Jian and Wang, Han and Manica, Matteo and Shen, Sheng and Yong, Zheng Xin and Pandey, Harshit and Bawden, Rachel and Wang, Thomas and Neeraj, Trishala and Rozen, Jos and Sharma, Abheesht and Santilli, Andrea and Fevry, Thibault and Fries, Jason Alan and Teehan, Ryan and Bers, Tali and Biderman, Stella and Gao, Leo and Wolf, Thomas and Rush, Alexander M.},
  title    = {Multitask {Prompted} {Training} {Enables} {Zero}-{Shot} {Task} {Generalization}},
  year     = {2022},
  month    = mar,
  note     = {arXiv:2110.08207 [cs] type: article},
  abstract = {Large language models have recently been shown to attain reasonable zero-shot generalization on a diverse set of tasks (Brown et al., 2020). It has been hypothesized that this is a consequence of implicit multitask learning in language models' pretraining (Radford et al., 2019). Can zero-shot generalization instead be directly induced by explicit multitask learning? To test this question at scale, we develop a system for easily mapping any natural language tasks into a human-readable prompted form. We convert a large set of supervised datasets, each with multiple prompts with diverse wording. These prompted datasets allow for benchmarking the ability of a model to perform completely held-out tasks. We fine-tune a pretrained encoder-decoder model (Raffel et al., 2020; Lester et al., 2021) on this multitask mixture covering a wide variety of tasks. The model attains strong zero-shot performance on several standard datasets, often outperforming models up to 16x its size. Further, our approach attains strong performance on a subset of tasks from the BIG-bench benchmark, outperforming models up to 6x its size. All trained models are available at https://github.com/bigscience-workshop/t-zero and all prompts are available at https://github.com/bigscience-workshop/promptsource.},
  annote   = {Comment: ICLR 2022 Spotlight (with extended discussion)},
  file     = {:Sanh2022 - Multitask Prompted Training Enables Zero Shot Task Generalization.pdf:PDF},
  keywords = {Computer Science - Machine Learning, Computer Science - Computation and Language},
  school   = {arXiv},
  url      = {http://arxiv.org/abs/2110.08207},
  urldate  = {2023-06-13}
}

'
@techreport{Chung2022,
  author   = {Chung, Hyung Won and Hou, Le and Longpre, Shayne and Zoph, Barret and Tay, Yi and Fedus, William and Li, Yunxuan and Wang, Xuezhi and Dehghani, Mostafa and Brahma, Siddhartha and Webson, Albert and Gu, Shixiang Shane and Dai, Zhuyun and Suzgun, Mirac and Chen, Xinyun and Chowdhery, Aakanksha and Castro-Ros, Alex and Pellat, Marie and Robinson, Kevin and Valter, Dasha and Narang, Sharan and Mishra, Gaurav and Yu, Adams and Zhao, Vincent and Huang, Yanping and Dai, Andrew and Yu, Hongkun and Petrov, Slav and Chi, Ed H. and Dean, Jeff and Devlin, Jacob and Roberts, Adam and Zhou, Denny and Le, Quoc V. and Wei, Jason},
  title    = {Scaling {Instruction}-{Finetuned} {Language} {Models}},
  year     = {2022},
  month    = dec,
  note     = {arXiv:2210.11416 [cs] type: article},
  abstract = {Finetuning language models on a collection of datasets phrased as instructions has been shown to improve model performance and generalization to unseen tasks. In this paper we explore instruction finetuning with a particular focus on (1) scaling the number of tasks, (2) scaling the model size, and (3) finetuning on chain-of-thought data. We find that instruction finetuning with the above aspects dramatically improves performance on a variety of model classes (PaLM, T5, U-PaLM), prompting setups (zero-shot, few-shot, CoT), and evaluation benchmarks (MMLU, BBH, TyDiQA, MGSM, open-ended generation). For instance, Flan-PaLM 540B instruction-finetuned on 1.8K tasks outperforms PALM 540B by a large margin (+9.4\% on average). Flan-PaLM 540B achieves state-of-the-art performance on several benchmarks, such as 75.2\% on five-shot MMLU. We also publicly release Flan-T5 checkpoints, which achieve strong few-shot performance even compared to much larger models, such as PaLM 62B. Overall, instruction finetuning is a general method for improving the performance and usability of pretrained language models.},
  annote   = {Comment: Public checkpoints: https://huggingface.co/docs/transformers/model\_doc/flan-t5},
  file     = {:Chung2022 - Scaling Instruction Finetuned Language Models.pdf:PDF},
  keywords = {Computer Science - Machine Learning, Computer Science - Computation and Language},
  school   = {arXiv},
  url      = {http://arxiv.org/abs/2210.11416},
  urldate  = {2023-06-13}
}

'
@techreport{Liu2023,
  author     = {Liu, Tiedong and Low, Bryan Kian Hsiang},
  title      = {Goat: {Fine}-tuned {LLaMA} {Outperforms} {GPT}-4 on {Arithmetic} {Tasks}},
  year       = {2023},
  month      = may,
  note       = {arXiv:2305.14201 [cs] type: article},
  abstract   = {We introduce Goat, a fine-tuned LLaMA model that significantly outperforms GPT-4 on a range of arithmetic tasks. Fine-tuned on a synthetically generated dataset, Goat achieves state-of-the-art performance on BIG-bench arithmetic sub-task. In particular, the zero-shot Goat-7B matches or even surpasses the accuracy achieved by the few-shot PaLM-540B. Surprisingly, Goat can achieve near-perfect accuracy on large-number addition and subtraction through supervised fine-tuning only, which is almost impossible with previous pretrained language models, such as Bloom, OPT, GPT-NeoX, etc. We attribute Goat's exceptional performance to LLaMA's consistent tokenization of numbers. To tackle more challenging tasks like large-number multiplication and division, we propose an approach that classifies tasks based on their learnability, and subsequently decomposes unlearnable tasks, such as multi-digit multiplication and division, into a series of learnable tasks by leveraging basic arithmetic principles. We thoroughly examine the performance of our model, offering a comprehensive evaluation of the effectiveness of our proposed decomposition steps. Additionally, Goat-7B can be easily trained using LoRA on a 24GB VRAM GPU, facilitating reproducibility for other researchers. We release our model, dataset, and the Python script for dataset generation.},
  comment    = {Pour un meilleur tokenizer},
  file       = {:Liu2023 - Goat_ Fine Tuned LLaMA Outperforms GPT 4 on Arithmetic Tasks.pdf:PDF},
  keywords   = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
  school     = {arXiv},
  shorttitle = {Goat},
  url        = {http://arxiv.org/abs/2305.14201},
  urldate    = {2023-06-20}
}

'
@techreport{Loshchilov2019,
  author   = {Loshchilov, Ilya and Hutter, Frank},
  title    = {Decoupled {Weight} {Decay} {Regularization}},
  year     = {2019},
  month    = jan,
  note     = {arXiv:1711.05101 [cs, math] version: 3 type: article},
  abstract = {L\$\_2\$ regularization and weight decay regularization are equivalent for standard stochastic gradient descent (when rescaled by the learning rate), but as we demonstrate this is {\textbackslash}emph\{not\} the case for adaptive gradient algorithms, such as Adam. While common implementations of these algorithms employ L\$\_2\$ regularization (often calling it "weight decay" in what may be misleading due to the inequivalence we expose), we propose a simple modification to recover the original formulation of weight decay regularization by {\textbackslash}emph\{decoupling\} the weight decay from the optimization steps taken w.r.t. the loss function. We provide empirical evidence that our proposed modification (i) decouples the optimal choice of weight decay factor from the setting of the learning rate for both standard SGD and Adam and (ii) substantially improves Adam's generalization performance, allowing it to compete with SGD with momentum on image classification datasets (on which it was previously typically outperformed by the latter). Our proposed decoupled weight decay has already been adopted by many researchers, and the community has implemented it in TensorFlow and PyTorch; the complete source code for our experiments is available at https://github.com/loshchil/AdamW-and-SGDW},
  annote   = {Comment: Published as a conference paper at ICLR 2019},
  file     = {:Loshchilov2019 - Decoupled Weight Decay Regularization.pdf:PDF},
  keywords = {Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, Mathematics - Optimization and Control},
  school   = {arXiv},
  url      = {http://arxiv.org/abs/1711.05101},
  urldate  = {2023-06-20}
}

'
@techreport{Holtzman2022,
  author     = {Holtzman, Ari and West, Peter and Shwartz, Vered and Choi, Yejin and Zettlemoyer, Luke},
  title      = {Surface {Form} {Competition}: {Why} the {Highest} {Probability} {Answer} {Isn}'t {Always} {Right}},
  year       = {2022},
  month      = nov,
  note       = {arXiv:2104.08315 [cs] type: article},
  abstract   = {Large language models have shown promising results in zero-shot settings (Brown et al.,2020; Radford et al., 2019). For example, they can perform multiple choice tasks simply by conditioning on a question and selecting the answer with the highest probability. However, ranking by string probability can be problematic due to surface form competition-wherein different surface forms compete for probability mass, even if they represent the same underlying concept, e.g. "computer" and "PC." Since probability mass is finite, this lowers the probability of the correct answer, due to competition from other strings that are valid answers (but not one of the multiple choice options). We introduce Domain Conditional Pointwise Mutual Information, an alternative scoring function that directly compensates for surface form competition by simply reweighing each option according to a term that is proportional to its a priori likelihood within the context of the specific zero-shot task. It achieves consistent gains in zero-shot performance over both calibrated (Zhao et al., 2021) and uncalibrated scoring functions on all GPT-2 and GPT-3 models over a variety of multiple choice datasets.},
  doi        = {10.48550/arXiv.2104.08315},
  file       = {:Holtzman2022 - Surface Form Competition_ Why the Highest Probability Answer Isn't Always Right.pdf:PDF},
  keywords   = {Computer Science - Computation and Language},
  school     = {arXiv},
  shorttitle = {Surface {Form} {Competition}},
  url        = {http://arxiv.org/abs/2104.08315},
  urldate    = {2023-10-12}
}

'
@techreport{Zhang2023,
  author     = {Zhang, Shengyu and Dong, Linfeng and Li, Xiaoya and Zhang, Sen and Sun, Xiaofei and Wang, Shuhe and Li, Jiwei and Hu, Runyi and Zhang, Tianwei and Wu, Fei and Wang, Guoyin},
  title      = {Instruction {Tuning} for {Large} {Language} {Models}: {A} {Survey}},
  year       = {2023},
  month      = oct,
  note       = {arXiv:2308.10792 [cs] type: article},
  abstract   = {This paper surveys research works in the quickly advancing field of instruction tuning (IT), a crucial technique to enhance the capabilities and controllability of large language models (LLMs). Instruction tuning refers to the process of further training LLMs on a dataset consisting of {\textbackslash}textsc\{(instruction, output)\} pairs in a supervised fashion, which bridges the gap between the next-word prediction objective of LLMs and the users' objective of having LLMs adhere to human instructions. In this work, we make a systematic review of the literature, including the general methodology of IT, the construction of IT datasets, the training of IT models, and applications to different modalities, domains and applications, along with an analysis on aspects that influence the outcome of IT (e.g., generation of instruction outputs, size of the instruction dataset, etc). We also review the potential pitfalls of IT along with criticism against it, along with efforts pointing out current deficiencies of existing strategies and suggest some avenues for fruitful research. Project page: github.com/xiaoya-li/Instruction-Tuning-Survey},
  annote     = {Comment: A Survey paper, Pre-print},
  file       = {:Zhang2023 - Instruction Tuning for Large Language Models_ a Survey.pdf:PDF},
  keywords   = {Computer Science - Computation and Language, Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
  school     = {arXiv},
  shorttitle = {Instruction {Tuning} for {Large} {Language} {Models}},
  url        = {http://arxiv.org/abs/2308.10792},
  urldate    = {2023-10-12}
}

@inproceedings{Zhao2023,
  author    = {Zhao, Xuandong and Ouyang, Siqi and Yu, Zhiguo and Wu, Ming and Li, Lei},
  booktitle = {Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
  title     = {Pre-trained Language Models Can be Fully Zero-Shot Learners},
  year      = {2023},
  address   = {Toronto, Canada},
  month     = jul,
  pages     = {15590--15606},
  publisher = {Association for Computational Linguistics},
  abstract  = {How can we extend a pre-trained model to many language understanding tasks, without labeled or additional unlabeled data? Pre-trained language models (PLMs) have been effective for a wide range of NLP tasks. However, existing approaches either require fine-tuning on downstream labeled datasets or manually constructing proper prompts. In this paper, we propose nonparametric prompting PLM (NPPrompt) for fully zero-shot language understanding. Unlike previous methods, NPPrompt uses only pre-trained language models and does not require any labeled data or additional raw corpus for further fine-tuning, nor does it rely on humans to construct a comprehensive set of prompt label words. We evaluate NPPrompt against previous major few-shot and zero-shot learning methods on diverse NLP tasks: including text classification, text entailment, similar text retrieval, paraphrasing, and multiple-choice question answering. Experimental results demonstrate that our NPPrompt outperforms the previous best fully zero-shot method by big margins, with absolute gains of 12.8{\%} in accuracy on text classification and 15.6{\%} on the GLUE benchmark. Our source code is available at \url{https://anonymous.4open.science/r/NPPrompt}.},
  doi       = {10.18653/v1/2023.acl-long.869},
  url       = {https://aclanthology.org/2023.acl-long.869}
}

'
@techreport{Xia2018,
  author   = {Xia, Congying and Zhang, Chenwei and Yan, Xiaohui and Chang, Yi and Yu, Philip S.},
  title    = {Zero-shot {User} {Intent} {Detection} via {Capsule} {Neural} {Networks}},
  year     = {2018},
  month    = sep,
  note     = {arXiv:1809.00385 [cs] type: article},
  abstract = {User intent detection plays a critical role in question-answering and dialog systems. Most previous works treat intent detection as a classification problem where utterances are labeled with predefined intents. However, it is labor-intensive and time-consuming to label users' utterances as intents are diversely expressed and novel intents will continually be involved. Instead, we study the zero-shot intent detection problem, which aims to detect emerging user intents where no labeled utterances are currently available. We propose two capsule-based architectures: INTENT-CAPSNET that extracts semantic features from utterances and aggregates them to discriminate existing intents, and INTENTCAPSNET-ZSL which gives INTENTCAPSNET the zero-shot learning ability to discriminate emerging intents via knowledge transfer from existing intents. Experiments on two real-world datasets show that our model not only can better discriminate diversely expressed existing intents, but is also able to discriminate emerging intents when no labeled utterances are available.},
  annote   = {Comment: In EMNLP 2018 as a long paper. Previously available on http://doi.org/10.13140/RG.2.2.11739.46889},
  doi      = {10.48550/arXiv.1809.00385},
  file     = {:Xia2018 - Zero Shot User Intent Detection Via Capsule Neural Networks.pdf:PDF},
  keywords = {Computer Science - Computation and Language, Computer Science - Artificial Intelligence},
  school   = {arXiv},
  url      = {http://arxiv.org/abs/1809.00385},
  urldate  = {2023-10-17}
}

@inproceedings{Lu2023,
  author    = {Lu, Jinghui and Zhu, Dongsheng and Han, Weidong and Zhao, Rui and Mac Namee, Brian and Tan, Fei},
  booktitle = {Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
  title     = {What Makes Pre-trained Language Models Better Zero-shot Learners?},
  year      = {2023},
  address   = {Toronto, Canada},
  month     = jul,
  pages     = {2288--2303},
  publisher = {Association for Computational Linguistics},
  abstract  = {Current methods for prompt learning in zero-shot scenarios widely rely on a development set with sufficient human-annotated data to select the best-performing prompt template a posteriori. This is not ideal because in a real-world zero-shot scenario of practical relevance, no labelled data is available. Thus, we propose a simple yet effective method for screening reasonable prompt templates in zero-shot text classification: Perplexity Selection (Perplection). We hypothesize that language discrepancy can be used to measure the efficacy of prompt templates, and thereby develop a substantiated perplexity-based scheme allowing for forecasting the performance of prompt templates in advance. Experiments show that our method leads to improved prediction performance in a realistic zero-shot setting, eliminating the need for any labelled examples.},
  doi       = {10.18653/v1/2023.acl-long.128},
  url       = {https://aclanthology.org/2023.acl-long.128}
}

'
@inproceedings{Meng2020,
  author     = {Meng, Yu and Zhang, Yunyi and Huang, Jiaxin and Xiong, Chenyan and Ji, Heng and Zhang, Chao and Han, Jiawei},
  booktitle  = {Proceedings of the 2020 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing} ({EMNLP})},
  title      = {Text {Classification} {Using} {Label} {Names} {Only}: {A} {Language} {Model} {Self}-{Training} {Approach}},
  year       = {2020},
  address    = {Online},
  month      = nov,
  pages      = {9006--9017},
  publisher  = {Association for Computational Linguistics},
  abstract   = {Current text classification methods typically require a good number of human-labeled documents as training data, which can be costly and difficult to obtain in real applications. Humans can perform classification without seeing any labeled examples but only based on a small set of words describing the categories to be classified. In this paper, we explore the potential of only using the label name of each class to train classification models on unlabeled data, without using any labeled documents. We use pre-trained neural language models both as general linguistic knowledge sources for category understanding and as representation learning models for document classification. Our method (1) associates semantically related words with the label names, (2) finds category-indicative words and trains the model to predict their implied categories, and (3) generalizes the model via self-training. We show that our model achieves around 90\% accuracy on four benchmark datasets including topic and sentiment classification without using any labeled documents but learning from unlabeled data supervised by at most 3 words (1 in most cases) per class as the label name.},
  doi        = {10.18653/v1/2020.emnlp-main.724},
  file       = {:Meng2020 - Text Classification Using Label Names Only_ a Language Model Self Training Approach.pdf:PDF},
  shorttitle = {Text {Classification} {Using} {Label} {Names} {Only}},
  url        = {https://aclanthology.org/2020.emnlp-main.724},
  urldate    = {2023-10-17}
}

'
@techreport{Wang2023a,
  author     = {Wang, Yidong and Yu, Zhuohao and Zeng, Zhengran and Yang, Linyi and Wang, Cunxiang and Chen, Hao and Jiang, Chaoya and Xie, Rui and Wang, Jindong and Xie, Xing and Ye, Wei and Zhang, Shikun and Zhang, Yue},
  title      = {{PandaLM}: {An} {Automatic} {Evaluation} {Benchmark} for {LLM} {Instruction} {Tuning} {Optimization}},
  year       = {2023},
  month      = jun,
  note       = {arXiv:2306.05087 [cs] type: article},
  abstract   = {Instruction tuning large language models (LLMs) remains a challenging task, owing to the complexity of hyperparameter selection and the difficulty involved in evaluating the tuned models. To determine the optimal hyperparameters, an automatic, robust, and reliable evaluation benchmark is essential. However, establishing such a benchmark is not a trivial task due to the challenges associated with evaluation accuracy and privacy protection. In response to these challenges, we introduce a judge large language model, named PandaLM, which is trained to distinguish the superior model given several LLMs. PandaLM's focus extends beyond just the objective correctness of responses, which is the main focus of traditional evaluation datasets. It addresses vital subjective factors such as relative conciseness, clarity, adherence to instructions, comprehensiveness, and formality. To ensure the reliability of PandaLM, we collect a diverse human-annotated test dataset, where all contexts are generated by humans and labels are aligned with human preferences. Our results indicate that PandaLM-7B achieves 93.75\% of GPT-3.5's evaluation ability and 88.28\% of GPT-4's in terms of F1-score on our test dataset. PandaLM enables the evaluation of LLM to be fairer but with less cost, evidenced by significant improvements achieved by models tuned through PandaLM compared to their counterparts trained with default Alpaca's hyperparameters. In addition, PandaLM does not depend on API-based evaluations, thus avoiding potential data leakage. All resources of PandaLM are released at https://github.com/WeOpenML/PandaLM.},
  file       = {:Wang2023 - PandaLM_ an Automatic Evaluation Benchmark for LLM Instruction Tuning Optimization.pdf:PDF},
  keywords   = {Computer Science - Computation and Language, Computer Science - Artificial Intelligence},
  school     = {arXiv},
  shorttitle = {{PandaLM}},
  url        = {http://arxiv.org/abs/2306.05087},
  urldate    = {2023-10-17}
}

'
@inproceedings{Lewis2020,
  author     = {Lewis, Mike and Liu, Yinhan and Goyal, Naman and Ghazvininejad, Marjan and Mohamed, Abdelrahman and Levy, Omer and Stoyanov, Veselin and Zettlemoyer, Luke},
  booktitle  = {Proceedings of the 58th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics}},
  title      = {{BART}: {Denoising} {Sequence}-to-{Sequence} {Pre}-training for {Natural} {Language} {Generation}, {Translation}, and {Comprehension}},
  year       = {2020},
  address    = {Online},
  month      = jul,
  pages      = {7871--7880},
  publisher  = {Association for Computational Linguistics},
  abstract   = {We present BART, a denoising autoencoder for pretraining sequence-to-sequence models. BART is trained by (1) corrupting text with an arbitrary noising function, and (2) learning a model to reconstruct the original text. It uses a standard Tranformer-based neural machine translation architecture which, despite its simplicity, can be seen as generalizing BERT (due to the bidirectional encoder), GPT (with the left-to-right decoder), and other recent pretraining schemes. We evaluate a number of noising approaches, finding the best performance by both randomly shuffling the order of sentences and using a novel in-filling scheme, where spans of text are replaced with a single mask token. BART is particularly effective when fine tuned for text generation but also works well for comprehension tasks. It matches the performance of RoBERTa on GLUE and SQuAD, and achieves new state-of-the-art results on a range of abstractive dialogue, question answering, and summarization tasks, with gains of up to 3.5 ROUGE. BART also provides a 1.1 BLEU increase over a back-translation system for machine translation, with only target language pretraining. We also replicate other pretraining schemes within the BART framework, to understand their effect on end-task performance.},
  doi        = {10.18653/v1/2020.acl-main.703},
  file       = {:Lewis2020 - BART_ Denoising Sequence to Sequence Pre Training for Natural Language Generation, Translation, and Comprehension.pdf:PDF},
  shorttitle = {{BART}},
  url        = {https://aclanthology.org/2020.acl-main.703},
  urldate    = {2023-10-17}
}

@misc{BigScienceWorkshop2022,
  author    = {{BigScience Workshop}},
  title     = {{BLOOM} (Revision 4ab0472)},
  year      = {2022},
  doi       = {10.57967/hf/0003},
  publisher = {Hugging Face},
  url       = {https://huggingface.co/bigscience/bloom}
}

'
@techreport{Muennighoff2023,
  author   = {Muennighoff, Niklas and Wang, Thomas and Sutawika, Lintang and Roberts, Adam and Biderman, Stella and Scao, Teven Le and Bari, M. Saiful and Shen, Sheng and Yong, Zheng-Xin and Schoelkopf, Hailey and Tang, Xiangru and Radev, Dragomir and Aji, Alham Fikri and Almubarak, Khalid and Albanie, Samuel and Alyafeai, Zaid and Webson, Albert and Raff, Edward and Raffel, Colin},
  title    = {Crosslingual {Generalization} through {Multitask} {Finetuning}},
  year     = {2023},
  month    = may,
  note     = {arXiv:2211.01786 [cs] type: article},
  abstract = {Multitask prompted finetuning (MTF) has been shown to help large language models generalize to new tasks in a zero-shot setting, but so far explorations of MTF have focused on English data and models. We apply MTF to the pretrained multilingual BLOOM and mT5 model families to produce finetuned variants called BLOOMZ and mT0. We find finetuning large multilingual language models on English tasks with English prompts allows for task generalization to non-English languages that appear only in the pretraining corpus. Finetuning on multilingual tasks with English prompts further improves performance on English and non-English tasks leading to various state-of-the-art zero-shot results. We also investigate finetuning on multilingual tasks with prompts that have been machine-translated from English to match the language of each dataset. We find training on these machine-translated prompts leads to better performance on human-written prompts in the respective languages. Surprisingly, we find models are capable of zero-shot generalization to tasks in languages they have never intentionally seen. We conjecture that the models are learning higher-level capabilities that are both task- and language-agnostic. In addition, we introduce xP3, a composite of supervised datasets in 46 languages with English and machine-translated prompts. Our code, datasets and models are freely available at https://github.com/bigscience-workshop/xmtf.},
  annote   = {Comment: 9 main pages (119 with appendix), 16 figures and 11 tables},
  doi      = {10.48550/arXiv.2211.01786},
  file     = {:Muennighoff2023 - Crosslingual Generalization through Multitask Finetuning.pdf:PDF},
  keywords = {Computer Science - Computation and Language, Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
  school   = {arXiv},
  url      = {http://arxiv.org/abs/2211.01786},
  urldate  = {2023-10-16}
}

'
@inproceedings{Wei2022,
  author   = {Wei, Jason and Bosma, Maarten and Zhao, Vincent and Guu, Kelvin and Yu, Adams Wei and Lester, Brian and Du, Nan and Dai, Andrew M. and Le, Quoc V.},
  title    = {Finetuned {Language} {Models} are {Zero}-{Shot} {Learners}},
  year     = {2022},
  month    = jan,
  abstract = {This paper explores a simple method for improving the zero-shot learning abilities of language models. We show that instruction tuning—finetuning language models on a collection of datasets described via instructions—substantially improves zero-shot performance on unseen tasks. We take a 137B parameter pretrained language model and instruction tune it on over 60 NLP datasets verbalized via natural language instruction templates. We evaluate this instruction-tuned model, which we call FLAN, on unseen task types. FLAN substantially improves the performance of its unmodified counterpart and surpasses zero-shot 175B GPT-3 on 20 of 25 datasets that we evaluate. FLAN even outperforms few-shot GPT-3 by a large margin on ANLI, RTE, BoolQ, AI2-ARC, OpenbookQA, and StoryCloze. Ablation studies reveal that number of finetuning datasets, model scale, and natural language instructions are key to the success of instruction tuning.},
  file     = {:Wei2022 - Finetuned Language Models Are Zero Shot Learners.pdf:PDF},
  language = {en},
  url      = {https://openreview.net/forum?id=gEZrGCozdqR},
  urldate  = {2023-06-13}
}

'
@techreport{Biderman2023,
  author     = {Biderman, Stella and Schoelkopf, Hailey and Anthony, Quentin and Bradley, Herbie and O'Brien, Kyle and Hallahan, Eric and Khan, Mohammad Aflah and Purohit, Shivanshu and Prashanth, USVSN Sai and Raff, Edward and Skowron, Aviya and Sutawika, Lintang and van der Wal, Oskar},
  title      = {Pythia: {A} {Suite} for {Analyzing} {Large} {Language} {Models} {Across} {Training} and {Scaling}},
  year       = {2023},
  month      = may,
  note       = {arXiv:2304.01373 [cs] type: article},
  abstract   = {How do large language models (LLMs) develop and evolve over the course of training? How do these patterns change as models scale? To answer these questions, we introduce {\textbackslash}textit\{Pythia\}, a suite of 16 LLMs all trained on public data seen in the exact same order and ranging in size from 70M to 12B parameters. We provide public access to 154 checkpoints for each one of the 16 models, alongside tools to download and reconstruct their exact training dataloaders for further study. We intend {\textbackslash}textit\{Pythia\} to facilitate research in many areas, and we present several case studies including novel results in memorization, term frequency effects on few-shot performance, and reducing gender bias. We demonstrate that this highly controlled setup can be used to yield novel insights toward LLMs and their training dynamics. Trained models, analysis code, training code, and training data can be found at {\textbackslash}url\{https://github.com/EleutherAI/pythia\}.},
  annote     = {Comment: Code at https://github.com/EleutherAI/pythia},
  doi        = {10.48550/arXiv.2304.01373},
  file       = {:Biderman2023 - Pythia_ a Suite for Analyzing Large Language Models across Training and Scaling.pdf:PDF},
  keywords   = {Computer Science - Computation and Language},
  school     = {arXiv},
  shorttitle = {Pythia},
  url        = {http://arxiv.org/abs/2304.01373},
  urldate    = {2023-10-16}
}

'
@techreport{Tay2023,
  author     = {Tay, Yi and Dehghani, Mostafa and Tran, Vinh Q. and Garcia, Xavier and Wei, Jason and Wang, Xuezhi and Chung, Hyung Won and Shakeri, Siamak and Bahri, Dara and Schuster, Tal and Zheng, Huaixiu Steven and Zhou, Denny and Houlsby, Neil and Metzler, Donald},
  title      = {{UL2}: {Unifying} {Language} {Learning} {Paradigms}},
  year       = {2023},
  month      = feb,
  note       = {arXiv:2205.05131 [cs] type: article},
  abstract   = {Existing pre-trained models are generally geared towards a particular class of problems. To date, there seems to be still no consensus on what the right architecture and pre-training setup should be. This paper presents a unified framework for pre-training models that are universally effective across datasets and setups. We begin by disentangling architectural archetypes with pre-training objectives -- two concepts that are commonly conflated. Next, we present a generalized \& unified perspective for self-supervision in NLP and show how different pre-training objectives can be cast as one another and how interpolating between different objectives can be effective. We then propose Mixture-of-Denoisers (MoD), a pre-training objective that combines diverse pre-training paradigms together. We furthermore introduce a notion of mode switching, wherein downstream fine-tuning is associated with specific pre-training schemes. We conduct extensive ablative experiments to compare multiple pre-training objectives and find that our method pushes the Pareto-frontier by outperforming T5 \& GPT-like models across multiple diverse setups. By scaling our model up to 20B parameters, we achieve SOTA performance on 50 well-established supervised finetuning based NLP tasks. Our model also achieve strong results at in-context learning, outperforming 175B GPT-3 on zero-shot SuperGLUE and tripling the performance of T5-XXL on one-shot summarization. On 0-shot MMLU, UL2 20B outperforms T0 and T5 models. UL2 20B also works well with chain-of-thought prompting and reasoning, making it an appealing choice for research into reasoning at a small to medium scale of 20B parameters. Finally, we apply FLAN instruction tuning to the UL2 20B model, achieving MMLU and Big-Bench scores competitive to FLAN-PaLM 62B. We release Flax-based T5X checkpoints for the UL2 20B \& Flan-UL2 20B.},
  annote     = {Comment: Updated Q1 2023 with Flan-UL2 20B release! :)},
  file       = {:Tay2023 - UL2_ Unifying Language Learning Paradigms.pdf:PDF},
  keywords   = {Computer Science - Computation and Language},
  school     = {arXiv},
  shorttitle = {{UL2}},
  url        = {http://arxiv.org/abs/2205.05131},
  urldate    = {2023-10-17}
}

'
@techreport{Wang2022,
  author    = {Wang, Thomas and Roberts, Adam and Hesslow, Daniel and Scao, Teven Le and Chung, Hyung Won and Beltagy, Iz and Launay, Julien and Raffel, Colin},
  title     = {What {Language} {Model} {Architecture} and {Pretraining} {Objective} {Work} {Best} for {Zero}-{Shot} {Generalization}?},
  year      = {2022},
  month     = apr,
  note      = {arXiv:2204.05832 [cs, stat] type: article},
  abstract  = {Large pretrained Transformer language models have been shown to exhibit zero-shot generalization, i.e. they can perform a wide variety of tasks that they were not explicitly trained on. However, the architectures and pretraining objectives used across state-of-the-art models differ significantly, and there has been limited systematic comparison of these factors. In this work, we present a large-scale evaluation of modeling choices and their impact on zero-shot generalization. In particular, we focus on text-to-text models and experiment with three model architectures (causal/non-causal decoder-only and encoder-decoder), trained with two different pretraining objectives (autoregressive and masked language modeling), and evaluated with and without multitask prompted finetuning. We train models with over 5 billion parameters for more than 170 billion tokens, thereby increasing the likelihood that our conclusions will transfer to even larger scales. Our experiments show that causal decoder-only models trained on an autoregressive language modeling objective exhibit the strongest zero-shot generalization after purely unsupervised pretraining. However, models with non-causal visibility on their input trained with a masked language modeling objective followed by multitask finetuning perform the best among our experiments. We therefore consider the adaptation of pretrained models across architectures and objectives. We find that pretrained non-causal decoder models can be adapted into performant generative causal decoder models, using autoregressive language modeling as a downstream task. Furthermore, we find that pretrained causal decoder models can be efficiently adapted into non-causal decoder models, ultimately achieving competitive performance after multitask finetuning. Code and checkpoints are available at https://github.com/bigscience-workshop/architecture-objective.},
  file      = {:Wang2022 - What Language Model Architecture and Pretraining Objective Work Best for Zero Shot Generalization_.pdf:PDF},
  keywords  = {Computer Science - Computation and Language, Computer Science - Machine Learning, Statistics - Machine Learning},
  relevance = {relevant},
  school    = {arXiv},
  url       = {http://arxiv.org/abs/2204.05832},
  urldate   = {2023-10-17}
}

@misc{Liu2019a,
  author        = {Yinhan Liu and Myle Ott and Naman Goyal and Jingfei Du and Mandar Joshi and Danqi Chen and Omer Levy and Mike Lewis and Luke Zettlemoyer and Veselin Stoyanov},
  title         = {RoBERTa: A Robustly Optimized BERT Pretraining Approach},
  year          = {2019},
  archiveprefix = {arXiv},
  eprint        = {1907.11692},
  primaryclass  = {cs.CL}
}

@misc{Sun2023,
  author        = {Yutao Sun and Li Dong and Shaohan Huang and Shuming Ma and Yuqing Xia and Jilong Xue and Jianyong Wang and Furu Wei},
  title         = {Retentive Network: A Successor to Transformer for Large Language Models},
  year          = {2023},
  archiveprefix = {arXiv},
  eprint        = {2307.08621},
  primaryclass  = {cs.CL}
}

@misc{Peng2023,
  author        = {Bo Peng and Eric Alcaide and Quentin Anthony and Alon Albalak and Samuel Arcadinho and Huanqi Cao and Xin Cheng and Michael Chung and Matteo Grella and Kranthi Kiran GV and Xuzheng He and Haowen Hou and Przemyslaw Kazienko and Jan Kocon and Jiaming Kong and Bartlomiej Koptyra and Hayden Lau and Krishna Sri Ipsit Mantri and Ferdinand Mom and Atsushi Saito and Xiangru Tang and Bolun Wang and Johan S. Wind and Stansilaw Wozniak and Ruichong Zhang and Zhenyuan Zhang and Qihang Zhao and Peng Zhou and Jian Zhu and Rui-Jie Zhu},
  title         = {RWKV: Reinventing RNNs for the Transformer Era},
  year          = {2023},
  archiveprefix = {arXiv},
  eprint        = {2305.13048},
  primaryclass  = {cs.CL}
}


'
@inproceedings{Yeh2022,
  author     = {Yeh, Hui-Syuan and Lavergne, Thomas and Zweigenbaum, Pierre},
  booktitle  = {Proceedings of the {Thirteenth} {Language} {Resources} and {Evaluation} {Conference}},
  title      = {Decorate the {Examples}: {A} {Simple} {Method} of {Prompt} {Design} for {Biomedical} {Relation} {Extraction}},
  year       = {2022},
  address    = {Marseille, France},
  month      = jun,
  pages      = {3780--3787},
  publisher  = {European Language Resources Association},
  abstract   = {Relation extraction is a core problem for natural language processing in the biomedical domain. Recent research on relation extraction showed that prompt-based learning improves the performance on both fine-tuning on full training set and few-shot training. However, less effort has been made on domain-specific tasks where good prompt design can be even harder. In this paper, we investigate prompting for biomedical relation extraction, with experiments on the ChemProt dataset. We present a simple yet effective method to systematically generate comprehensive prompts that reformulate the relation extraction task as a cloze-test task under a simple prompt formulation. In particular, we experiment with different ranking scores for prompt selection. With BioMed-RoBERTa-base, our results show that prompting-based fine-tuning obtains gains by 14.21 F1 over its regular fine-tuning baseline, and 1.14 F1 over SciFive-Large, the current state-of-the-art on ChemProt. Besides, we find prompt-based learning requires fewer training examples to make reasonable predictions. The results demonstrate the potential of our methods in such a domain-specific relation extraction task.},
  comment    = {Resultats de CHemprot :0.172},
  file       = {:Yeh2022 - Decorate the Examples_ a Simple Method of Prompt Design for Biomedical Relation Extraction.pdf:PDF},
  shorttitle = {Decorate the {Examples}},
  url        = {https://aclanthology.org/2022.lrec-1.403},
  urldate    = {2023-09-27}
}

'
@techreport{Cho2023,
  author     = {Cho, Hyunsoo and Kim, Youna and Lee, Sang-goo},
  title      = {{CELDA}: {Leveraging} {Black}-box {Language} {Model} as {Enhanced} {Classifier} without {Labels}},
  year       = {2023},
  month      = jun,
  note       = {arXiv:2306.02693 [cs] type: article},
  abstract   = {Utilizing language models (LMs) without internal access is becoming an attractive paradigm in the field of NLP as many cutting-edge LMs are released through APIs and boast a massive scale. The de-facto method in this type of black-box scenario is known as prompting, which has shown progressive performance enhancements in situations where data labels are scarce or unavailable. Despite their efficacy, they still fall short in comparison to fully supervised counterparts and are generally brittle to slight modifications. In this paper, we propose Clustering-enhanced Linear Discriminative Analysis, a novel approach that improves the text classification accuracy with a very weak-supervision signal (i.e., name of the labels). Our framework draws a precise decision boundary without accessing weights or gradients of the LM model or data labels. The core ideas of CELDA are twofold: (1) extracting a refined pseudo-labeled dataset from an unlabeled dataset, and (2) training a lightweight and robust model on the top of LM, which learns an accurate decision boundary from an extracted noisy dataset. Throughout in-depth investigations on various datasets, we demonstrated that CELDA reaches new state-of-the-art in weakly-supervised text classification and narrows the gap with a fully-supervised model. Additionally, our proposed methodology can be applied universally to any LM and has the potential to scale to larger models, making it a more viable option for utilizing large LMs.},
  annote     = {Comment: ACL 2023},
  comment    = {Scores : T5 (11B)
                AGNews : 62.51 % 
                SST2: 71.00%
                IMDb: 71.78 %},
  file       = {:Cho2023 - CELDA_ Leveraging Black Box Language Model As Enhanced Classifier without Labels.pdf:PDF},
  keywords   = {Computer Science - Computation and Language},
  school     = {arXiv},
  shorttitle = {{CELDA}},
  url        = {http://arxiv.org/abs/2306.02693},
  urldate    = {2023-09-27}
}

'
@inproceedings{Clarke2023,
  author    = {Clarke, Christopher and Heng, Yuzhao and Kang, Yiping and Flautner, Krisztian and Tang, Lingjia and Mars, Jason},
  booktitle = {Findings of the {Association} for {Computational} {Linguistics}: {ACL} 2023},
  title     = {Label {Agnostic} {Pre}-training for {Zero}-shot {Text} {Classification}},
  year      = {2023},
  address   = {Toronto, Canada},
  month     = jul,
  pages     = {1009--1021},
  publisher = {Association for Computational Linguistics},
  abstract  = {Conventional approaches to text classification typically assume the existence of a fixed set of predefined labels to which a given text can be classified. However, in real-world applications, there exists an infinite label space for describing a given text. In addition, depending on the aspect (sentiment, topic, etc.) and domain of the text (finance, legal, etc.), the interpretation of the label can vary greatly. This makes the task of text classification, particularly in the zero-shot scenario, extremely challenging. In this paper, we investigate the task of zero-shot text classification with the aim of improving the ability of pre-trained language models (PLMs) to generalize to both seen and unseen data across varying aspects and domains. To solve this we introduce two new simple yet effective pre-training strategies, Implicit and Explicit pre-training. These methods inject aspect-level understanding into the model at train time with the goal of conditioning the model to build task-level understanding. To evaluate this, we construct and release UTCD, a new benchmark dataset for evaluating text classification in zero-shot settings. Experimental results on UTCD show that our approach achieves improved zero-shot generalization on a suite of challenging datasets across an array of zero-shot formalizations.},
  comment   = {Scores : 
               
               Financial phrase : 0.528},
  doi       = {10.18653/v1/2023.findings-acl.64},
  file      = {Full Text PDF:https\://aclanthology.org/2023.findings-acl.64.pdf:application/pdf},
  url       = {https://aclanthology.org/2023.findings-acl.64},
  urldate   = {2023-09-27}
}

'
@inproceedings{Wang2023b,
  author    = {Wang, Yuqi and Wang, Wei and Chen, Qi and Huang, Kaizhu and Nguyen, Anh and De, Suparna},
  booktitle = {Proceedings of the 61st {Annual} {Meeting} of the {Association} for {Computational} {Linguistics} ({Volume} 4: {Student} {Research} {Workshop})},
  title     = {Prompt-based {Zero}-shot {Text} {Classification} with {Conceptual} {Knowledge}},
  year      = {2023},
  address   = {Toronto, Canada},
  month     = jul,
  pages     = {30--38},
  publisher = {Association for Computational Linguistics},
  abstract  = {In recent years, pre-trained language models have garnered significant attention due to their effectiveness, which stems from the rich knowledge acquired during pre-training. To mitigate the inconsistency issues between pre-training tasks and downstream tasks and to facilitate the resolution of language-related issues, prompt-based approaches have been introduced, which are particularly useful in low-resource scenarios. However, existing approaches mostly rely on verbalizers to translate the predicted vocabulary to task-specific labels. The major limitations of this approach are the ignorance of potentially relevant domain-specific words and being biased by the pre-training data. To address these limitations, we propose a framework that incorporates conceptual knowledge for text classification in the extreme zero-shot setting. The framework includes prompt-based keyword extraction, weight assignment to each prompt keyword, and final representation estimation in the knowledge graph embedding space. We evaluated the method on four widely-used datasets for sentiment analysis and topic detection, demonstrating that it consistently outperforms recently-developed prompt-based approaches in the same experimental settings.},
  comment   = {Scors :
               Yelp :  88.82 %},
  doi       = {10.18653/v1/2023.acl-srw.4},
  file      = {:Wang2023 - Prompt Based Zero Shot Text Classification with Conceptual Knowledge.pdf:PDF},
  url       = {https://aclanthology.org/2023.acl-srw.4},
  urldate   = {2023-09-27}
}

'
@techreport{Zhang2023,
  author   = {Zhang, Kai and Gutiérrez, Bernal Jiménez and Su, Yu},
  title    = {Aligning {Instruction} {Tasks} {Unlocks} {Large} {Language} {Models} as {Zero}-{Shot} {Relation} {Extractors}},
  year     = {2023},
  month    = may,
  note     = {arXiv:2305.11159 [cs] version: 1 type: article},
  abstract = {Recent work has shown that fine-tuning large language models (LLMs) on large-scale instruction-following datasets substantially improves their performance on a wide range of NLP tasks, especially in the zero-shot setting. However, even advanced instruction-tuned LLMs still fail to outperform small LMs on relation extraction (RE), a fundamental information extraction task. We hypothesize that instruction-tuning has been unable to elicit strong RE capabilities in LLMs due to RE's low incidence in instruction-tuning datasets, making up less than 1\% of all tasks (Wang et al., 2022). To address this limitation, we propose QA4RE, a framework that aligns RE with question answering (QA), a predominant task in instruction-tuning datasets. Comprehensive zero-shot RE experiments over four datasets with two series of instruction-tuned LLMs (six LLMs in total) demonstrate that our QA4RE framework consistently improves LLM performance, strongly verifying our hypothesis and enabling LLMs to outperform strong zero-shot baselines by a large margin. Additionally, we provide thorough experiments and discussions to show the robustness, few-shot effectiveness, and strong transferability of our QA4RE framework. This work illustrates a promising way of adapting LLMs to challenging and underrepresented tasks by aligning these tasks with more common instruction-tuning tasks like QA.},
  annote   = {Comment: ACL 2023 Findings; The code is available at https://github.com/OSU-NLP-Group/QA4RE},
  comment  = {Scores :
              
              Semeval : 0.435},
  file     = {:Zhang2023 - Aligning Instruction Tasks Unlocks Large Language Models As Zero Shot Relation Extractors.pdf:PDF},
  keywords = {Computer Science - Computation and Language},
  school   = {arXiv},
  url      = {http://arxiv.org/abs/2305.11159},
  urldate  = {2023-09-27}
}

@techreport{Raffel2020,
  author   = {Raffel, Colin and Shazeer, Noam and Roberts, Adam and Lee, Katherine and Narang, Sharan and Matena, Michael and Zhou, Yanqi and Li, Wei and Liu, Peter J.},
  title    = {Exploring the {Limits} of {Transfer} {Learning} with a {Unified} {Text}-to-{Text} {Transformer}},
  year     = {2020},
  month    = jul,
  note     = {arXiv:1910.10683 [cs, stat] type: article},
  abstract = {Transfer learning, where a model is first pre-trained on a data-rich task before being fine-tuned on a downstream task, has emerged as a powerful technique in natural language processing (NLP). The effectiveness of transfer learning has given rise to a diversity of approaches, methodology, and practice. In this paper, we explore the landscape of transfer learning techniques for NLP by introducing a unified framework that converts all text-based language problems into a text-to-text format. Our systematic study compares pre-training objectives, architectures, unlabeled data sets, transfer approaches, and other factors on dozens of language understanding tasks. By combining the insights from our exploration with scale and our new ``Colossal Clean Crawled Corpus'', we achieve state-of-the-art results on many benchmarks covering summarization, question answering, text classification, and more. To facilitate future work on transfer learning for NLP, we release our data set, pre-trained models, and code.},
  annote   = {Comment: Final version as published in JMLR},
  file     = {arXiv Fulltext PDF:https\://arxiv.org/pdf/1910.10683.pdf:application/pdf},
  keywords = {Computer Science - Machine Learning, Computer Science - Computation and Language, Statistics - Machine Learning},
  school   = {arXiv},
  url      = {http://arxiv.org/abs/1910.10683},
  urldate  = {2023-06-20}
}

@inproceedings{Fei2022,
  author     = {Fei, Yu and Meng, Zhao and Nie, Ping and Wattenhofer, Roger and Sachan, Mrinmaya},
  booktitle  = {Proceedings of the 2022 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing}},
  title      = {Beyond prompting: {Making} {Pre}-trained {Language} {Models} {Better} {Zero}-shot {Learners} by {Clustering} {Representations}},
  year       = {2022},
  address    = {Abu Dhabi, United Arab Emirates},
  editor     = {Goldberg, Yoav and Kozareva, Zornitsa and Zhang, Yue},
  month      = dec,
  pages      = {8560--8579},
  publisher  = {Association for Computational Linguistics},
  abstract   = {Recent work has demonstrated that pre-trained language models (PLMs) are zero-shot learners. However, most existing zero-shot methods involve heavy human engineering or complicated self-training pipelines, hindering their application to new situations. In this work, we show that zero-shot text classification can be improved simply by clustering texts in the embedding spaces of PLMs. Specifically, we fit the unlabeled texts with a Bayesian Gaussian Mixture Model after initializing cluster positions and shapes using class names. Despite its simplicity, this approach achieves superior or comparable performance on both topic and sentiment classification datasets and outperforms prior works significantly on unbalanced datasets. We further explore the applicability of our clustering approach by evaluating it on 14 datasets with more diverse topics, text lengths, and numbers of classes. Our approach achieves an average of 20\% absolute improvement over prompt-based zero-shot learning. Finally, we compare different PLM embedding spaces and find that texts are well-clustered by topics even if the PLM is not explicitly pre-trained to generate meaningful sentence embeddings. This work indicates that PLM embeddings can categorize texts without task-specific fine-tuning, thus providing a new way to analyze and utilize their knowledge and zero-shot learning ability.},
  doi        = {10.18653/v1/2022.emnlp-main.587},
  file       = {:Fei2022 - Beyond Prompting_ Making Pre Trained Language Models Better Zero Shot Learners by Clustering Representations.pdf:PDF},
  groups     = {Prompting, EMNLP},
  shorttitle = {Beyond prompting},
  url        = {https://aclanthology.org/2022.emnlp-main.587},
  urldate    = {2023-10-31}
}

@article{Wu2023,
  author        = {Minghao Wu and Abdul Waheed and Chiyu Zhang and Muhammad Abdul-Mageed and Alham Fikri Aji},
  journal       = {CoRR},
  title         = {LaMini-LM: A Diverse Herd of Distilled Models from Large-Scale Instructions},
  year          = {2023},
  volume        = {abs/2304.14402},
  archiveprefix = {arXiv},
  eprint        = {2304.14402},
  url           = {https://arxiv.org/abs/2304.14402}
}

@techreport{Dey2023,
  author     = {Dey, Nolan and Gosal, Gurpreet and Zhiming and Chen and Khachane, Hemant and Marshall, William and Pathria, Ribhu and Tom, Marvin and Hestness, Joel},
  title      = {Cerebras-{GPT}: {Open} {Compute}-{Optimal} {Language} {Models} {Trained} on the {Cerebras} {Wafer}-{Scale} {Cluster}},
  year       = {2023},
  month      = apr,
  note       = {arXiv:2304.03208 [cs] type: article},
  abstract   = {We study recent research advances that improve large language models through efficient pre-training and scaling, and open datasets and tools. We combine these advances to introduce Cerebras-GPT, a family of open compute-optimal language models scaled from 111M to 13B parameters. We train Cerebras-GPT models on the Eleuther Pile dataset following DeepMind Chinchilla scaling rules for efficient pre-training (highest accuracy for a given compute budget). We characterize the predictable power-law scaling and compare Cerebras-GPT with other publicly-available models to show all Cerebras-GPT models have state-of-the-art training efficiency on both pre-training and downstream objectives. We describe our learnings including how Maximal Update Parameterization (\${\textbackslash}mu\$P) can further improve large model scaling, improving accuracy and hyperparameter predictability at scale. We release our pre-trained models and code, making this paper the first open and reproducible work comparing compute-optimal model scaling to models trained on fixed dataset sizes. Cerebras-GPT models are available on HuggingFace: https://huggingface.co/cerebras.},
  annote     = {Comment: 13 pages main text, 16 pages appendix, 13 figures},
  file       = {:Dey2023 - Cerebras GPT_ Open Compute Optimal Language Models Trained on the Cerebras Wafer Scale Cluster.pdf:PDF},
  keywords   = {Computer Science - Machine Learning, Computer Science - Computation and Language},
  school     = {arXiv},
  shorttitle = {Cerebras-{GPT}},
  url        = {http://arxiv.org/abs/2304.03208},
  urldate    = {2023-06-22}
}


@techreport{Wang2023_Self_instruct,
  author     = {Wang, Yizhong and Kordi, Yeganeh and Mishra, Swaroop and Liu, Alisa and Smith, Noah A. and Khashabi, Daniel and Hajishirzi, Hannaneh},
  title      = {Self-{Instruct}: {Aligning} {Language} {Models} with {Self}-{Generated} {Instructions}},
  year       = {2023},
  month      = may,
  note       = {arXiv:2212.10560 [cs] type: article},
  abstract   = {Large "instruction-tuned" language models (i.e., finetuned to respond to instructions) have demonstrated a remarkable ability to generalize zero-shot to new tasks. Nevertheless, they depend heavily on human-written instruction data that is often limited in quantity, diversity, and creativity, therefore hindering the generality of the tuned model. We introduce Self-Instruct, a framework for improving the instruction-following capabilities of pretrained language models by bootstrapping off their own generations. Our pipeline generates instructions, input, and output samples from a language model, then filters invalid or similar ones before using them to finetune the original model. Applying our method to the vanilla GPT3, we demonstrate a 33\% absolute improvement over the original model on Super-NaturalInstructions, on par with the performance of InstructGPT-001, which was trained with private user data and human annotations. For further evaluation, we curate a set of expert-written instructions for novel tasks, and show through human evaluation that tuning GPT3 with Self-Instruct outperforms using existing public instruction datasets by a large margin, leaving only a 5\% absolute gap behind InstructGPT-001. Self-Instruct provides an almost annotation-free method for aligning pre-trained language models with instructions, and we release our large synthetic dataset to facilitate future studies on instruction tuning. Our code and data are available at https://github.com/yizhongw/self-instruct.},
  annote     = {Comment: ACL 2023 camera ready, 23 pages, 9 figures, 11 tables},
  file       = {arXiv Fulltext PDF:https\://arxiv.org/pdf/2212.10560.pdf:application/pdf},
  keywords   = {Computer Science - Computation and Language, Computer Science - Artificial Intelligence},
  school     = {arXiv},
  shorttitle = {Self-{Instruct}},
  url        = {http://arxiv.org/abs/2212.10560},
  urldate    = {2023-06-23}
}


'
@techreport{Kaplan2020_scaling_laws,
  author   = {Kaplan, Jared and McCandlish, Sam and Henighan, Tom and Brown, Tom B. and Chess, Benjamin and Child, Rewon and Gray, Scott and Radford, Alec and Wu, Jeffrey and Amodei, Dario},
  title    = {Scaling {Laws} for {Neural} {Language} {Models}},
  year     = {2020},
  month    = jan,
  note     = {arXiv:2001.08361 [cs, stat] type: article},
  abstract = {We study empirical scaling laws for language model performance on the cross-entropy loss. The loss scales as a power-law with model size, dataset size, and the amount of compute used for training, with some trends spanning more than seven orders of magnitude. Other architectural details such as network width or depth have minimal effects within a wide range. Simple equations govern the dependence of overfitting on model/dataset size and the dependence of training speed on model size. These relationships allow us to determine the optimal allocation of a fixed compute budget. Larger models are significantly more sample-efficient, such that optimally compute-efficient training involves training very large models on a relatively modest amount of data and stopping significantly before convergence.},
  annote   = {Comment: 19 pages, 15 figures},
  doi      = {10.48550/arXiv.2001.08361},
  file     = {:Kaplan2020 - Scaling Laws for Neural Language Models.pdf:PDF},
  keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
  school   = {arXiv},
  url      = {http://arxiv.org/abs/2001.08361},
  urldate  = {2023-10-12}
}


@inproceedings{Radford2019_gpt2,
  author   = {Radford, Alec and Wu, Jeff and Child, Rewon and Luan, D. and Amodei, Dario and Sutskever, Ilya},
  title    = {Language {Models} are {Unsupervised} {Multitask} {Learners}},
  year     = {2019},
  abstract = {Natural language processing tasks, such as question answering, machine translation, reading comprehension, and summarization, are typically approached with supervised learning on taskspecific datasets. We demonstrate that language models begin to learn these tasks without any explicit supervision when trained on a new dataset of millions of webpages called WebText. When conditioned on a document plus questions, the answers generated by the language model reach 55 F1 on the CoQA dataset matching or exceeding the performance of 3 out of 4 baseline systems without using the 127,000+ training examples. The capacity of the language model is essential to the success of zero-shot task transfer and increasing it improves performance in a log-linear fashion across tasks. Our largest model, GPT-2, is a 1.5B parameter Transformer that achieves state of the art results on 7 out of 8 tested language modeling datasets in a zero-shot setting but still underfits WebText. Samples from the model reflect these improvements and contain coherent paragraphs of text. These findings suggest a promising path towards building language processing systems which learn to perform tasks from their naturally occurring demonstrations.},
  annote   = {[TLDR] It is demonstrated that language models begin to learn these tasks without any explicit supervision when trained on a new dataset of millions of webpages called WebText, suggesting a promising path towards building language processing systems which learn to perform tasks from their naturally occurring demonstrations.},
  file     = {:Radford2019 - Language Models Are Unsupervised Multitask Learners.html:URL;:Radford2019 - Language Models Are Unsupervised Multitask Learners.pdf:PDF},
  url      = {https://www.semanticscholar.org/paper/Language-Models-are-Unsupervised-Multitask-Learners-Radford-Wu/9405cc0d6169988371b2755e573cc28650d14dfe},
  urldate  = {2023-06-20}
}

'
@inproceedings{Liu2019_capsule_network,
  author    = {Liu, Han and Zhang, Xiaotong and Fan, Lu and Fu, Xuandi and Li, Qimai and Wu, Xiao-Ming and Lam, Albert Y.S.},
  booktitle = {Proceedings of the 2019 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing} and the 9th {International} {Joint} {Conference} on {Natural} {Language} {Processing} ({EMNLP}-{IJCNLP})},
  title     = {Reconstructing {Capsule} {Networks} for {Zero}-shot {Intent} {Classification}},
  year      = {2019},
  address   = {Hong Kong, China},
  month     = nov,
  pages     = {4799--4809},
  publisher = {Association for Computational Linguistics},
  abstract  = {Intent classification is an important building block of dialogue systems. With the burgeoning of conversational AI, existing systems are not capable of handling numerous fast-emerging intents, which motivates zero-shot intent classification. Nevertheless, research on this problem is still in the incipient stage and few methods are available. A recently proposed zero-shot intent classification method, IntentCapsNet, has been shown to achieve state-of-the-art performance. However, it has two unaddressed limitations: (1) it cannot deal with polysemy when extracting semantic capsules; (2) it hardly recognizes the utterances of unseen intents in the generalized zero-shot intent classification setting. To overcome these limitations, we propose to reconstruct capsule networks for zero-shot intent classification. First, we introduce a dimensional attention mechanism to fight against polysemy. Second, we reconstruct the transformation matrices for unseen intents by utilizing abundant latent information of the labeled utterances, which significantly improves the model generalization ability. Experimental results on two task-oriented dialogue datasets in different languages show that our proposed method outperforms IntentCapsNet and other strong baselines.},
  doi       = {10.18653/v1/D19-1486},
  file      = {:Liu2019a - Reconstructing Capsule Networks for Zero Shot Intent Classification.pdf:PDF},
  url       = {https://aclanthology.org/D19-1486},
  urldate   = {2023-10-17}
}

@techreport{Zhao2023_survey_llm,
  author   = {Zhao, Wayne Xin and Zhou, Kun and Li, Junyi and Tang, Tianyi and Wang, Xiaolei and Hou, Yupeng and Min, Yingqian and Zhang, Beichen and Zhang, Junjie and Dong, Zican and Du, Yifan and Yang, Chen and Chen, Yushuo and Chen, Zhipeng and Jiang, Jinhao and Ren, Ruiyang and Li, Yifan and Tang, Xinyu and Liu, Zikang and Liu, Peiyu and Nie, Jian-Yun and Wen, Ji-Rong},
  title    = {A {Survey} of {Large} {Language} {Models}},
  year     = {2023},
  month    = may,
  note     = {arXiv:2303.18223 [cs] type: article},
  abstract = {Language is essentially a complex, intricate system of human expressions governed by grammatical rules. It poses a significant challenge to develop capable AI algorithms for comprehending and grasping a language. As a major approach, language modeling has been widely studied for language understanding and generation in the past two decades, evolving from statistical language models to neural language models. Recently, pre-trained language models (PLMs) have been proposed by pre-training Transformer models over large-scale corpora, showing strong capabilities in solving various NLP tasks. Since researchers have found that model scaling can lead to performance improvement, they further study the scaling effect by increasing the model size to an even larger size. Interestingly, when the parameter scale exceeds a certain level, these enlarged language models not only achieve a significant performance improvement but also show some special abilities that are not present in small-scale language models. To discriminate the difference in parameter scale, the research community has coined the term large language models (LLM) for the PLMs of significant size. Recently, the research on LLMs has been largely advanced by both academia and industry, and a remarkable progress is the launch of ChatGPT, which has attracted widespread attention from society. The technical evolution of LLMs has been making an important impact on the entire AI community, which would revolutionize the way how we develop and use AI algorithms. In this survey, we review the recent advances of LLMs by introducing the background, key findings, and mainstream techniques. In particular, we focus on four major aspects of LLMs, namely pre-training, adaptation tuning, utilization, and capacity evaluation. Besides, we also summarize the available resources for developing LLMs and discuss the remaining issues for future directions.},
  annote   = {Comment: ongoing work; 58 pages},
  file     = {:Zhao2023 - A Survey of Large Language Models.pdf:PDF},
  keywords = {Computer Science - Computation and Language, Computer Science - Artificial Intelligence},
  school   = {arXiv},
  url      = {http://arxiv.org/abs/2303.18223},
  urldate  = {2023-06-22}
}

@techreport{Schick2021_cloze_style,
  author   = {Schick, Timo and Schütze, Hinrich},
  title    = {Exploiting {Cloze} {Questions} for {Few} {Shot} {Text} {Classification} and {Natural} {Language} {Inference}},
  year     = {2021},
  month    = jan,
  note     = {arXiv:2001.07676 [cs] type: article},
  abstract = {Some NLP tasks can be solved in a fully unsupervised fashion by providing a pretrained language model with "task descriptions" in natural language (e.g., Radford et al., 2019). While this approach underperforms its supervised counterpart, we show in this work that the two ideas can be combined: We introduce Pattern-Exploiting Training (PET), a semi-supervised training procedure that reformulates input examples as cloze-style phrases to help language models understand a given task. These phrases are then used to assign soft labels to a large set of unlabeled examples. Finally, standard supervised training is performed on the resulting training set. For several tasks and languages, PET outperforms supervised training and strong semi-supervised approaches in low-resource settings by a large margin.},
  annote   = {Comment: Accepted at EACL2021},
  file     = {:Schick2021 - Exploiting Cloze Questions for Few Shot Text Classification and Natural Language Inference.pdf:PDF},
  keywords = {Computer Science - Computation and Language},
  school   = {arXiv},
  url      = {http://arxiv.org/abs/2001.07676},
  urldate  = {2023-06-05}
}

@InProceedings{zhu-etal-2024-zero-shot,
  author    = {Zhu, Zhihong and Cheng, Xuxin and An, Hao and Wang, Zhichang and Chen, Dongsheng and Huang, Zhiqi},
  booktitle = {Proceedings of the 2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation (LREC-COLING 2024)},
  title     = {Zero-Shot Spoken Language Understanding via Large Language Models: A Preliminary Study},
  year      = {2024},
  address   = {Torino, Italia},
  editor    = {Calzolari, Nicoletta and Kan, Min-Yen and Hoste, Veronique and Lenci, Alessandro and Sakti, Sakriani and Xue, Nianwen},
  month     = may,
  pages     = {17877--17883},
  publisher = {ELRA and ICCL},
  abstract  = {Zero-shot Spoken Language Understanding (SLU) aims to enable task-oriented dialogue systems to understand user needs without training data. Challenging but worthwhile, zero-shot SLU reduces the time and effort that data labeling takes. Recent advancements in large language models (LLMs), such as GPT3.5 and ChatGPT, have shown promising results in zero-shot settings, which motivates us to explore prompt-based methods. In this study, we investigate whether strong SLU models can be constructed by directly prompting LLMs. Specifically, we propose a simple yet effective two-stage framework dubbed GPT-SLU, which transforms the SLU task into a question-answering problem. Powered by multi-stage mutual guided prompts, GPT-SLU can leverage the correlations between two subtasks in SLU to achieve better predictions, which is greatly explored in the traditional fine-tuning paradigm. Experimental results on three SLU benchmark datasets demonstrate the significant potential of LLMs for zero-shot SLU. Comprehensive analyses validate the effectiveness of our proposed framework and also indicate that there is still room for further improvement of LLMs in SLU scenarios.},
  url       = {https://aclanthology.org/2024.lrec-main.1554},
}

@InProceedings{Wu2022,
  author    = {Wu, Yangjun and Wang, Han and Zhang, Dongxiang and Chen, Gang and Zhang, Hao},
  booktitle = {Proceedings of the 29th International Conference on Computational Linguistics},
  title     = {Incorporating Instructional Prompts into a Unified Generative Framework for Joint Multiple Intent Detection and Slot Filling},
  year      = {2022},
  address   = {Gyeongju, Republic of Korea},
  editor    = {Calzolari, Nicoletta and Huang, Chu-Ren and Kim, Hansaem and Pustejovsky, James and Wanner, Leo and Choi, Key-Sun and Ryu, Pum-Mo and Chen, Hsin-Hsi and Donatelli, Lucia and Ji, Heng and Kurohashi, Sadao and Paggio, Patrizia and Xue, Nianwen and Kim, Seokhwan and Hahm, Younggyun and He, Zhong and Lee, Tony Kyungil and Santus, Enrico and Bond, Francis and Na, Seung-Hoon},
  month     = oct,
  pages     = {7203--7208},
  publisher = {International Committee on Computational Linguistics},
  abstract  = {The joint multiple Intent Detection (ID) and Slot Filling (SF) is a significant challenge in spoken language understanding. Because the slots in an utterance may relate to multi-intents, most existing approaches focus on utilizing task-specific components to capture the relations between intents and slots. The customized networks restrict models from modeling commonalities between tasks and generalization for broader applications. To address the above issue, we propose a Unified Generative framework (UGEN) based on a prompt-based paradigm, and formulate the task as a question-answering problem. Specifically, we design 5-type templates as instructional prompts, and each template includes a question that acts as the driver to teach UGEN to grasp the paradigm, options that list the candidate intents or slots to reduce the answer search space, and the context denotes original utterance. Through the instructional prompts, UGEN is guided to understand intents, slots, and their implicit correlations. On two popular multi-intent benchmark datasets, experimental results demonstrate that UGEN achieves new SOTA performances on full-data and surpasses the baselines by a large margin on 5-shot (28.1{\%}) and 10-shot (23{\%}) scenarios, which verify that UGEN is robust and effective.},
  groups    = {AttempsSLU},
  url       = {https://aclanthology.org/2022.coling-1.631},
}

@InProceedings{Wang2023_fewshotclassif,
  author    = {Wang, Yufan and Mei, Jie and Zou, Bowei and Fan, Rui and He, Tingting and Aw, Ai Ti},
  booktitle = {Findings of the Association for Computational Linguistics: ACL 2023},
  title     = {Making Pre-trained Language Models Better Learn Few-Shot Spoken Language Understanding in More Practical Scenarios},
  year      = {2023},
  address   = {Toronto, Canada},
  editor    = {Rogers, Anna and Boyd-Graber, Jordan and Okazaki, Naoaki},
  month     = jul,
  pages     = {13508--13523},
  publisher = {Association for Computational Linguistics},
  abstract  = {Most previous few-shot Spoken Language Understanding (SLU) models typically need to be trained on a set of data-rich source domains and adapt to the target domain with a few examples. In this paper, we explore a more practical scenario for few-shot SLU, in which we only assume access to a pre-trained language model and a few labeled examples without any other source domain data. We concentrate on understanding how far the few-shot SLU could be pushed in this setting. To this end, we develop a prompt-based intent detection model in few-shot settings, which leverages the BERT original pre-training next sentence prediction task and the prompt template to detect the user{'}s intent. For slot filling, we propose an approach of reconstructing slot labels, which reduces the training complexity by reducing the number of slot labels in few-shot settings. To evaluate the few-shot SLU for a more practical scenario, we present two benchmarks, FewShotATIS and FewShotSNIPS. And a dynamic sampling strategy is designed to construct the two datasets according to the learning difficulty of each intent and slot. Experiments on FewShotATIS and FewShotSNIPS demonstrate that our proposed model achieves state-of-the-art performance.},
  doi       = {10.18653/v1/2023.findings-acl.853},
  groups    = {AttempsSLU},
  url       = {https://aclanthology.org/2023.findings-acl.853},
}

@Misc{misc_sms_spam_collection_228,
  author       = {Almeida,Tiago and Hidalgo,Jos},
  howpublished = {UCI Machine Learning Repository},
  note         = {{DOI}: https://doi.org/10.24432/C5CC84},
  title        = {{SMS Spam Collection}},
  year         = {2012},
}

@InProceedings{hendrickx2010semeval,
  author    = {Hendrickx, Iris and Kim, Su Nam and Kozareva, Zornitsa and Nakov, Preslav and {\'O} S{\'e}aghdha, Diarmuid and Pad{\'o}, Sebastian and Pennacchiotti, Marco and Romano, Lorenza and Szpakowicz, Stan},
  booktitle = {Proceedings of the 5th International Workshop on Semantic Evaluation},
  title     = {{S}em{E}val-2010 Task 8: Multi-Way Classification of Semantic Relations between Pairs of Nominals},
  year      = {2010},
  address   = {Uppsala, Sweden},
  month     = jul,
  pages     = {33--38},
  publisher = {Association for Computational Linguistics},
  url       = {https://aclanthology.org/S10-1006},
}

@Article{liu_few-shot_2022,
  author   = {Liu, Haokun and Tam, Derek and Muqeeth, Mohammed and Mohta, Jay and Huang, Tenghao and Bansal, Mohit and Raffel, Colin A.},
  journal  = {Advances in Neural Information Processing Systems},
  title    = {Few-{Shot} {Parameter}-{Efficient} {Fine}-{Tuning} is {Better} and {Cheaper} than {In}-{Context} {Learning}},
  year     = {2022},
  month    = dec,
  pages    = {1950--1965},
  volume   = {35},
  language = {en},
  url      = {https://proceedings.neurips.cc/paper_files/paper/2022/hash/0cde695b83bd186c1fd456302888454c-Abstract-Conference.html},
  urldate  = {2023-09-29},
}

@InProceedings{li_generative_2023,
  author    = {Li, Xuefeng and Wang, Liwen and Dong, Guanting and He, Keqing and Zhao, Jinzheng and Lei, Hao and Liu, Jiachi and Xu, Weiran},
  booktitle = {Findings of the {Association} for {Computational} {Linguistics}: {ACL} 2023},
  title     = {Generative {Zero}-{Shot} {Prompt} {Learning} for {Cross}-{Domain} {Slot} {Filling} with {Inverse} {Prompting}},
  year      = {2023},
  address   = {Toronto, Canada},
  month     = jul,
  pages     = {825--834},
  publisher = {Association for Computational Linguistics},
  abstract  = {Zero-shot cross-domain slot filling aims to transfer knowledge from the labeled source domain to the unlabeled target domain. Existing models either encode slot descriptions and examples or design handcrafted question templates using heuristic rules, suffering from poor generalization capability or robustness. In this paper, we propose a generative zero-shot prompt learning framework for cross-domain slot filling, both improving generalization and robustness than previous work. Besides, we introduce a novel inverse prompting strategy to distinguish different slot types to avoid the multiple prediction problem, and an efficient prompt tuning strategy to boost higher performance only training fewer prompt parameters. Experiments and analysis demonstrate the effectiveness of our proposed framework, especially huge improvements (+13.44\% F1) on the unseen slots.},
  url       = {https://aclanthology.org/2023.findings-acl.52},
  urldate   = {2023-08-04},
}

@Article{falcon40b,
  author = {Almazrouei, Ebtesam and others},
  title  = {{Falcon-40B}: an open large language model with state-of-the-art performance},
  year   = {2023},
}

@InProceedings{devlin_bert_2019,
  author     = {Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  booktitle  = {Proceedings of the 2019 {Conference} of the {North} {American} {Chapter} of the {Association} for {Computational} {Linguistics}: {Human} {Language} {Technologies}, {Volume} 1 ({Long} and {Short} {Papers})},
  title      = {{BERT}: {Pre}-training of {Deep} {Bidirectional} {Transformers} for {Language} {Understanding}},
  year       = {2019},
  address    = {Minneapolis, Minnesota},
  month      = jun,
  pages      = {4171--4186},
  publisher  = {Association for Computational Linguistics},
  abstract   = {We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5 (7.7 point absolute improvement), MultiNLI accuracy to 86.7\% (4.6\% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).},
  doi        = {10.18653/v1/N19-1423},
  shorttitle = {{BERT}},
  url        = {https://aclanthology.org/N19-1423},
  urldate    = {2023-10-08},
}

@Languageresource{longpre_flan_2023,
  abstract   = {We study the design decisions of publicly available instruction tuning methods, and break down the development of Flan 2022 (Chung et al., 2022). Through careful ablation studies on the Flan Collection of tasks and methods, we tease apart the effect of design decisions which enable Flan-T5 to outperform prior work by 3-17\%+ across evaluation settings. We find task balancing and enrichment techniques are overlooked but critical to effective instruction tuning, and in particular, training with mixed prompt settings (zero-shot, few-shot, and chain-of-thought) actually yields stronger (2\%+) performance in all settings. In further experiments, we show Flan-T5 requires less finetuning to converge higher and faster than T5 on single downstream tasks, motivating instruction-tuned models as more computationally-efficient starting checkpoints for new tasks. Finally, to accelerate research on instruction tuning, we make the Flan 2022 collection of datasets, templates, and methods publicly available at https://github.com/google-research/FLAN/tree/main/flan/v2.},
  author     = {Longpre, Shayne and Hou, Le and Vu, Tu and Webson, Albert and Chung, Hyung Won and Tay, Yi and Zhou, Denny and Le, Quoc V. and Zoph, Barret and Wei, Jason and Roberts, Adam},
  copyright  = {Creative Commons Attribution 4.0 International},
  doi        = {10.48550/ARXIV.2301.13688},
  keywords   = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning},
  month      = feb,
  note       = {arXiv:2301.13688 [cs]},
  publisher  = {arXiv},
  shorttitle = {The {Flan} {Collection}},
  title      = {The {Flan} {Collection}: {Designing} {Data} and {Methods} for {Effective} {Instruction} {Tuning}},
  url        = {https://arxiv.org/abs/2301.13688},
  urldate    = {2023-09-29},
  year       = {2023},
}

@Languageresource{penedo_refinedweb_2023,
  abstract   = {Large language models are commonly trained on a mixture of filtered web data and curated high-quality corpora, such as social media conversations, books, or technical papers. This curation process is believed to be necessary to produce performant models with broad zero-shot generalization abilities. However, as larger models requiring pretraining on trillions of tokens are considered, it is unclear how scalable is curation and whether we will run out of unique high-quality data soon. At variance with previous beliefs, we show that properly filtered and deduplicated web data alone can lead to powerful models; even significantly outperforming models from the state-of-the-art trained on The Pile. Despite extensive filtering, the high-quality data we extract from the web is still plentiful, and we are able to obtain five trillion tokens from CommonCrawl. We publicly release an extract of 600 billion tokens from our RefinedWeb dataset, and 1.3/7.5B parameters language models trained on it.},
  author     = {Penedo, Guilherme and Malartic, Quentin and Hesslow, Daniel and Cojocaru, Ruxandra and Cappelli, Alessandro and Alobeidli, Hamza and Pannier, Baptiste and Almazrouei, Ebtesam and Launay, Julien},
  doi        = {10.48550/arXiv.2306.01116},
  file       = {:Penedo2023 - The RefinedWeb Dataset for Falcon LLM_ Outperforming Curated Corpora with Web Data, and Web Data Only.pdf:PDF},
  keywords   = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
  month      = jun,
  note       = {arXiv:2306.01116 [cs]},
  publisher  = {arXiv},
  school     = {arXiv},
  shorttitle = {The {RefinedWeb} {Dataset} for {Falcon} {LLM}},
  title      = {The {RefinedWeb} {Dataset} for {Falcon} {LLM}: {Outperforming} {Curated} {Corpora} with {Web} {Data}, and {Web} {Data} {Only}},
  url        = {http://arxiv.org/abs/2306.01116},
  urldate    = {2023-09-29},
  year       = {2023},
}

@Misc{bang_task-optimized_2023,
  author    = {Bang, Namo and Lee, Jeehyun and Koo, Myoung-Wan},
  month     = may,
  note      = {arXiv:2305.02468 [cs]},
  title     = {Task-{Optimized} {Adapters} for an {End}-to-{End} {Task}-{Oriented} {Dialogue} {System}},
  year      = {2023},
  abstract  = {Task-Oriented Dialogue (TOD) systems are designed to carry out specific tasks by tracking dialogue states and generating appropriate responses to help users achieve defined goals. Recently, end-to-end dialogue models pre-trained based on large datasets have shown promising performance in the conversational system. However, they share the same parameters to train tasks of the dialogue system (NLU, DST, NLG), so debugging each task is challenging. Also, they require a lot of effort to fine-tune large parameters to create a task-oriented chatbot, making it difficult for non-experts to handle. Therefore, we intend to train relatively lightweight and fast models compared to PLM. In this paper, we propose an End-to-end TOD system with Task-Optimized Adapters which learn independently per task, adding only small number of parameters after fixed layers of pre-trained network. We also enhance the performance of the DST and NLG modules through reinforcement learning, overcoming the learning curve that has lacked at the adapter learning and enabling the natural and consistent response generation that is appropriate for the goal. Our method is a model-agnostic approach and does not require prompt-tuning as only input data without a prompt. As results of the experiment, our method shows competitive performance on the MultiWOZ benchmark compared to the existing end-to-end models. In particular, we attain state-of-the-art performance on the DST task of 2.2 dataset.},
  doi       = {10.48550/arXiv.2305.02468},
  journal   = {arXiv.org},
  keywords  = {Important},
  language  = {en},
  publisher = {arXiv},
  url       = {https://arxiv.org/abs/2305.02468v3},
  urldate   = {2023-10-30},
}

@InProceedings{cui_template-based_2021,
  author    = {Cui, Leyang and Wu, Yu and Liu, Jian and Yang, Sen and Zhang, Yue},
  booktitle = {Findings of the {Association} for {Computational} {Linguistics}: {ACL}-{IJCNLP} 2021},
  title     = {Template-{Based} {Named} {Entity} {Recognition} {Using} {BART}},
  year      = {2021},
  address   = {Online},
  editor    = {Zong, Chengqing and Xia, Fei and Li, Wenjie and Navigli, Roberto},
  month     = aug,
  pages     = {1835--1845},
  publisher = {Association for Computational Linguistics},
  doi       = {10.18653/v1/2021.findings-acl.161},
  groups    = {Constrained Prompting},
  url       = {https://aclanthology.org/2021.findings-acl.161},
  urldate   = {2023-10-08},
}

@Misc{instruct-llm-survey,
  author     = {Zhang, Shengyu and Dong, Linfeng and Li, Xiaoya and Zhang, Sen and Sun, Xiaofei and Wang, Shuhe and Li, Jiwei and Hu, Runyi and Zhang, Tianwei and Wu, Fei and Wang, Guoyin},
  month      = aug,
  note       = {arXiv:2308.10792 [cs]},
  title      = {Instruction {Tuning} for {Large} {Language} {Models}: {A} {Survey}},
  year       = {2023},
  abstract   = {This paper surveys research works in the quickly advancing field of instruction tuning (IT), a crucial technique to enhance the capabilities and controllability of large language models (LLMs). Instruction tuning refers to the process of further training LLMs on a dataset consisting of {\textbackslash}textsc\{(instruction, output)\} pairs in a supervised fashion, which bridges the gap between the next-word prediction objective of LLMs and the users' objective of having LLMs adhere to human instructions. In this work, we make a systematic review of the literature, including the general methodology of IT, the construction of IT datasets, the training of IT models, and applications to different modalities, domains and applications, along with an analysis on aspects that influence the outcome of IT (e.g., generation of instruction outputs, size of the instruction dataset, etc). We also review the potential pitfalls of IT along with criticism against it, along with efforts pointing out current deficiencies of existing strategies and suggest some avenues for fruitful research.},
  keywords   = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning},
  publisher  = {arXiv},
  shorttitle = {Instruction {Tuning} for {Large} {Language} {Models}},
  url        = {http://arxiv.org/abs/2308.10792},
  urldate    = {2023-09-18},
}

@Misc{flant5,
  author    = {Wei, Jason and Bosma, Maarten and Zhao, Vincent Y. and Guu, Kelvin and Yu, Adams Wei and Lester, Brian and Du, Nan and Dai, Andrew M. and Le, Quoc V.},
  month     = feb,
  note      = {arXiv:2109.01652 [cs]},
  title     = {Finetuned {Language} {Models} {Are} {Zero}-{Shot} {Learners}},
  year      = {2022},
  abstract  = {This paper explores a simple method for improving the zero-shot learning abilities of language models. We show that instruction tuning -- finetuning language models on a collection of tasks described via instructions -- substantially improves zero-shot performance on unseen tasks. We take a 137B parameter pretrained language model and instruction-tune it on over 60 NLP tasks verbalized via natural language instruction templates. We evaluate this instruction-tuned model, which we call FLAN, on unseen task types. FLAN substantially improves the performance of its unmodified counterpart and surpasses zero-shot 175B GPT-3 on 20 of 25 tasks that we evaluate. FLAN even outperforms few-shot GPT-3 by a large margin on ANLI, RTE, BoolQ, AI2-ARC, OpenbookQA, and StoryCloze. Ablation studies reveal that number of finetuning datasets, model scale, and natural language instructions are key to the success of instruction tuning.},
  doi       = {10.48550/arXiv.2109.01652},
  keywords  = {Computer Science - Computation and Language},
  publisher = {arXiv},
  url       = {http://arxiv.org/abs/2109.01652},
  urldate   = {2023-09-18},
}

@Misc{zheng_judging_2023,
  author   = {Zheng, Lianmin and Chiang, Wei-Lin and Sheng, Ying and Zhuang, Siyuan and Wu, Zhanghao and Zhuang, Yonghao and Lin, Zi and Li, Zhuohan and Li, Dacheng and Xing, Eric P. and Zhang, Hao and Gonzalez, Joseph E. and Stoica, Ion},
  month    = jun,
  title    = {Judging {LLM}-as-a-judge with {MT}-{Bench} and {Chatbot} {Arena}},
  year     = {2023},
  abstract = {Evaluating large language model (LLM) based chat assistants is challenging due to their broad capabilities and the inadequacy of existing benchmarks in measuring human preferences. To address this, we explore using strong LLMs as judges to evaluate these models on more open-ended questions. We examine the usage and limitations of LLM-as-a-judge, including position, verbosity, and self-enhancement biases, as well as limited reasoning ability, and propose solutions to mitigate some of them. We then verify the agreement between LLM judges and human preferences by introducing two benchmarks: MT-bench, a multi-turn question set; and Chatbot Arena, a crowdsourced battle platform. Our results reveal that strong LLM judges like GPT-4 can match both controlled and crowdsourced human preferences well, achieving over 80{\textbackslash}\% agreement, the same level of agreement between humans. Hence, LLM-as-a-judge is a scalable and explainable way to approximate human preferences, which are otherwise very expensive to obtain. Additionally, we show our benchmark and traditional benchmarks complement each other by evaluating several variants of LLaMA and Vicuna. We will publicly release MT-bench questions, 3K expert votes, and 30K conversations with human preferences from Chatbot Arena.},
  journal  = {arXiv.org},
  language = {en},
  url      = {https://arxiv.org/abs/2306.05685v2},
  urldate  = {2023-10-09},
}

@InProceedings{muennighoff_crosslingual_2023,
  author    = {Muennighoff, Niklas and Wang, Thomas and Sutawika, Lintang and Roberts, Adam and Biderman, Stella and Le Scao, Teven and Bari, M Saiful and Shen, Sheng and Yong, Zheng Xin and Schoelkopf, Hailey and Tang, Xiangru and Radev, Dragomir and Aji, Alham Fikri and Almubarak, Khalid and Albanie, Samuel and Alyafeai, Zaid and Webson, Albert and Raff, Edward and Raffel, Colin},
  booktitle = {Proceedings of the 61st {Annual} {Meeting} of the {Association} for {Computational} {Linguistics} ({Volume} 1: {Long} {Papers})},
  title     = {Crosslingual {Generalization} through {Multitask} {Finetuning}},
  year      = {2023},
  address   = {Toronto, Canada},
  month     = jul,
  pages     = {15991--16111},
  publisher = {Association for Computational Linguistics},
  abstract  = {Multitask prompted finetuning (MTF) has been shown to help large language models generalize to new tasks in a zero-shot setting, but so far explorations of MTF have focused on English data and models. We apply MTF to the pretrained multilingual BLOOM and mT5 model families to produce finetuned variants called BLOOMZ and mT0. We find finetuning large multilingual language models on English tasks with English prompts allows for task genrealization to non-English languages that appear only in the pretraining corpus. Finetuning on multilingual tasks with English prompts further improves performance on English and non-English tasks leading to various state-of-the-art zero-shot results. We also investigate finetuning on multilingual tasks with prompts that have been machine-translated from English to match the language of each dataset. We find training on these machine-translated prompts leads to better performance on human-written prompts in the respective languages. Surprisingly, we find models are capable of zero-shot generalization to tasks in languages they have never intentionally seen. We conjecture that the models are learning higher-level capabilities that are both task- and language-agnostic. In addition, we introduce xP3, a composite of supervised datasets in 46 languages with English and machine-translated prompts. Our code, datasets and models are freely available at https://github.com/bigscience-workshop/xmtf.},
  doi       = {10.18653/v1/2023.acl-long.891},
  url       = {https://aclanthology.org/2023.acl-long.891},
  urldate   = {2023-10-09},
}

@article{ouyang_training_2022,
  title    = {Training language models to follow instructions with human feedback},
  volume   = {35},
  url      = {https://proceedings.neurips.cc/paper_files/paper/2022/hash/b1efde53be364a73914f58805a001731-Abstract-Conference.html},
  language = {en},
  urldate  = {2023-10-09},
  journal  = {Advances in Neural Information Processing Systems},
  author   = {Ouyang, Long and Wu, Jeffrey and Jiang, Xu and Almeida, Diogo and Wainwright, Carroll and Mishkin, Pamela and Zhang, Chong and Agarwal, Sandhini and Slama, Katarina and Ray, Alex and Schulman, John and Hilton, Jacob and Kelton, Fraser and Miller, Luke and Simens, Maddie and Askell, Amanda and Welinder, Peter and Christiano, Paul F. and Leike, Jan and Lowe, Ryan},
  month    = dec,
  year     = {2022},
  pages    = {27730--27744}
}

@InProceedings{peft-lora,
  author     = {Hu, Edward J. and Shen, Yelong and Wallis, Phillip and Allen-Zhu, Zeyuan and Li, Yuanzhi and Wang, Shean and Wang, Lu and Chen, Weizhu},
  booktitle  = {International Conference on Learning Representations},
  title      = {{LoRA}: {Low}-{Rank} {Adaptation} of {Large} {Language} {Models}},
  year       = {2022},
  month      = oct,
  abstract   = {An important paradigm of natural language processing consists of large-scale pre-training on general domain data and adaptation to particular tasks or domains. As we pre-train larger models, full fine-tuning, which retrains all model parameters, becomes less feasible. Using GPT-3 175B as an example -- deploying independent instances of fine-tuned models, each with 175B parameters, is prohibitively expensive. We propose Low-Rank Adaptation, or LoRA, which freezes the pre-trained model weights and injects trainable rank decomposition matrices into each layer of the Transformer architecture, greatly reducing the number of trainable parameters for downstream tasks. Compared to GPT-3 175B fine-tuned with Adam, LoRA can reduce the number of trainable parameters by a factor of 10,000 and the GPU memory requirement by a factor of 3. LoRA performs on-par or better than fine-tuning in model quality on RoBERTa, DeBERTa, GPT-2, and GPT-3, despite having fewer trainable parameters, a higher training throughput, and, unlike adapters, no additional inference latency. We also provide an empirical investigation into rank-deficiency in language model adaptation, which sheds light on the efficacy of LoRA. We release a package that facilitates the integration of LoRA with PyTorch models and provide our implementations and model checkpoints for RoBERTa, DeBERTa, and GPT-2 at https://github.com/microsoft/LoRA.},
  language   = {en},
  shorttitle = {{LoRA}},
  url        = {https://openreview.net/forum?id=nZeVKeeFYf9},
  urldate    = {2023-10-09},
}

@Misc{tunstall_efficient_2022,
  author    = {Tunstall, Lewis and Reimers, Nils and Jo, Unso Eun Seo and Bates, Luke and Korat, Daniel and Wasserblat, Moshe and Pereg, Oren},
  month     = sep,
  note      = {arXiv:2209.11055 [cs]},
  title     = {Efficient {Few}-{Shot} {Learning} {Without} {Prompts}},
  year      = {2022},
  abstract  = {Recent few-shot methods, such as parameter-efficient fine-tuning (PEFT) and pattern exploiting training (PET), have achieved impressive results in label-scarce settings. However, they are difficult to employ since they are subject to high variability from manually crafted prompts, and typically require billion-parameter language models to achieve high accuracy. To address these shortcomings, we propose SetFit (Sentence Transformer Fine-tuning), an efficient and prompt-free framework for few-shot fine-tuning of Sentence Transformers (ST). SetFit works by first fine-tuning a pretrained ST on a small number of text pairs, in a contrastive Siamese manner. The resulting model is then used to generate rich text embeddings, which are used to train a classification head. This simple framework requires no prompts or verbalizers, and achieves high accuracy with orders of magnitude less parameters than existing techniques. Our experiments show that SetFit obtains comparable results with PEFT and PET techniques, while being an order of magnitude faster to train. We also show that SetFit can be applied in multilingual settings by simply switching the ST body. Our code is available at https://github.com/huggingface/setfit and our datasets at https://huggingface.co/setfit .},
  doi       = {10.48550/arXiv.2209.11055},
  journal   = {arXiv.org},
  keywords  = {Computer Science - Computation and Language},
  language  = {en},
  publisher = {arXiv},
  url       = {https://arxiv.org/abs/2209.11055v1},
  urldate   = {2023-09-29},
}

@InProceedings{kwon-etal-2023-sidlr,
  author     = {Kwon, Sang Yun and Bhatia, Gagan and Nagoudi, Elmoatez Billah and Alcoba Inciarte, Alcides and Abdul-mageed, Muhammad},
  booktitle  = {Tenth Workshop on NLP for Similar Languages, Varieties and Dialects (VarDial 2023)},
  title      = {{SIDLR}: Slot and Intent Detection Models for Low-Resource Language Varieties},
  year       = {2023},
  address    = {Dubrovnik, Croatia},
  month      = may,
  pages      = {241--250},
  publisher  = {Association for Computational Linguistics},
  abstract   = {Intent detection and slot filling are two critical tasks in spoken and natural language understandingfor task-oriented dialog systems. In this work, we describe our participation in slot and intent detection for low-resource language varieties (SID4LR) (Aepli et al., 2023). We investigate the slot and intent detection (SID) tasks using a wide range of models and settings. Given the recent success of multitask promptedfinetuning of the large language models, we also test the generalization capability of the recent encoder-decoder model mT0 (Muennighoff et al., 2022) on new tasks (i.e., SID) in languages they have never intentionally seen. We show that our best model outperforms the baseline by a large margin (up to +30 F1 points) in both SID tasks.},
  doi        = {10.18653/v1/2023.vardial-1.24},
  keywords   = {Important},
  shorttitle = {{SIDLR}},
  url        = {https://aclanthology.org/2023.vardial-1.24},
  urldate    = {2023-10-08},
}

@Misc{chang2023prompting,
  author        = {Kai-Wei Chang and Ming-Hsin Chen and Yun-Ping Lin and Jing Neng Hsu and Paul Kuo-Ming Huang and Chien-yu Huang and Shang-Wen Li and Hung-yi Lee},
  month         = oct,
  title         = {Prompting and Adapter Tuning for Self-supervised Encoder-Decoder Speech Model},
  year          = {2023},
  abstract      = {Prompting and adapter tuning have emerged as efficient alternatives to fine-tuning (FT) methods. However, existing studies on speech prompting focused on classification tasks and failed on more complex sequence generation tasks. Besides, adapter tuning is primarily applied with a focus on encoder-only self-supervised models. Our experiments show that prompting on Wav2Seq, a self-supervised encoder-decoder model, surpasses previous works in sequence generation tasks. It achieves a remarkable 53\% relative improvement in word error rate for ASR and a 27\% in F1 score for slot filling. Additionally, prompting competes with the FT method in the low-resource scenario. Moreover, we show the transferability of prompting and adapter tuning on Wav2Seq in cross-lingual ASR. When limited trainable parameters are involved, prompting and adapter tuning consistently outperform conventional FT across 7 languages. Notably, in the low-resource scenario, prompting consistently outperforms adapter tuning.},
  archiveprefix = {arXiv},
  eprint        = {2310.02971},
  journal       = {arXiv.org},
  language      = {en},
  primaryclass  = {eess.AS},
  url           = {https://arxiv.org/abs/2310.02971v1},
  urldate       = {2023-10-08},
}

@Misc{fuisz2022improved,
  author        = {Gabor Fuisz and Ivan Vulić and Samuel Gibbons and Inigo Casanueva and Paweł Budzianowski},
  month         = apr,
  title         = {Improved and Efficient Conversational Slot Labeling through Question Answering},
  year          = {2022},
  abstract      = {Transformer-based pretrained language models (PLMs) offer unmatched performance across the majority of natural language understanding (NLU) tasks, including a body of question answering (QA) tasks. We hypothesize that improvements in QA methodology can also be directly exploited in dialog NLU; however, dialog tasks must be {\textbackslash}textit\{reformatted\} into QA tasks. In particular, we focus on modeling and studying {\textbackslash}textit\{slot labeling\} (SL), a crucial component of NLU for dialog, through the QA optics, aiming to improve both its performance and efficiency, and make it more effective and resilient to working with limited task data. To this end, we make a series of contributions: 1) We demonstrate how QA-tuned PLMs can be applied to the SL task, reaching new state-of-the-art performance, with large gains especially pronounced in such low-data regimes. 2) We propose to leverage contextual information, required to tackle ambiguous values, simply through natural language. 3) Efficiency and compactness of QA-oriented fine-tuning are boosted through the use of lightweight yet effective adapter modules. 4) Trading-off some of the quality of QA datasets for their size, we experiment with larger automatically generated QA datasets for QA-tuning, arriving at even higher performance. Finally, our analysis suggests that our novel QA-based slot labeling models, supported by the PLMs, reach a performance ceiling in high-data regimes, calling for more challenging and more nuanced benchmarks in future work.},
  archiveprefix = {arXiv},
  eprint        = {2204.02123},
  journal       = {arXiv.org},
  language      = {en},
  primaryclass  = {cs.CL},
  url           = {https://arxiv.org/abs/2204.02123v1},
  urldate       = {2023-10-08},
}

@InProceedings{Wang_2022,
  author    = {Weizhi Wang and Zhirui Zhang and Junliang Guo and Yinpei Dai and Boxing Chen and Weihua Luo},
  booktitle = {Proceedings of the 45th International {ACM} {SIGIR} Conference on Research and Development in Information Retrieval},
  title     = {Task-Oriented Dialogue System as Natural Language Generation},
  year      = {2022},
  month     = {jul},
  publisher = {{ACM}},
  abstract  = {In this paper, we propose to formulate the task-oriented dialogue system as the purely natural language generation task, so as to fully leverage the large-scale pre-trained models like GPT-2 and simplify complicated delexicalization prepossessing. However, directly applying this method heavily suffers from the dialogue entity inconsistency caused by the removal of delexicalized tokens, as well as the catastrophic forgetting problem of the pre-trained model during fine-tuning, leading to unsatisfactory performance. To alleviate these problems, we design a novel GPT-Adapter-CopyNet network, which incorporates the lightweight adapter and CopyNet modules into GPT-2 to achieve better performance on transfer learning and dialogue entity generation. Experimental results conducted on the DSTC8 Track 1 benchmark and MultiWOZ dataset demonstrate that our proposed approach significantly outperforms baseline models with a remarkable performance on automatic and human evaluations.},
  doi       = {10.1145/3477495.3531920},
  journal   = {arXiv.org},
  language  = {en},
  url       = {https://doi.org/10.1145%2F3477495.3531920},
  urldate   = {2023-10-09},
}

@InProceedings{gupta_show_2022,
  author     = {Gupta, Raghav and Lee, Harrison and Zhao, Jeffrey and Cao, Yuan and Rastogi, Abhinav and Wu, Yonghui},
  booktitle  = {Proceedings of the 2022 {Conference} of the {North} {American} {Chapter} of the {Association} for {Computational} {Linguistics}: {Human} {Language} {Technologies}},
  title      = {Show, {Don}'t {Tell}: {Demonstrations} {Outperform} {Descriptions} for {Schema}-{Guided} {Task}-{Oriented} {Dialogue}},
  year       = {2022},
  address    = {Seattle, United States},
  month      = jul,
  pages      = {4541--4549},
  publisher  = {Association for Computational Linguistics},
  abstract   = {Building universal dialogue systems that operate across multiple domains/APIs and generalize to new ones with minimal overhead is a critical challenge. Recent works have leveraged natural language descriptions of schema elements to enable such systems; however, descriptions only indirectly convey schema semantics. In this work, we propose Show, Don't Tell, which prompts seq2seq models with a labeled example dialogue to show the semantics of schema elements rather than tell the model through descriptions. While requiring similar effort from service developers as generating descriptions, we show that using short examples as schema representations with large language models results in state-of-the-art performance on two popular dialogue state tracking benchmarks designed to measure zero-shot generalization - the Schema-Guided Dialogue dataset and the MultiWOZ leave-one-out benchmark.},
  doi        = {10.18653/v1/2022.naacl-main.336},
  keywords   = {Important},
  shorttitle = {Show, {Don}'t {Tell}},
  url        = {https://aclanthology.org/2022.naacl-main.336},
  urldate    = {2023-08-04},
}

@Misc{xue_mt5_2021,
  author     = {Xue, Linting and Constant, Noah and Roberts, Adam and Kale, Mihir and Al-Rfou, Rami and Siddhant, Aditya and Barua, Aditya and Raffel, Colin},
  month      = mar,
  note       = {arXiv:2010.11934 [cs]},
  title      = {{mT5}: {A} massively multilingual pre-trained text-to-text transformer},
  year       = {2021},
  abstract   = {The recent "Text-to-Text Transfer Transformer" (T5) leveraged a unified text-to-text format and scale to attain state-of-the-art results on a wide variety of English-language NLP tasks. In this paper, we introduce mT5, a multilingual variant of T5 that was pre-trained on a new Common Crawl-based dataset covering 101 languages. We detail the design and modified training of mT5 and demonstrate its state-of-the-art performance on many multilingual benchmarks. We also describe a simple technique to prevent "accidental translation" in the zero-shot setting, where a generative model chooses to (partially) translate its prediction into the wrong language. All of the code and model checkpoints used in this work are publicly available.},
  doi        = {10.48550/arXiv.2010.11934},
  keywords   = {Computer Science - Computation and Language},
  publisher  = {arXiv},
  shorttitle = {{mT5}},
  url        = {http://arxiv.org/abs/2010.11934},
  urldate    = {2024-03-25},
}

@Comment{jabref-meta: databaseType:bibtex;}

@Comment{jabref-meta: grouping:
0 AllEntriesGroup:;
1 StaticGroup:AttempsSLU\;0\;0\;0x99cc99ff\;\;\;;
1 StaticGroup:Constrained Prompting\;0\;1\;0xff0000ff\;\;\;;
1 StaticGroup:General LLM-Prompting\;0\;1\;0x8a8a8aff\;\;\;;
1 StaticGroup:Tagging Prompting\;0\;1\;0x99b3ffff\;\;\;;
1 StaticGroup:SLU\;0\;1\;0x7dc4d5ff\;\;\;;
1 StaticGroup:Datasets\;0\;0\;\;\;\;;
}
